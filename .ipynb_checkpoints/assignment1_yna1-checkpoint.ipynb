{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "fc711722f79780a71983d5305349753c3d72eab68e9e5adcb844c834"
   },
   "source": [
    "# COMP90042 Assignment #1: Sentiment analysis for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false,
    "signature": "468a2785d07562970725c3116f9f0f75b86a68a7bb2ee68484b893d7"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-293-0014b4a086b8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-293-0014b4a086b8>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Student Name: Yan Na\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Student Name: Yan Na\n",
    "Student ID: 615913"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "c947d9b84d29318c51e0cf513879cc6d3e6fe78fa8089c32d6731f89"
   },
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "08c7860b96ac8b99bf1cb83288696c3049751d7ba55307861b32abe5"
   },
   "source": [
    "<b>Due date</b>: 5pm, Mon April 12\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this ipython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day, no late submissions after the first week\n",
    "\n",
    "<b>Marks</b>: 25% of mark for class\n",
    "\n",
    "<b>Overview</b>: For this project, you'll be building a 3-way polarity classification system for tweets, using a logistic regression classifier, BOW features, as well as polarity lexicons built from external sources. A key focus of this project is critical analysis and experimental evaluation, for which you will need to report on the relative merits of various options. \n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Sci-kit Learn, and Gemsim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python build-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. You are encouraged to use the iPython notebooks released for this class as well as other online documentation to guide your responses, but you should not copy directly from any source. The only other data you will need is three sets of tagged tweets, the first two of which (training and dev) were released at the same time as this notebook, and a third set (test) which will be made available about a week before the assignment is due, see Final Testing below. This data comes from the recent SemEval 2016 shared task. Do not distribute this data indiscriminately (i.e. put it on a public website), you should use it only for this assignment, and delete it afterwards. The corpus is comprised of unfiltered text from the web, and may include offensive or objectionable material. This reflects the general composition of the web and the general challenges present in web based text analysis. The University of Melbourne takes no responsibility for opinions expressed in the corpus, nor takes any responsibility for offence caused by these documents.\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time (less than 10 minutes on a lab desktop), and you must follow all instructions provided below, including specific implementation requirements. You will be marked not only on the correctness of your methods, but also on your explanation and analysis. Please do not change any of instruction text in the notebook. Where applicable, leave the output cells in the code, particularly when you are commenting on that output. You should add your answers and code by inserting a markdown cell between every major function or other block of code explaining its purpose or anywhere a result needs to be discussed (see the class notebooks for examples). Note that even if you do something wrong, you might get partial credit if you explain it enough that we can follow your reasoning, whereas a fully correct assignment with no text commentary will not receive a passing score. You will not be marked directly on the performance of your final classifier, but each of the steps you take to build it should be reasonable and well-defended.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this project, and we encourage you to discuss it in general terms with other students. However, it is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the Universityâ€™s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "62814f6051a286e1032378817d70f0b7dec6d45958bc7507b64e873a"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "6013043f0af4cf3a579c99ac727a51ea984814d57ec82f1a9fd0a8e2"
   },
   "source": [
    "<b>Instructions</b>: Your first task is to carry out preprocessing on the tweets. Use the code below as a starter. Each line of the input files is a json including the tweet and the label (and the tweet id), this code just loads them into a list without any preprocessing. Note that for the labels, 1 = positive, 0 = neutral, -1 = negative. Here is a list of things your preprocessing code must do:\n",
    "\n",
    "<ul>\n",
    "<li>Segment into sentences: Use NLTK punkt sentence segmenter</li>\n",
    "<li>Tokenize sentences: Use the NLTK regex WordPunct tokenizer</li>\n",
    "<li>Lowercase all words</li>\n",
    "<li>Remove Twitter usernames: Usernames on twitter begin with @</li>\n",
    "<li>Remove URLs: URLs start with http</li> \n",
    "<li>Remove any hashtags from their original location in the tweet, tokenize them, and add them as a separate sentences with the hash tag removed: for tokenization, use capitalized letters when they occur (e.g. #RefugeesWelcome -> Refugees Welcome), or when there is no capitalization (#refugeeswelcome -> refugees welcome) use the MaxMatch algorithm and the list of English words included in NLTK (nltk.corpus.words.words()). Two notes about the English word list: 1. you should convert it to a python set before you use it (sets are hashed, so you get much quicker lookup) 2. It contains only base forms, so you will need to lemmatize words before you look them up.</li>\n",
    "</ul>\n",
    "\n",
    "You can do these in almost any order you like, but it may be useful to do the main segmentation/tokenization last (or almost last), since for the other tasks it is easier to deal with the raw string rather than a list of tokens. The use of regular expressions is recommended, but not required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false,
    "signature": "6ff9aff067405f7ab97124991cbd12fa25385fac7588b70ea8e3a1b3"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "theDictionary = nltk.corpus.words.words()\n",
    "theDicSet = set(theDictionary)\n",
    "\n",
    "\n",
    "def hashtags_seperation(tweet): # this function is used to seperate the hashtags from the tweets in the required format\n",
    "    global theCurrTw\n",
    "    global twTokens\n",
    "    global moreThanOne\n",
    "    newTweet = tweet[:]\n",
    "    hashtags = re.findall(r'#\\S*',newTweet)\n",
    "    #print newTweet # for testing\n",
    "    newTweet=re.sub(r'#\\S*','',newTweet)\n",
    "    newTweet = newTweet.lower() # put lowercase function here to avoid its affect to hastags\n",
    "    #print newTweet # for testing\n",
    "    for hashtag in hashtags:\n",
    "        hashtag = re.sub(r'#','',hashtag).strip()\n",
    "        #print \"the original hashtag is:\",hashtag #for testing \n",
    "        newTweet = newTweet + '. ' + hashtag_tokenization(hashtag)\n",
    "        wordsInTag = hashtag_tokenization(hashtag).split(' ') #convert the hashtag(string) to hastag(list)\n",
    "        for element in wordsInTag: # remove the '' in the list which is generated by max_match\n",
    "            if element == '':\n",
    "                del wordsInTag[wordsInTag.index(element)]              \n",
    "        wordNum = len(wordsInTag) #calculate word numbers in the hashtag\n",
    "        \n",
    "        #print \"wordNUM= \",wordNum, # for testing \n",
    "        #print \"type:\", type(wordsInTag[1]) # for testing\n",
    "        #print \"wordsInTag = \",wordsInTag # for testing\n",
    "        if wordNum > 1:\n",
    "            moreThanOne = True\n",
    "    #print hashtags # for testing\n",
    "    return newTweet\n",
    "\n",
    "\n",
    "def hashtags_seperation_v2(tweet): # this function is used to seperate the hashtags from the tweets in the required format\n",
    "    global theCurrTw\n",
    "    global twTokens\n",
    "    global moreThanOne\n",
    "    newTweet = tweet[:]\n",
    "    hashtags = re.findall(r'#\\S*',newTweet)\n",
    "    #print newTweet # for testing\n",
    "    newTweet=re.sub(r'#\\S*','',newTweet)\n",
    "    newTweet = newTweet.lower() # put lowercase function here to avoid its affect to hastags\n",
    "    #print newTweet # for testing\n",
    "    for hashtag in hashtags:\n",
    "        hashtag = re.sub(r'#','',hashtag).strip()\n",
    "        #print \"the original hashtag is:\",hashtag #for testing \n",
    "        newTweet = newTweet + '. ' + hashtag_tokenization_v2(hashtag)\n",
    "        wordsInTag = hashtag_tokenization_v2(hashtag).split(' ') #convert the hashtag(string) to hastag(list)\n",
    "        for element in wordsInTag: # remove the '' in the list which is generated by max_match\n",
    "            if element == '':\n",
    "                del wordsInTag[wordsInTag.index(element)]              \n",
    "        wordNum = len(wordsInTag) #calculate word numbers in the hashtag\n",
    "        \n",
    "        #print \"wordNUM= \",wordNum, # for testing \n",
    "        #print \"type:\", type(wordsInTag[1]) # for testing\n",
    "        #print \"wordsInTag = \",wordsInTag # for testing\n",
    "        if wordNum > 1:\n",
    "            moreThanOne = True\n",
    "    #print hashtags # for testing\n",
    "    return newTweet\n",
    "      \n",
    "def hashtag_tokenization(hashtag): # this is for tokenizing hashtags extracted from \"hashtags_seperation(tweet)\"\n",
    "    thetag = hashtag[:]\n",
    "    theNewTag = \"\"\n",
    "    tokensList = []\n",
    "    if re.search(r'[A-Z]',thetag):\n",
    "        tokensList = re.findall(r'[A-Z][^A-Z]*',thetag)\n",
    "        theNewTag = ' '.join(tokensList)\n",
    "    else:\n",
    "        #global theDictionary # = nltk.corpus.words.words()[:]\n",
    "        try:\n",
    "            theNewTag = max_match(thetag)\n",
    "        except Exception,e:\n",
    "            print e\n",
    "            print \"the error is \" + thetag         \n",
    "    theNewTag = theNewTag.lower() # whether the hastags should be displayed in lowercase??\n",
    "    return theNewTag\n",
    "    \n",
    "def hashtag_tokenization_v2(hashtag): # this is for tokenizing hashtags extracted from \"hashtags_seperation(tweet)\"\n",
    "    thetag = hashtag[:]\n",
    "    theNewTag = \"\"\n",
    "    tokensList = []\n",
    "    if re.search(r'^[A-Z]+$',thetag):\n",
    "        thetag = thetag.lower() #if all the letters in the tag are uppercase, transfer them all to lowercase\n",
    "    if re.search(r'[A-Z]',thetag):\n",
    "        tokensList = re.findall(r'[A-Z][^A-Z]*',thetag)\n",
    "        theNewTag = ' '.join(tokensList)\n",
    "    else:\n",
    "        #global theDictionary # = nltk.corpus.words.words()[:]\n",
    "        try:\n",
    "            theNewTag = max_match(thetag)\n",
    "        except Exception,e:\n",
    "            print e\n",
    "            print \"the error is \" + thetag         \n",
    "    theNewTag = theNewTag.lower() # whether the hastags should be displayed in lowercase??\n",
    "    return theNewTag\n",
    "\n",
    "def max_match(hashtag): # this is referenced from the M3 ch2\n",
    "    thetag = hashtag[:]\n",
    "    global theDicSet\n",
    "    #thedic = dictionary[:]\n",
    "    #theDicSet = set(thedic) #transfer \"nltk.corpus.words.words()\" array to set\n",
    "    i = len(thetag)\n",
    "    if thetag == '':\n",
    "        return ''\n",
    "        \n",
    "    while i >= 1:\n",
    "        firstword = thetag[0:i]\n",
    "        remainder = thetag[i:]\n",
    "        #if thedic.count(lemmatization(firstword)) != 0: \n",
    "        if lemmatization(firstword) in theDicSet: \n",
    "            # use lemmatized hastag because the word list only containts base forms\n",
    "            #print firstword + ' ' + remainder # for testing\n",
    "            return firstword + ' ' + max_match(remainder)       \n",
    "        i=i-1\n",
    "        \n",
    "    firstword = thetag[0]\n",
    "    remainder = thetag[1:]\n",
    "    return firstword + ' ' + max_match(remainder)\n",
    "\n",
    "def lemmatization(word):\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    lemma_word = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma_word == word:\n",
    "        lemma_word = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma_word\n",
    "\n",
    "def username_remove(tweet):\n",
    "    newTweet = tweet[:]\n",
    "    newTweet = re.sub(r'@\\S*','',newTweet)\n",
    "    return newTweet\n",
    "\n",
    "def URL_removal(tweet):\n",
    "    newTweet = tweet[:]\n",
    "    newTweet = re.sub(r'http\\S*','',newTweet)\n",
    "    return newTweet\n",
    "    \n",
    "def sentence_segmenter(tweet):\n",
    "    segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = segmenter.tokenize(tweet)\n",
    "    return sentences\n",
    "\n",
    "def tokenizer(aSentence):\n",
    "    word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "    tokens = word_tokenizer.tokenize(aSentence)\n",
    "    return tokens\n",
    "    \n",
    "def preprocess(tweet):\n",
    "    global twTokens\n",
    "    global moreThanOne\n",
    "    global exampleNum\n",
    "    \n",
    "    theTweet = tweet[:]\n",
    "    theTweet = username_remove(theTweet) #remove username\n",
    "    theTweet = URL_removal(theTweet) # remove url\n",
    "    theTweet = hashtags_seperation(theTweet) #lowercasing and seperating hastags        \n",
    "    theSentences = sentence_segmenter(theTweet) #segment the tweet into sentences\n",
    "    word_tokens = []\n",
    "    for sentence in theSentences: #tokenization of tweets\n",
    "        theNewTokens = tokenizer(sentence)[:]\n",
    "        theNewTokens = [lemmatization(token) for token in theNewTokens] #lemmatizing the final tokens of each tweet\n",
    "        word_tokens = word_tokens + theNewTokens\n",
    "    twTokens = word_tokens[:]\n",
    "    if moreThanOne and (exampleNum < 20):\n",
    "        print \"the tweet with hashtags of more than one word is: \" + theCurrTw\n",
    "        print \"the preprocessed tweet with hashtags of more than one word is: \", twTokens, \"\\n\"\n",
    "        exampleNum = exampleNum + 1\n",
    "    return word_tokens\n",
    "\n",
    "def preprocess_no_lemma(tweet): # to preprocess tweet without lemmatize the final tokens\n",
    "    global twTokens\n",
    "    global moreThanOne\n",
    "    global exampleNum\n",
    "    \n",
    "    theTweet = tweet[:]\n",
    "    theTweet = username_remove(theTweet) #remove username\n",
    "    theTweet = URL_removal(theTweet) # remove url\n",
    "    theTweet = hashtags_seperation(theTweet) #lowercasing and seperating hastags        \n",
    "    theSentences = sentence_segmenter(theTweet) #segment the tweet into sentences\n",
    "    word_tokens = []\n",
    "    for sentence in theSentences: #tokenization of tweets\n",
    "        theNewTokens = tokenizer(sentence)[:]\n",
    "        #theNewTokens = [lemmatization(token) for token in theNewTokens] #lemmatizing the final tokens of each tweet\n",
    "        word_tokens = word_tokens + theNewTokens\n",
    "    twTokens = word_tokens[:]\n",
    "    if moreThanOne and (exampleNum < 20):\n",
    "        print \"the tweet with hashtags of more than one word is: \" + theCurrTw\n",
    "        print \"the preprocessed tweet with hashtags of more than one word is: \", twTokens, \"\\n\"\n",
    "        exampleNum = exampleNum + 1\n",
    "    return word_tokens\n",
    "\n",
    "def preprocess_no_lemma_hashtagV2(tweet): # to preprocess tweet without lemmatize the final tokens\n",
    "    global twTokens\n",
    "    global moreThanOne\n",
    "    global exampleNum\n",
    "    \n",
    "    theTweet = tweet[:]\n",
    "    theTweet = username_remove(theTweet) #remove username\n",
    "    theTweet = URL_removal(theTweet) # remove url\n",
    "    theTweet = hashtags_seperation_v2(theTweet) #lowercasing and seperating hastags        \n",
    "    theSentences = sentence_segmenter(theTweet) #segment the tweet into sentences\n",
    "    word_tokens = []\n",
    "    for sentence in theSentences: #tokenization of tweets\n",
    "        theNewTokens = tokenizer(sentence)[:]\n",
    "        #theNewTokens = [lemmatization(token) for token in theNewTokens] #lemmatizing the final tokens of each tweet\n",
    "        word_tokens = word_tokens + theNewTokens\n",
    "    twTokens = word_tokens[:]\n",
    "    if moreThanOne and (exampleNum < 20):\n",
    "        print \"the tweet with hashtags of more than one word is: \" + theCurrTw\n",
    "        print \"the preprocessed tweet with hashtags of more than one word is: \", twTokens, \"\\n\"\n",
    "        exampleNum = exampleNum + 1\n",
    "    return word_tokens\n",
    "\n",
    "def preprocess_file(filename): # to preprocess tweet file with lemmatizing the final tokens\n",
    "    global theCurrTw\n",
    "    global moreThanOne\n",
    "    #print filename\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    f = open(filename,'r') \n",
    "    for line in f:\n",
    "        tweet_dict = json.loads(line)\n",
    "        theCurrTw = tweet_dict[\"text\"]\n",
    "        tweets.append(preprocess(theCurrTw))\n",
    "        labels.append(int(tweet_dict[\"label\"]))\n",
    "        moreThanOne = False\n",
    "    return tweets,labels\n",
    "\n",
    "def preprocess_file_no_lemma(filename): # to preprocess tweet file without lemmatizing the final tokens\n",
    "    global theCurrTw\n",
    "    global moreThanOne\n",
    "    #print filename\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    f = open(filename,'r') \n",
    "    for line in f:\n",
    "        tweet_dict = json.loads(line)\n",
    "        theCurrTw = tweet_dict[\"text\"]\n",
    "        tweets.append(preprocess_no_lemma(theCurrTw))\n",
    "        labels.append(int(tweet_dict[\"label\"]))\n",
    "        moreThanOne = False\n",
    "    return tweets,labels\n",
    "\n",
    "def preprocess_file_no_lemma_hashtagV2(filename): # to preprocess tweet file with new hashtag method\n",
    "    global theCurrTw\n",
    "    global moreThanOne\n",
    "    #print filename\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    f = open(filename,'r') \n",
    "    for line in f:\n",
    "        tweet_dict = json.loads(line)\n",
    "        theCurrTw = tweet_dict[\"text\"]\n",
    "        tweets.append(preprocess_no_lemma_hashtagV2(theCurrTw))\n",
    "        labels.append(int(tweet_dict[\"label\"]))\n",
    "        moreThanOne = False\n",
    "    return tweets,labels\n",
    "\n",
    "\n",
    "#tweet=\"Hi, Ryan, #afs8#*\"\n",
    "#hashtags_seperation(tweet)\n",
    "#tweet = \"I like you#HashTag\"\n",
    "#print hashtags_seperation(tweet)\n",
    "\n",
    "#try:\n",
    "#    hashtag = \"maxhash\"\n",
    "#    theDictionary = nltk.corpus.words.words()[:]\n",
    "#    print max_match (hashtag,theDictionary)\n",
    "#except Exception,e:\n",
    "#    print e\n",
    "\n",
    "#t1 = \"@HeartNorthWest #time you can't beat a bit off Michael Jackson on a Wednesday morning tunnel\"\n",
    "#t2 = \"Hi, I hate you,#HashTag @hahaha , https://t.co/baLteWO0P9\"\n",
    "#t3 = \"Hi, I hate bug #\"\n",
    "#tag = \"maxhash\"\n",
    "#print max_match(tag,theDictionary)\n",
    "#print t3 + \" ---- \" + hashtags_seperation(t3)\n",
    "#print t1 + \" ---- \" + hashtags_seperation(t1)\n",
    "#print t2 + \" ---- \" + hashtags_seperation(t2)\n",
    "#print username_remove(t2)\n",
    "#print preprocess(t1)\n",
    "\n",
    "#theCurrTw = \"\"\n",
    "#tweetsList,labelList = preprocess_file\\\n",
    "#(r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\dev.json\")\n",
    "#print tweetsList[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "04fa1596ff6e06a2fbc91194f5ef89ad5fd5b8f3c721e2a7d7b8c51d"
   },
   "source": [
    "<b>Instructions</b>: Once your basic preprocessing module is working, run it on the training set and have it print out 10 examples where your system identified a hashtag with more than one word inside; print out both the original tweet string as well as result after preprocessing. It's okay if you have to duplicate some code from above to do this. Point out any errors you see in the preprocessing, and discuss possible solutions; these can be related to the hashtags, or any other errors you see. You do not have to fix the errors unless they actually indicate a actual bug in your code (at which point you should go back to the previous section, fix the code, and print out the samples again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false,
    "signature": "29c40a9608245971b1e2ff9c2ac6ae80436fc8c42a354b5872db6391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tweet with hashtags of more than one word is: If I make a game as a #windows10 Universal App. Will #xboxone owners be able to download and play it in November? @majornelson @Microsoft\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'if', u'i', u'make', u'a', u'game', u'as', u'a', u'universal', u'app', u'.', u'will', u'owners', u'be', u'able', u'to', u'download', u'and', u'play', u'it', u'in', u'november', u'?', u'.', u'windows', u'1', u'0', u'.', u'x', u'box', u'one'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: For the 1st time @Skype has a \"High Startup impact\" Does anyone at @Microsoft have a clue?#Windows10Fail http://t.co/loO3yd5rwe\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'for', u'the', u'1st', u'time', u'has', u'a', u'\"', u'high', u'startup', u'impact', u'\"', u'does', u'anyone', u'at', u'have', u'a', u'clue', u'?', u'.', u'windows10', u'fail'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: #teens @BillGates 1st company failed miserably. When Gates & @PaulGAllen tried to sell the product it wouldn't work #nevergiveup @Microsoft\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'1st', u'company', u'failed', u'miserably', u'.', u'when', u'gates', u'&', u'tried', u'to', u'sell', u'the', u'product', u'it', u'wouldn', u\"'\", u't', u'work', u'.', u'teens', u'.', u'never', u'give', u'up'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: #Vote for @AIESEC to become the 10th Global non profit partner of @Microsoft for us to #UpgradeYourWorld together. @AIESECGermany\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'for', u'to', u'become', u'the', u'10th', u'global', u'non', u'profit', u'partner', u'of', u'for', u'us', u'to', u'together', u'.', u'.', u'vote', u'.', u'upgrade', u'your', u'world'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: Top 5 most searched for Back-to-School topics -- the list may surprise you http://t.co/Xj21uMVo0p  @bing @MSFTnews #backtoschool @Microsoft\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'top', u'5', u'most', u'searched', u'for', u'back', u'-', u'to', u'-', u'school', u'topics', u'--', u'the', u'list', u'may', u'surprise', u'you', u'.', u'back', u'to', u'school'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: @taehongmin1 We have an IOT workshop by @Microsoft at 11PM on the Friday - definitely worth going for inspiration! #HackThePlanet\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'we', u'have', u'an', u'iot', u'workshop', u'by', u'at', u'11pm', u'on', u'the', u'friday', u'-', u'definitely', u'worth', u'going', u'for', u'inspiration', u'!', u'.', u'hack', u'the', u'planet'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: @ForbesRussia #MBA #casestudy Namaste 2 #google and @Microsoft's CEOs, but #Multiculturals mttr!May B the era of Bad Translations wd B Over?\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'namaste', u'2', u'and', u'ceos', u',', u'but', u'mttr', u'!', u'may', u'b', u'the', u'era', u'of', u'bad', u'translations', u'wd', u'b', u'over', u'?.', u'm', u'b', u'a', u'.', u'cases', u'tu', u'd', u'y', u'.', u'goo', u'g', u'l', u'e', u'.', u'multiculturals'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: After 75 minutes of being on hold with @Microsoft in India 1-800-936-5700 \"Adrian\" wants to transfer my call again(3rd time) #Windows10Fail\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'after', u'75', u'minutes', u'of', u'being', u'on', u'hold', u'with', u'in', u'india', u'1', u'-', u'800', u'-', u'936', u'-', u'5700', u'\"', u'adrian', u'\"', u'wants', u'to', u'transfer', u'my', u'call', u'again', u'(', u'3rd', u'time', u')', u'.', u'windows10', u'fail'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: We're excited to learn about #cloud #analytics from @Microsoft tomorrow! Join us https://t.co/p0bMREBBHC #tech #rva http://t.co/1XHmPdSvzq\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'we', u\"'\", u're', u'excited', u'to', u'learn', u'about', u'from', u'tomorrow', u'!', u'join', u'us', u'.', u'cloud', u'.', u'analytics', u'.', u'tech', u'.', u'r', u'v', u'a'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: Good Friday morning. This city is changing. Are you with it? #300MenMarch @targetnews @mcdonalds @BP_America @cocacola @microsoft @comcast\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'good', u'friday', u'morning', u'.', u'this', u'city', u'is', u'changing', u'.', u'are', u'you', u'with', u'it', u'?', u'.', u'men', u'march'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: http://t.co/luX5VvBrmJ  Register 4 the NACR Skype for Business event with @Microsoft for Sept 16th Chevy Chase, MD #skype4B #contactcenter\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'register', u'4', u'the', u'nacr', u'skype', u'for', u'business', u'event', u'with', u'for', u'sept', u'16th', u'chevy', u'chase', u',', u'md', u'.', u'b', u'.', u'contact', u'center'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: Looks like @Microsoft may still be in the handset/smartphone business!  #SurfacePhone #Windows10 http://t.co/SubGXXMqlz\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'looks', u'like', u'may', u'still', u'be', u'in', u'the', u'handset', u'/', u'smartphone', u'business', u'!', u'.', u'surface', u'phone', u'.', u'windows10'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: Enjoying feasibility workshop replacing #Cognos with @Microsoft #BI. 4th customer in less than a year!\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'enjoying', u'feasibility', u'workshop', u'replacing', u'with', u'4th', u'customer', u'in', u'less', u'than', u'a', u'year', u'!.', u'cognos', u'.', u'b', u'i', u'.'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: I hate you @Adobe, u crash more then any other application Tho @Microsoft office ur close 2nd U both can share #firstplace 4 my dislike 2day\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'i', u'hate', u'you', u'u', u'crash', u'more', u'then', u'any', u'other', u'application', u'tho', u'office', u'ur', u'close', u'2nd', u'u', u'both', u'can', u'share', u'4', u'my', u'dislike', u'2day', u'.', u'first', u'place'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: Startup your Monday in the right way. Apply 2 our Pitch battle by @microsoft at #jointheUPRISE http://t.co/MoWnEAObqk http://t.co/Fkl9WdFkUZ\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'startup', u'your', u'monday', u'in', u'the', u'right', u'way', u'.', u'apply', u'2', u'our', u'pitch', u'battle', u'by', u'at', u'.', u'u', u'p', u'r', u'i', u's', u'e'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: Predictive Analytics with @Microsoft #Azure #MachineLearning 2nd ed. Now available. http://t.co/frOUbXXOzU @MSAdvAnalytics\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'predictive', u'analytics', u'with', u'2nd', u'ed', u'.', u'now', u'available', u'.', u'.', u'azure', u'.', u'machine', u'learning'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: I can't wait to speak at @Microsoft's#MSAPC2015 event tomorrow, then fly to #NYC for the weekend, & then #INBOUND15! http://t.co/WwOUjsslKt\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'i', u'can', u\"'\", u't', u'wait', u'to', u'speak', u'at', u'event', u'tomorrow', u',', u'then', u'fly', u'to', u'for', u'the', u'weekend', u',', u'&', u'then', u'.', u'n', u'y', u'c', u'.', u'i', u'n', u'b', u'o', u'u', u'n', u'd15', u'!'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: Happy hour at @Microsoft #msapc2015 with @sarahvaughan and friends. Good luck for tomorrow's keynote http://t.co/emvqoeRS6j\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'happy', u'hour', u'at', u'with', u'and', u'friends', u'.', u'good', u'luck', u'for', u'tomorrow', u\"'\", u's', u'keynote', u'.', u'ms', u'a', u'p', u'c', u'2', u'0', u'1', u'5'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: #llvm 3.7 is the 2nd release to incorporate contributions from my team at @microsoft. Congrats to all involved! :) https://t.co/IUtbNxonfw\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'3', u'.', u'7', u'is', u'the', u'2nd', u'release', u'to', u'incorporate', u'contributions', u'from', u'my', u'team', u'at', u'congrats', u'to', u'all', u'involved', u'!', u':)', u'.', u'l', u'l', u'v', u'm'] \n",
      "\n",
      "the tweet with hashtags of more than one word is: Thank you to @Microsoft for inviting us to join them at the #IBCShow in Amsterdam next Thursday - http://t.co/AuaXij5BQL\n",
      "the preprocessed tweet with hashtags of more than one word is:  [u'thank', u'you', u'to', u'for', u'inviting', u'us', u'to', u'join', u'them', u'at', u'the', u'in', u'amsterdam', u'next', u'thursday', u'-', u'.', u'i', u'b', u'c', u'show'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "theCurrTw = \"\" # this is for tracking the current tweet that will be preprocessed\n",
    "twTokens = [] # this is for tracking the tweet that has been preprocessed \n",
    "moreThanOne = False # this is for indicating whether the tweet has a hashtag with more than one word inside\n",
    "exampleNum = 0\n",
    "#tweetsList,labelList = preprocess_file\\ # to preprocess tweet file with lemmatizing the final tokens\n",
    "#(r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\train.json\")\n",
    "tweetsList,labelList = preprocess_file_no_lemma\\\n",
    "(r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\train.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################analysis########################################################################\n",
    "# this block can print out 10 example, each of them has a hashtag with more than one word inside. I also tried to \n",
    "# lemmatize final tokens. But I think the inflection of words could be useful information, so I use the tokens without\n",
    "# lemmatization as my formal results\n",
    "#\n",
    "# Errors:\n",
    "# There are some kind of errors in the preprocessing result:\n",
    "# \n",
    "# 1. special words misunderstood\n",
    "# because of the limitation of the word list, there are some special words in hastags that cannot be extracted correctly.\n",
    "# For instance, the #google will be tokenized to ['goo','g','l','e']. \n",
    "# For addressing this, we could need to expand the list of words for making more words involved. \n",
    "# \n",
    "# 2. capitalization misunderstood\n",
    "# capitalization cannot always be used for tokenization of hashtags correctly, for instance, #MBA will be tokenized to \n",
    "# ['M','B','A'], #jointheUPRISE will be tokenized to ['U','P','R','I','S','E'], where the \"jointhe\" has been lost.\n",
    "#\n",
    "# For addressing this problem, we could treat the continuous capital letters as one token. \n",
    "#\n",
    "# 3. meanless informal words\n",
    "# some informal usage of words become meanless when become a token. for example, the 'B' in \"May B\" will become \"b\". This\n",
    "# error is accepted, I think, in machine learning project. \n",
    "#\n",
    "# 4. numbers in hashtags misunderstood\n",
    "# the number in a hashtag will be tokenized one by one, which always gives meanless results. for instance, #msapc2015 will\n",
    "# become ['m''s''a''p''c''2''0''1''5']\n",
    "# For addressing this, the continuous digits could be treated as one token.\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "941f0b024053b63c9deb8b7945238b1c7140c0490d2887f6404ac55b"
   },
   "source": [
    "<b>Instructions</b>: The next step will be to convert each of your preprocessed tweets into a feature dictionary, that is, a python dictionary where each entry corresponds to a feature and its value. At this stage, you should just build a bag-of-word feature dict, though you must allow for two possible options: one is to remove stopwords (using the NLTK stopword list), and the other is to remove words appearing <em>less</em> than n times across the entire training set (n<=0 should have no effect). The outer function (convert_to_feature dicts) should take the list of tweets resulting from the preprocess_file, and return a list of feature dictionaries in the same order (so they correspond to the label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false,
    "signature": "eb50682cfbfc6281964316a0ae615396d0ccf77788632bd6cd7f3035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'c': 1, u\"'\": 1, u'great': 1, u',': 1, u'mac': 1, u'mon': 1, u'.': 1, u'?': 1}, {u'!': 1, u\"'\": 1, u'make': 1, u'2nd': 1, u'time': 1, u'.': 1}, {u'play': 1, u'make': 1, u'.': 3, u'1': 1, u'0': 1, u'game': 1, u'x': 1, u'november': 1, u'one': 1, u'?': 1}, {u'may': 1, u'make': 1, u',': 2, u'.': 3}, {u'.': 3, u'let': 1, u'1st': 1}, {u'!': 1, u'.': 1, u'!!!': 1, u'2nd': 1}, {u',': 1, u'/': 1, u'.': 1, u'3': 1, u'1st': 1, u';': 1, u'ever': 1}, {u'\"': 1, u'says': 1, u'$': 1, u'may': 1, u',': 3, u'.': 2, u'still': 1}, {u',': 1, u'.': 1, u'sunday': 1, u'time': 1, u'morning': 1, u'day': 1}, {u'10': 1, u\"'\": 1, u'get': 1, u'wednesday': 1, u'.': 1, u'?': 1}]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def convert_to_feature_dicts(tweets,remove_stop_words,n):\n",
    "    feature_dicts = []\n",
    "    for tweet in tweets:\n",
    "        # build feature dictionary for tweet\n",
    "        feature_dict = {}\n",
    "        feature_dict = BOW_no_stopwords_no_rarewords(tweet,remove_stop_words,n)\n",
    "        feature_dicts.append(feature_dict)\n",
    "    return feature_dicts\n",
    "\n",
    "def BOW_no_stopwords_no_rarewords(tweet,stop_words,n): #to generate BOW without stop words \n",
    "    global tweetDcit\n",
    "    theBOW = {}    \n",
    "    for token in tweet:\n",
    "        if (token not in stop_words) and (tweetDict.get(token,0)>=n):\n",
    "            theBOW[token] = theBOW.get(token,0) + 1\n",
    "    return theBOW\n",
    "\n",
    "def entire_set(tweetsList): # to combine all the tokenized tweets together to one List\n",
    "    entireTweets = []\n",
    "    for element in tweetsList:\n",
    "        entireTweets = entireTweets + element\n",
    "    return entireTweets\n",
    "\n",
    "def dictionary_tweet(tweetsList):# for generating a BOW of all the tokens in input dateset\n",
    "    entireList = entire_set(tweetsList)\n",
    "    theBOW_all = {}\n",
    "    for token in entireList:\n",
    "        theBOW_all[token] = theBOW_all.get(token,0) + 1\n",
    "    return theBOW_all\n",
    "\n",
    "#########################Testing####################################\n",
    "#tweetDict = dictionary_tweet(tweetsList)\n",
    "#engStop = stopwords.words('english')\n",
    "#feaDict = convert_to_feature_dicts(tweetsList,engStop,100)\n",
    "#print feaDict[0:10]\n",
    "\n",
    "# use global variable to avoid repeated generation of the dictionary of the whole training set\n",
    "# print dictionary_tweet(tweetsList)\n",
    "####################################################################\n",
    "\n",
    "########################Running#####################################\n",
    "tweetDict = dictionary_tweet(tweetsList)\n",
    "engStop = stopwords.words('english')\n",
    "feaDict = convert_to_feature_dicts(tweetsList,engStop,100)\n",
    "print feaDict[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "786eaf810797fcdbbeb6bead3bf4133160d759e3835328fc73939fd1"
   },
   "source": [
    "## Tuning and classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "af8eb9197534643f78fa9db10c004ab2d89a999bea94d33bd780422b"
   },
   "source": [
    "<b>Instructions</b>: Using the functions you've written, you should produce lists of feature dictionaries for both training and development sets; for the training set, remove stopwords and all words that appear only once (do <em>not</em> this for the dev set). Using scikit learn, convert the data to the sparse representation used for training classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false,
    "signature": "c2cf35fbe735d63028fe68e8b6285f730ed5cdef0d47b4c6da1cbdff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 52)\t1.0\n",
      "  (0, 93)\t1.0\n",
      "  (0, 111)\t1.0\n",
      "  (0, 523)\t1.0\n",
      "  (0, 1843)\t1.0\n",
      "  (0, 2852)\t1.0\n",
      "  (0, 4388)\t1.0\n",
      "  (0, 5999)\t1.0\n",
      "  (0, 6435)\t1.0\n",
      "  (0, 10214)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 52)\t1.0\n",
      "  (1, 111)\t1.0\n",
      "  (1, 313)\t1.0\n",
      "  (1, 3363)\t1.0\n",
      "  (1, 4082)\t1.0\n",
      "  (1, 4542)\t1.0\n",
      "  (1, 6040)\t1.0\n",
      "  (1, 8861)\t1.0\n",
      "  (1, 9521)\t1.0\n",
      "  (1, 9798)\t1.0\n",
      "  (2, 111)\t3.0\n",
      "  (2, 152)\t1.0\n",
      "  (2, 169)\t1.0\n",
      "  (2, 523)\t1.0\n",
      "  :\t:\n",
      "  (16803, 9003)\t1.0\n",
      "  (16803, 9849)\t1.0\n",
      "  (16803, 10528)\t1.0\n",
      "  (16803, 10722)\t1.0\n",
      "  (16804, 25)\t1.0\n",
      "  (16804, 93)\t1.0\n",
      "  (16804, 111)\t2.0\n",
      "  (16804, 154)\t1.0\n",
      "  (16804, 170)\t1.0\n",
      "  (16804, 250)\t1.0\n",
      "  (16804, 474)\t1.0\n",
      "  (16804, 497)\t1.0\n",
      "  (16804, 528)\t1.0\n",
      "  (16804, 1324)\t1.0\n",
      "  (16804, 2006)\t1.0\n",
      "  (16804, 2819)\t1.0\n",
      "  (16804, 2822)\t1.0\n",
      "  (16804, 5280)\t1.0\n",
      "  (16804, 5783)\t1.0\n",
      "  (16804, 7830)\t1.0\n",
      "  (16804, 9624)\t1.0\n",
      "  (16804, 9796)\t1.0\n",
      "  (16804, 9798)\t1.0\n",
      "  (16804, 9979)\t1.0\n",
      "  (16804, 10690)\t1.0\n",
      "  (0, 60)\t1.0\n",
      "  (0, 68)\t1.0\n",
      "  (0, 100)\t2.0\n",
      "  (0, 162)\t1.0\n",
      "  (0, 294)\t1.0\n",
      "  (0, 543)\t1.0\n",
      "  (0, 545)\t1.0\n",
      "  (0, 900)\t1.0\n",
      "  (0, 1344)\t1.0\n",
      "  (0, 3388)\t1.0\n",
      "  (0, 4603)\t1.0\n",
      "  (0, 5206)\t1.0\n",
      "  (0, 6302)\t1.0\n",
      "  (0, 9751)\t1.0\n",
      "  (1, 120)\t1.0\n",
      "  (1, 474)\t1.0\n",
      "  (1, 949)\t1.0\n",
      "  (1, 5073)\t2.0\n",
      "  (1, 5206)\t1.0\n",
      "  (1, 5245)\t2.0\n",
      "  (1, 5313)\t1.0\n",
      "  (1, 5315)\t1.0\n",
      "  (1, 6302)\t1.0\n",
      "  (1, 8543)\t1.0\n",
      "  (1, 9992)\t1.0\n",
      "  :\t:\n",
      "  (1826, 8649)\t1.0\n",
      "  (1826, 9284)\t1.0\n",
      "  (1826, 9315)\t1.0\n",
      "  (1827, 40)\t2.0\n",
      "  (1827, 111)\t2.0\n",
      "  (1827, 125)\t1.0\n",
      "  (1827, 292)\t2.0\n",
      "  (1827, 766)\t1.0\n",
      "  (1827, 1190)\t1.0\n",
      "  (1827, 1732)\t1.0\n",
      "  (1827, 3336)\t1.0\n",
      "  (1827, 4074)\t2.0\n",
      "  (1827, 4139)\t1.0\n",
      "  (1827, 5572)\t1.0\n",
      "  (1827, 5573)\t1.0\n",
      "  (1827, 6684)\t1.0\n",
      "  (1827, 7381)\t1.0\n",
      "  (1827, 9913)\t1.0\n",
      "  (1828, 52)\t2.0\n",
      "  (1828, 3694)\t1.0\n",
      "  (1828, 4313)\t1.0\n",
      "  (1828, 7598)\t1.0\n",
      "  (1828, 9302)\t1.0\n",
      "  (1828, 9422)\t1.0\n",
      "  (1828, 9727)\t1.0\n",
      "[-1, -1, 0, 1, -1, -1, 1, -1, 1, -1, -1, 0, 1, 0, 0, -1, -1, 1, 1, -1, 1, 1, 1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 0, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 0, 1, 0, -1, -1, 1, 0, 0, 1, 1, 1, 0, 1, -1, 1, 0, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 0, 1, -1, 1, 1, 1, 0, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, -1, 1, -1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, -1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, -1, 0, 1, 1, 1, 0, 0, -1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 0, -1, 1, 1, -1, 0, 0, -1, 1, -1, 1, -1, 1, -1, 1, 0, 1, -1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, -1, 0, 1, -1, 1, 1, 1, 1, 1, 0, 0, 1, 0, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 0, 1, 1, -1, 1, 1, 0, 0, 1, 0, 1, 0, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 0, 1, 1, -1, 1, 1, 0, 0, -1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, 0, 1, -1, 1, -1, 1, 1, 0, 1, -1, 1, 1, -1, -1, 1, 1, 0, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, -1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, -1, 0, 0, 0, -1, 1, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, 1, -1, 1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, -1, -1, 1, 0, 1, -1, 0, 1, 0, 0, -1, 0, 1, -1, -1, 0, 0, 0, 0, 1, 0, -1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, -1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, -1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, -1, 0, 1, 1, 1, 0, -1, 1, 1, 1, 1, 0, 1, -1, 1, 1, -1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, -1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, -1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, -1, 0, 0, 0, 0, 0, -1, 1, 1, 1, 0, 0, -1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, -1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, 1, 0, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, -1, 1, 1, -1, 0, 0, 0, 1, 1, 0, 1, -1, -1, 0, 0, 0, 1, 1, 0, -1, 0, 0, 0, 0, 0, 1, 1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, -1, 0, 1, 1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, -1, 1, 0, 1, 0, 1, 0, 1, -1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, -1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, -1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, -1, 1, 0, -1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, -1, 1, 0, -1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, -1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, -1, 1, 0, -1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, -1, 1, 1, 0, 0, 1, -1, 0, 0, 1, 0, 0, 0, -1, 0, 0, 1, -1, -1, -1, -1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, -1, 1, 1, 1, 0, 1, 0, -1, 1, 1, 0, -1, -1, 1, 0, 0, 0, 1, 1, 0, -1, -1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, -1, 0, -1, 1, 0, 1, 1, 0, 1, 0, -1, 1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, -1, -1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, -1, 0, 0, 1, 0, 1, 0, 1, 0, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, -1, -1, 1, 1, -1, -1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, -1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, -1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 0, 1, 1, 0, 0, 1, 1, -1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, -1, -1, -1, 0, 1, 1, -1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, -1, 1, 1, 1, 0, 1, 1, 1, 1, -1, 1, 1, 1, 0, 1, 1, -1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 1, 0, 1, 0, 0, 1, -1, 1, 1, -1, 0, -1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 1, 0, 1, 1, 0, 0, -1, 0, 0, 1, 0, -1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 1, -1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, -1, 0, 1, 1, 1, 1, -1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, -1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, -1, 1, 1, 1, 1, -1, 0, 0, 0, 1, 1, 1, -1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, -1, 1, 0, -1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, -1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, -1, 0, 0, 1, 1, 0, 1, 0, 1, 1, -1, -1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, -1, 0, 0, 0, 1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, -1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, -1, 0, 0, 0, 0, 0, -1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, -1, 1, 1, 1, 1, 1, -1, -1, 0, 0, 1, 1, 0, 0, 0, -1, -1, 1, 1, 1, 0, -1, 1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, 1, 1, 1, -1, 0, 0, 0, 1, 1, -1, 0, 0, 1, 1, -1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, -1, 0, 1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, -1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, -1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, -1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, -1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, -1, 0, -1, 1, 0, -1, 1, 1, 1, 1, -1, 1, 0, 1, 1, -1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 0, 0, 0, 0, 1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 1, 1, 0, -1, 0, 0, 0, 1, -1, 1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 1, -1, 0, 0, 1, -1, 0, -1, 0, -1, 0, -1, -1, 1, -1, 1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 1, -1, 1, -1, -1, -1, -1, -1, 1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, 1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 0, -1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, -1, -1, 1, 1, 1, 0, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, -1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, -1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, -1, -1, 0, -1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, -1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, -1, 0, -1, 1, 0, 0, 0, 1, 1, -1, 1, 1, 0, 1, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, -1, 0, 0, 0, 0, 1, 0, 1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, -1, -1, 1, 0, 0, -1, -1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, -1, 0, 1, 0, 0, 1, -1, -1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, -1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, -1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, -1, 1, 1, -1, -1, 0, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, -1, 0, 0, -1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, -1, -1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, -1, 1, -1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, -1, 1, 1, 1, 1, 0, 1, 0, 1, -1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, -1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, -1, 1, -1, 0, 0, -1, 1, 1, 1, -1, 1, 1, 0, -1, 0, 1, -1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, -1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, -1, 0, 1, -1, 1, 1, -1, 0, -1, -1, 0, 0, 0, 1, 0, 0, -1, -1, 0, 1, 1, 0, 0, -1, -1, 0, 1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 1, 0, 0, 0, 1, 1, -1, 0, 1, 1, 1, 1, 0, 1, 0, -1, 0, 0, -1, -1, 0, 1, -1, 0, -1, -1, 1, 0, 1, 1, 1, -1, -1, 0, 0, 0, -1, -1, 0, 1, 0, 0, 0, -1, 1, 1, 1, -1, 1, 1, 0, 1, -1, 0, 0, 0, 1, 0, 0, -1, 0, 1, 1, 1, 1, 1, -1, 1, -1, 0, 0, -1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, -1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 0, -1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, -1, -1, -1, 0, 0, 0, 1, 0, -1, 0, 0, 1, 1, -1, -1, -1, 1, 0, 0, 0, -1, -1, 0, 1, -1, 0, -1, 0, -1, 1, 0, 0, 1, 1, 0, 0, 0, 1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 1, 0, 0, 1, -1, -1, -1, 0, -1, 1, 1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, -1, -1, -1, 1, 1, 0, 1, -1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, -1, 0, -1, -1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, -1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, -1, 1, 1, 1, -1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, -1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 1, 0, 0, -1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, -1, 0, 0, 1, 1, 1, 0, 1, 1, 1, -1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, -1, 1, 1, -1, -1, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 0, -1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, -1, 0, 1, 0, 1, 0, 0, -1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, -1, 1, -1, 1, 1, 1, 0, -1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, -1, 0, 1, 1, 0, 1, 0, 0, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, -1, -1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 0, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 0, -1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, -1, 0, 1, 0, 1, -1, -1, 0, 1, 0, 1, -1, 1, 0, -1, 0, 1, -1, -1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, -1, -1, 1, 0, 0, -1, 1, 0, 0, -1, 0, 0, 0, -1, 1, 1, -1, -1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, -1, 0, 1, 1, 0, 0, 1, 1, 0, -1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, -1, 0, 0, 1, 1, 1, 1, 1, 1, -1, 0, 1, -1, -1, 0, 1, 1, 0, 0, 0, 1, 1, 1, -1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, 1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 1, 0, 0, -1, 1, -1, -1, -1, 1, -1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, -1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, -1, -1, -1, -1, 1, -1, 1, -1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, -1, 0, -1, 1, 0, -1, -1, 1, 1, 1, 1, 0, 0, 1, -1, 1, 1, 0, 1, -1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, -1, 1, 1, 1, 1, 1, -1, 1, 0, 1, 1, 1, 1, 1, 0, 1, -1, 1, 0, 1, 1, 0, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, -1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, -1, -1, 1, 1, 1, 1, 0, -1, 1, 1, 1, 1, -1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, -1, -1, -1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, -1, 1, 0, 0, 1, 1, 1, 0, -1, 0, 1, 1, -1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, -1, 0, -1, 1, -1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, -1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, -1, 0, 0, -1, 1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 1, 0, 0, -1, 0, 0, 1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, -1, 1, 0, 0, 0, 1, 0, -1, 1, -1, 1, -1, 0, 0, 0, 1, 0, 1, 1, 1, -1, 0, 1, 1, 0, -1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, -1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, -1, -1, 1, -1, 0, 1, 0, -1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 1, 0, -1, 1, 0, 0, 1, -1, 1, 1, 0, 0, 0, 0, 0, 0, 1, -1, 1, 1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, -1, 0, -1, 0, -1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, -1, -1, 1, 1, -1, 1, 0, 1, 1, 0, 0, -1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, -1, 1, 1, 0, 0, 1, 0, 0, 0, 1, -1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, -1, 0, 1, 0, 1, 0, 0, -1, -1, 1, 1, 1, 1, -1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, -1, 1, 1, -1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, -1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, -1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, -1, 0, -1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, -1, 0, 0, 0, 1, 1, -1, -1, -1, 0, 0, 0, 1, 0, 0, 1, -1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, -1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, -1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, -1, -1, 1, 1, 0, 1, 1, 1, 1, 1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 0, 1, 1, 1, 0, -1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, -1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, -1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, -1, 0, -1, 0, 1, 1, 1, 1, 0, -1, 0, 0, -1, 1, 0, -1, 1, -1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, -1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, -1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, -1, 1, 1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, -1, 1, 0, 0, 1, -1, 1, -1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, -1, 0, 1, 1, 1, 1, 0, -1, 0, 0, 1, 0, 0, 0, 0, 1, -1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, -1, -1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, -1, -1, -1, 0, 0, 1, -1, 0, -1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, -1, -1, -1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, -1, -1, 1, 1, -1, 0, 0, -1, 0, 1, 1, 1, 1, 1, -1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, -1, 1, 1, 1, 0, -1, 1, 0, 1, -1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, -1, 0, 0, 0, 1, -1, 0, 0, 0, 0, 1, 1, 0, -1, 1, 0, 0, 0, 1, 0, -1, 1, -1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, -1, 0, 1, 0, 0, 1, -1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, -1, 0, 0, 0, 1, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, 1, -1, -1, -1, 1, 1, 0, 0, 0, 1, -1, 0, 0, 1, 1, 0, -1, 0, 0, 0, 0, 1, -1, 0, 0, 1, -1, -1, 0, -1, 1, 1, 0, 1, 0, 0, -1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 1, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 1, -1, 1, 0, 1, 1, -1, -1, 1, 0, 1, 0, 0, 0, 0, 1, 0, -1, 1, 1, 0, 1, -1, 1, 0, 1, 0, 0, 0, -1, 1, 0, 1, -1, 0, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, -1, 1, 0, 0, 1, -1, 0, 1, -1, 1, 1, 0, 0, 0, -1, 0, 0, 1, 0, -1, 0, 1, 1, 1, -1, 0, -1, 1, 1, 1, 0, 0, 1, -1, -1, -1, 0, 0, 1, -1, 1, 1, 0, 1, 0, 0, 1, 0, 0, -1, 1, 0, 1, 0, 1, 0, -1, 0, 0, 1, 1, -1, 0, 0, 1, 1, 0, -1, 0, 1, 0, 1, 0, -1, -1, 0, 1, -1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, -1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, -1, 0, 0, 0, 0, -1, 1, 0, 0, 0, -1, 0, -1, -1, 1, 0, 0, -1, 1, 1, 1, 0, -1, 1, 0, 1, 0, -1, 1, 1, -1, 1, 1, -1, 1, 0, 0, 0, 0, 1, 0, 0, -1, 1, 0, 1, -1, -1, 1, 1, 0, -1, 0, -1, 0, -1, 1, 0, -1, 0, -1, 0, 1, 1, 1, 0, 1, -1, 0, 0, -1, 0, 1, 0, 1, 0, -1, 1, 1, 1, 1, 1, 0, 1, -1, 0, 0, 0, -1, 0, 1, 1, 0, 0, -1, 0, 1, 0, 0, -1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, -1, -1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, -1, 0, 0, 0, 1, -1, 0, -1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, -1, 0, 0, 1, -1, 1, -1, 0, 1, 0, -1, 1, 1, 0, 0, 1, 1, -1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, -1, 1, -1, 0, -1, 1, 0, 1, 1, 1, 0, 0, 1, -1, 0, -1, 0, 1, 0, 1, -1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, -1, 0, 0, -1, 1, 0, 0, 1, 0, 1, 1, 1, -1, 0, 1, 1, 1, -1, -1, 0, 1, 1, 0, 0, -1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, -1, 0, 0, -1, 0, 1, 1, -1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, -1, 1, -1, -1, 1, 0, 0, 1, -1, 0, -1, 0, 0, 0, 1, 1, 1, -1, -1, 0, 1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, -1, 1, -1, -1, 1, 1, 0, 1, 1, -1, -1, 0, 0, 1, -1, 0, 1, 1, -1, 0, 1, 0, -1, 0, 1, -1, 1, -1, 0, -1, 0, -1, -1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, -1, 0, 0, -1, 1, 1, 0, -1, 1, 0, 0, 0, -1, 1, 0, -1, 0, 0, -1, 1, -1, 0, 0, 1, 1, -1, 0, 0, 1, -1, 0, 0, 1, -1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, -1, 0, 0, 0, -1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 0, 1, -1, 0, 0, -1, 1, -1, 1, 1, 1, -1, 1, 0, 0, 1, 1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 1, -1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, -1, -1, 1, -1, 0, 0, 0, 1, -1, 1, 0, 0, 1, -1, 1, 0, -1, 0, -1, 1, 1, -1, 1, -1, 0, 1, 1, -1, 0, 0, 0, 1, 1, 0, -1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, -1, 1, -1, 1, 1, 0, 1, 0, 1, 0, 0, -1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, -1, 0, 1, -1, 1, 0, 1, 1, 1, 0, 0, -1, 1, -1, 0, 0, 1, 0, -1, 0, 1, 1, 1, 1, 0, -1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, -1, -1, -1, 1, 1, 1, 0, 1, -1, 0, 0, 0, 1, 1, -1, -1, 1, 0, 1, -1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, -1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, -1, 1, 0, 1, 0, 1, -1, 1, 0, 0, 1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 1, 0, 1, 0, 0, 0, -1, -1, 0, 1, -1, -1, 0, -1, 1, 0, -1, 0, 1, 0, 0, 0, 1, 1, 0, -1, 0, -1, 0, 1, 1, 1, 0, -1, 0, 1, -1, -1, 0, 1, 1, 1, 0, 0, -1, 0, 1, -1, 0, 1, 1, 0, 0, 0, 1, 0, 1, -1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, -1, 0, 1, 1, -1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, -1, -1, 0, -1, 0, -1, 0, 1, 0, 0, 0, -1, 0, 0, 1, 0, 0, 0, 1, -1, 0, 0, 0, 1, 1, -1, 1, -1, 1, 0, 1, -1, 1, 0, 0, -1, 0, 0, 1, 1, 0, 1, -1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, -1, 0, 1, -1, 1, 1, 0, 1, -1, -1, 0, 1, -1, 1, -1, 0, 0, 1, 1, -1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, -1, 1, 1, 0, 0, 1, -1, 1, 0, -1, 0, 1, 0, 0, 1, 0, -1, 0, -1, 0, 1, 1, 1, 1, 0, -1, 1, 1, 0, -1, 0, 0, 0, 1, 0, 1, 1, -1, 1, 0, 1, 0, 0, 1, 0, -1, 1, 1, 0, -1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, -1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, -1, 0, 1, 0, -1, 0, 0, -1, 1, 1, 0, 1, 0, -1, 1, -1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, -1, 0, 0, -1, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 1, 1, -1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, -1, 1, 0, 0, -1, 0, 0, 1, 1, 0, 1, -1, 0, 0, -1, -1, 1, 1, 0, 1, 0, 0, 1, 1, -1, -1, 0, 1, 1, -1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, -1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, -1, -1, -1, 0, 1, 1, 1, 0, 0, 1, -1, -1, -1, -1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, -1, 0, 1, 0, 1, 1, 0, 0, -1, 0, -1, 0, -1, 1, -1, -1, 1, 1, 0, -1, 1, 0, -1, 1, 0, 0, 1, -1, -1, 1, 0, 0, 1, 0, 0, 0, -1, 1, 0, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 1, 0, 0, -1, 0, 0, 0, 0, 1, -1, 0, 1, 0, 1, -1, -1, 0, -1, 0, 1, 1, -1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, -1, 0, 0, 0, 1, -1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, -1, 0, 0, 0, 0, 1, 0, 0, -1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, -1, 0, 0, 1, -1, 1, 0, 0, 0, -1, 1, -1, -1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, -1, 1, -1, 0, 0, 0, -1, 0, 1, 0, 1, -1, 1, 1, 0, -1, 0, 0, 1, 1, 0, -1, 1, 0, -1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, -1, -1, 1, 1, 0, 0, 1, 1, -1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, -1, 0, 1, 1, 1, 1, 0, 1, 0, -1, 1, -1, 0, 0, 0, 1, 0, -1, 0, 0, -1, 0, 0, 1, 0, -1, -1, 1, 0, 0, -1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, -1, 1, 1, 1, 0, 0, -1, 0, -1, 0, -1, 1, 1, 0, 0, 0, 0, 1, -1, 1, 1, 1, 0, 1, 0, 0, -1, -1, 0, 1, 1, 0, 1, 1, 1, -1, -1, 1, 1, -1, 1, 0, 1, 1, 0, 0, 1, 0, -1, 0, 1, 1, 1, 0, 1, 0, 0, 1, -1, 0, 0, -1, 1, 0, -1, 1, 1, 1, 1, 0, 0, -1, 1, -1, 0, 0, 0, 1, 1, -1, 0, -1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, -1, -1, 0, 0, 1, 0, 0, -1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, -1, -1, 0, 0, 1, 0, -1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, -1, 1, 1, 0, 0, 0, 1, 1, 1, 0, -1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, -1, 0, -1, 1, 0, 1, 0, -1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 1, 1, -1, 0, 1, -1, 0, 1, 0, 0, 1, -1, 1, 1, 1, 1, 0, 0, 0, 1, -1, 0, 1, 0, 1, 0, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, -1, -1, 0, 0, 1, 0, -1, 1, 0, 0, 1, -1, 0, 1, 0, -1, 0, -1, 1, -1, 0, 1, 0, 1, 1, 1, 0, 1, -1, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 1, 0, 0, 1, 0, 1, 1, 1, 0, -1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, -1, 1, 1, 0, -1, 0, 1, 1, 1, 0, 1, 1, -1, 0, -1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1, 1, 0, -1, -1, 0, 0, 1, -1, -1, 0, -1, 1, 0, 0, 0, -1, 0, -1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, -1, 1, 1, 0, -1, 0, 0, 0, 1, 1, -1, 0, 0, 1, 1, 0, -1, 1, 0, 0, -1, 1, 0, -1, 0, 0, 1, 0, 1, -1, 0, 1, 0, 0, 1, 0, -1, 0, 1, 1, 0, 1, 0, -1, 1, 0, 1, 1, -1, 0, 1, 0, 0, 0, 1, 0, -1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, -1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, -1, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, -1, 1, 1, -1, 1, 0, 0, -1, 0, -1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, -1, 0, 1, 0, 1, 0, 0, -1, 1, 0, 0, 1, -1, 0, 1, 0, -1, 1, 0, 0, 1, -1, 0, 1, 1, 1, 0, -1, 0, -1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, -1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, -1, 0, 1, 0, -1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0, -1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, -1, 1, -1, 0, 1, -1, 1, 1, 0, 0, -1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, -1, -1, 1, 1, 0, -1, 0, 1, -1, 1, -1, 0, -1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, -1, 1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, -1, -1, 1, 0, 1, 0, 0, 0, 1, 0, -1, 1, 0, -1, 0, -1, 0, 0, 1, 0, 0, -1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, -1, -1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, -1, 0, 1, 0, 0, -1, 1, 1, 1, -1, 1, 1, 0, 0, 1, 1, 1, -1, -1, 1, -1, 0, -1, 0, 0, 1, 1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, -1, 0, 1, 1, -1, -1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, -1, 1, 0, 0, 0, -1, 0, -1, 0, 0, 1, 1, 1, 1, 1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 1, 0, -1, -1, -1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, -1, -1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, -1, 0, 0, 1, 1, -1, -1, 0, 1, 1, -1, 1, 0, 1, -1, 0, 0, 0, 0, 0, -1, 1, 0, -1, 1, 0, 1, 1, -1, 0, 1, -1, 0, 0, 0, -1, 1, 1, -1, 0, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 1, 1, -1, 1, 0, 1, -1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 1, 0, 1, 1, 1, 1, -1, 0, 0, 0, 0, -1, 1, 0, 1, 1, 0, 0, 0, -1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, -1, 0, -1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1, 1, -1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, -1, 0, 0, 0, 1, 0, 0, -1, 1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 1, 0, 0, -1, -1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, -1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, -1, 1, -1, 0, 0, 0, 1, 1, -1, 0, 1, 1, 0, 0, 0, 0, 1, -1, 0, 1, 1, 1, 0, 0, 1, 1, -1, 0, 0, 0, 0, 0, 1, 0, 0, -1, -1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, -1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, -1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, -1, 0, -1, 1, 1, 1, 1, 0, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 1, -1, 0, 0, 0, 1, -1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, -1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 1, 1, -1, 0, 1, 0, -1, 0, 0, -1, 0, 0, 1, 1, 0, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 1, -1, -1, 0, -1, 1, 0, -1, 1, 0, 1, 0, -1, -1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, -1, 0, 1, -1, 0, 0, 0, 0, 1, 1, 0, -1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, -1, 0, 0, 1, 0, 1, -1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 1, 0, 0, 1, 0, -1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, -1, 0, 0, 0, 0, 1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 1, -1, 0, 1, 1, 0, 0, 0, 1, -1, 1, -1, 1, 0, 0, 1, 0, 0, -1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1, 0, 1, 0, 1, 0, 0, -1, 1, 1, 0, 1, 1, -1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, 0, 0, 1, 0, 0, 0, 1, 1, -1, 1, 0, 0, -1, 1, 1, 0, 0, 0, 1, 1, -1, 1, 0, 0, -1, 0, -1, 0, -1, 0, 1, -1, 1, 1, 0, 1, 1, 1, 0, 1, -1, 0, 0, 0, 1, 1, -1, 1, 0, 0, 0, -1, 1, 1, 1, 0, 0, 1, -1, 0, -1, 1, -1, -1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, -1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, -1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, -1, 0, -1, 1, 0, 1, 0, 1, 0, 0, -1, 1, 0, 0, 0, -1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, -1, 0, 1, 0, 0, -1, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 1, 1, 0, 0, -1, 1, 0, 1, 1, 1, -1, 0, 0, 0, 1, 0, 1, 1, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, -1, 1, 1, 1, 1, 1, 1, 0, 0, 0, -1, 0, 1, 0, 0, 0, 1, 0, 0, -1, -1, -1, 0, 0, 1, 1, 0, 0, -1, 1, 0, 0, 1, 0, 0, 0, 0, 1, -1, 0, 1, 1, 1, 1, 0, 0, 1, -1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 1, 0, 0, 0, 0, 1, 1, -1, 1, 0, 1, 0, 1, 0, -1, 0, -1, 1, 1, 1, 1, 0, -1, 1, -1, 0, 1, 0, 1, -1, 0, 1, 1, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, -1, -1, 1, -1, 0, 1, 0, 0, 1, 0, -1, 1, -1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, -1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 1, 0, 1, 1, 0, 0, 0, -1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, -1, 0, 1, 1, -1, -1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, -1, 0, 0, 0, 0, 0, 1, -1, 0, -1, 0, -1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, -1, -1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 1, 1, 0, 0, 0, 1, 1, 1, -1, -1, 0, 0, 0, -1, 1, 1, 0, 1, 1, 1, -1, 0, 0, 0, 1, 0, -1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, -1, 1, 0, 1, 0, 0, 1, -1, -1, 0, 0, 0, 0, 0, 1, -1, -1, 1, -1, -1, 0, 0, 1, 0, 0, 0, -1, 0, 0, 1, -1, 0, 0, -1, 1, -1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, -1, 1, 1, 0, 0, 0, -1, 1, 1, 0, -1, 1, 1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 1, 0, 0, 1, 1, 0, 0, 1, -1, 1, 1, 0, 0, 0, -1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 1, 0, 0, 0, 1, -1, 0, 0, 1, 0, 0, 0, 0, 1, -1, 1, -1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, -1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, -1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, -1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, -1, 0, 0, 1, 0, 1, 0, 1, 1, -1, 0, 0, 1, 0, 0, -1, 1, 0, 0, 1, 0, -1, 0, -1, 0, 0, 0, 1, -1, 1, 0, 1, 0, 0, 0, 0, -1, 0, 1, 1, -1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, -1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, -1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, -1, 1, 1, 0, 1, 1, 1, 1, 0, -1, 1, 0, 0, 0, -1, 1, 0, 0, 1, 1, -1, -1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, -1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, -1, 0, 0, 1, -1, 0, 0, 1, 0, 1, -1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, -1, 1, 0, 0, 1, 0, 1, 0, 1, 1, -1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, -1, 0, -1, 1, 0, 0, 0, -1, 1, 0, -1, 0, 1, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 1, -1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, -1, 0, 1, -1, 0, 0, 1, 0, 1, -1, 1, -1, -1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, -1, -1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, -1, 1, 0, -1, -1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, -1, 0, 0, -1, 1, 1, 0, 0, -1, -1, 0, 1, -1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, -1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, -1, 0, 0, 1, 1, 1, -1, -1, 1, 0, 1, 0, -1, -1, 0, 1, 1, 1, -1, 0, 0, 0, 0, 1, 0, -1, 1, 0, 1, 0, 1, -1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, -1, 1, 0, 1, 0, 1, 1, 0, -1, 1, 0, 1, 0, 1, -1, 1, 1, 0, 0, 0, 1, 1, 0, 1, -1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, -1, -1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, -1, -1, 1, 0, 0, 0, 0, 0, 0, 1, 1, -1, 1, -1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, -1, 1, 0, 0, 1, 0, 1, 0, 1, 0, -1, 0, 1, 1, 1, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, 1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, -1, -1, 1, 1, 1, 0, 1, 0, 1, 1, 0, -1, 0, -1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, -1, 1, 1, 0, 0, 0, -1, 1, -1, 0, 0, 1, 0, 0, 0, 1, -1, 0, 0, -1, 0, 0, -1, 0, 1, 0, 1, -1, 1, 1, 0, 1, -1, 0, 1, -1, -1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, -1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, -1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 1, -1, 0, 1, 1, 0, 1, 1, -1, 0, 0, 1, 1, 0, 0, 1, -1, 0, 0, -1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, -1, 1, -1, 0, 0, 1, 0, 0, 0, 0, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, -1, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, 1, 0, 0, -1, 0, 0, -1, 1, 0, 0, 0, -1, 1, 1, -1, 0, 0, 0, 0, 1, -1, 0, 1, 0, -1, 1, 0, 0, 0, 0, -1, 0, 1, 0, -1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, -1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 1, 0, -1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, -1, -1, 0, -1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, -1, 1, 0, 0, 0, 1, -1, 0, -1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, -1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, -1, 0, 1, 1, 0, -1, 0, 1, 0, 1, -1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, -1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, -1, -1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 1, -1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, -1, 1, -1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, -1, 1, 1, 0, 1, 0, 0, 0, 0, 0, -1, 1, 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, -1, 1, 1, 1, 0, 1, 0, -1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, 0, 0, 0, 1, 0, 0, -1, 1, 0, -1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, -1, 1, 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, -1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, -1, 0, 0, 1, 1, -1, 0, -1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, 0, 1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 1, -1, 0, 0, 0, 1, 0, 1, 1, 1, -1, 0, 1, 0, -1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, -1, 0, 0, 1, 0, 0, -1, -1, 0, 0, 0, 0, 1, 1, -1, 1, 0, 0, 1, 0, 1, 1, 1, 1, -1, 0, -1, 0, 0, 0, 1, 0, 0, -1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, -1, -1, 1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, -1, 1, 0, -1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, -1, 0, 1, -1, 1, 1, 0, -1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, -1, 0, -1, 1, 1, 0, 1, 1, 0, -1, -1, 1, 1, -1, 0, 0, 0, 0, 1, 1, 0, 1, -1, 0, 1, -1, 0, 1, 0, 0, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 0, 1, 0, 0, -1, 0, 1, 1, 1, 0, 0, 0, 1, 0, -1, 0, -1, 0, 0, -1, -1, 0, 1, 0, 0, 0, -1, 1, 0, -1, 1, 0, 1, -1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, -1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, -1, 1, 0, 0, 1, 1, 0, 1, 0, -1, 1, -1, 0, 1, 1, -1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, -1, 1, 1, 0, -1, 1, 0, 1, 0, -1, 0, 1, 0, 1, 1, -1, 1, 1, -1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, -1, 0, 0, 1, 1, 0, 1, -1, -1, -1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, -1, 0, -1, -1, -1, -1, 0, 0, 1, 1, -1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0, 1, -1, -1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, -1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, -1, 1, 0, 0, 0, -1, 0, 1, -1, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, -1, 0, -1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, -1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, -1, 0, 1, -1, 0, 1, 1, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1, -1, 0, 1, 1, 1, 0, 1, -1, 0, 0, 1, -1, 1, 1, 0, -1, 1, -1, 0, 0, 1, 1, 0, 0, 0, 1, -1, 1, 1, 0, 0, 0, -1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, -1, 0, 1, -1, 0, 0, 1, -1, 1, -1, 1, -1, 0, 1, 0, 0, -1, 0, 1, 1, -1, 0, -1, 1, -1, 1, 0, 0, 0, 0, -1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, -1, 1, 0, 1, 0, 0, 0, 1, -1, -1, 0, 1, 0, 1, 1, -1, 1, 0, 1, 1, 1, 0, -1, -1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, -1, -1, 0, 0, 0, 0, -1, 0, 1, 0, -1, 1, 1, 1, 1, 0, 0, 0, 1, 0, -1, 1, 0, 1, -1, 1, 1, 1, 1, -1, 0, 1, -1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, -1, 0, 1, 0, 0, 0, 0, -1, 0, 0, 1, 1, 0, 0, 0, 1, 1, -1, -1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, -1, 1, 1, 0, 1, -1, 0, -1, -1, 0, 0, -1, -1, 1, 0, 1, 0, -1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 0, 1, -1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, -1, -1, 0, 1, 1, 1, 0, 1, -1, 0, 0, 0, 1, 0, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, -1, 0, 1, 0, -1, 0, 1, 0, -1, 1, 1, -1, 1, 1, -1, 1, 1, 0, 1, -1, 1, 0, 1, 0, 0, 0, -1, 1, 1, 1, 0, -1, 1, 1, 1, 0, 0, -1, 0, 0, 0, 1, -1, -1, 0, 0, 1, 0, 1, 0, 1, 0, 0, -1, 1, -1, -1, 1, 0, 0, 1, 0, 1, 1, 0, 0, -1, 1, 1, 0, -1, -1, 1, 1, 1, 0, 0, 0, -1, 0, -1, 0, 1, -1, 1, 1, -1, 0, 0, -1, 0, 0, 1, 0, 1, 0, 0, 0, 0, -1, 1, 0, 1, 0, -1, -1, 1, 0, 0, 1, 1, -1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, -1, -1, 1, -1, -1, 1, 1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 1, 0, -1, 0, 0, -1, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, -1, 0, 1, 1, 0, 0, 1, 1, 0, -1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, -1, 1, 1, 0, 0, 0, 0, 0, 1, -1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, -1, 1, 1, 0, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 0, 0, 0, 1, 1, -1, 0, 0, 1, -1, 0, 1, -1, -1, 1, -1, -1, 1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 1, 1, 0, 1, 0, 0, -1, 0, 1, 0, -1, -1, 0, 1, -1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, -1, 1, 0, 1, 1, 1, -1, 1, 1, -1, 0, 1, 1, 0, -1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, -1, 1, 1, 0, 1, 0, 0, 1, 0, 1, -1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, -1, 1, 0, 1, -1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, -1, 1, -1, 1, 1, -1, 0, 1, 1, 1, 0, 0, 0, -1, 0, 1, -1, 1, 0, 0, -1, 1, 1, 1, 1, 1, 0, 1, 1, -1, 1, 0, 1, 1, -1, -1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, -1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, -1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, -1, 1, 1, -1, 1, 1, 0, -1, 1, 1, 1, 0, -1, 0, -1, 1, 0, 1, 0, 1, 1, 0, -1, 1, -1, 1, 0, -1, 0, 1, 1, 1, 0, 1, 1, -1, 1, 1, -1, 0, 1, 1, 1, -1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, -1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, -1, -1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, -1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, -1, 1, 1, 0, 0, 1, 1, 1, 1, -1, 0, -1, 1, 0, 1, 1, -1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, -1, 0, -1, 0, 1, 1, 1, 0, 0, -1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, -1, -1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0, 1, 1, 0, 0, 0, 1, 0, -1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, -1, -1, 1, 0, 1, -1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, -1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, -1, 0, 1, 1, -1, 1, 0, 1, -1, 0, 1, 0, 1, 0, 1, -1, -1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, -1, 0, 1, 0, 0, -1, 1, 1, 0, 0, 1, 1, -1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, -1, 1, 0, 1, 1, 0, -1, 0, 0, -1, 1, 0, 1, 0, 0, 1, 0, 0, -1, 1, 0, 0, 0, 1, -1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, -1, 1, -1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, -1, 1, 1, 1, 0, 0, 1, 0, 1, 0, -1, 1, 0, 0, 0, 0, 0, -1, 1, 1, -1, 1, 1, -1, 0, -1, -1, 1, 1, 0, 0, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, -1, -1, 1, 1, 0, 1, -1, -1, 1, 1, 1, 0, -1, 0, -1, 0, 1, 0, 0, 1, 1, 1, 1, 0, -1, 0, 0, 0, 1, 1, -1, 1, -1, -1, -1, 1, 0, 0, 0, 0, 1, -1, 0, -1, 0, -1, 0, 1, -1, -1, 1, 1, 1, -1, 0, 1, 1, 1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, 1, -1, 1, 1, 0, -1, 0, 0, 1, 0, -1, 1, 1, -1, 0, 0, -1, 1, 0, 1, 1, -1, 0, 0, 0, 1, 1, -1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, -1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, -1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, -1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, -1, 0, -1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, -1, 0, 1, 0, 0, 1, 1, 0, 0, -1, 0, 0, 1, 0, -1, 0, 0, 1, 1, 1, 0, 0, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 1, -1, 0, -1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, -1, 1, -1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, -1, 1, -1, 0, 0, 1, 0, 0, -1, 0, -1, 0, 1, 1, -1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, -1, 0, 0, 1, 0, 0, 1, -1, 0, 1, 0, 0, -1, 0, 1, 0, 1, 1, 1, 0, 0, -1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, -1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, -1, -1, 0, 0, 1, 1, 1, 1, -1, 0, 1, 0, 0, 1, -1, 0, 0, 0, 1, 0, -1, 0, -1, 1, 0, -1, 0, 0, 1, 0, -1, 0, -1, 1, 1, 0, 1, 0, 1, -1, 1, 1, 1, 0, 1, 0, 1, 1, -1, 1, 0, 0, 0, 1, -1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, -1, -1, 0, -1, 0, 1, 0, 1, -1, -1, 0, 1, -1, 0, 0, 0, 1, -1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, -1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, -1, -1, 1, 0, -1, 0, -1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, -1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, -1, 1, 1, 0, 1, 1, 1, 1, 0, -1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 1, 1, -1, -1, 0, -1, -1, 1, 1, 0, 0, -1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, -1, 0, 1, 0, 0, 0, 1, 1, -1, -1, -1, 0, -1, 0, 0, 1, 0, -1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, -1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, -1, -1, 0, 1, 0, -1, 0, 1, -1, 1, -1, 1, 0, 0, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 0, -1, -1, -1, 1, -1, -1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, -1, -1, 1, 0, 0, -1, -1, 0, 1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 1, 1, -1, 1, -1, 0, 1, 0, 0, 0, -1, 1, 0, 1, 0, 1, 0, 1, 1, 1, -1, 1, 1, 1, 0, -1, -1, -1, -1, 1, 0, -1, 1, 0, -1, 0, 0, -1, 1, 0, 0, 0, 0, 1, -1, 1, 0, 1, 0, 1, 0, 0, -1, 1, 1, 1, 1, -1, -1, -1, 0, -1, 1, -1, 1, 0, -1, 1, 0, 1, 1, 1, 0, 0, -1, 1, -1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, -1, 0, -1, 0, 0, -1, 1, 1, -1, 0, 0, 0, 1, 0, 0, 1, 1, 1, -1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, -1, -1, 0, -1, -1, 0, 0, 1, 1, 0, 0, 0, -1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, -1, 0, 0, 1, 0, 0, -1, 0, 1, 1, -1, 0, 1, -1, 0, 1, 0, 1, 0, 1, 1, 0, 1, -1, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, -1, 1, 1, 1, 0, -1, 1, 1, -1, 0, 1, 1, 0, 1, -1, -1, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1, -1, 0, 1, -1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, -1, 1, 0, 1, 1, 1, 0, -1, 0, 1, 1, -1, 1, -1, -1, 1, -1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, -1, 0, 0, 0, 0, 1, 1, 1, -1, 1, 1, 1, 0, 0, 0, -1, 0, -1, 1, 1, 1, 1, 1, 0, -1, -1, 0, 1, -1, 1, 0, 1, -1, 1, 0, -1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, -1, 0, 1, 0, 1, 0, 1, -1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, -1, -1, 0, 1, -1, 1, 0, 0, 0, 1, 0, 1, 1, 0, -1, 1, 0, 0, 1, 0, -1, 1, -1, 0, 1, 0, -1, 0, 1, -1, 0, 1, 1, 1, 1, 0, -1, 0, 0, 0, -1, 0, 1, -1, 1, 1, 1, 1, 1, 1, 0, 1, -1, 0, -1, 0, -1, 1, 0, -1, -1, 1, -1, -1, 0, 1, 1, 0, 0, 1, -1, 0, 0, 1, 0, -1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, -1, -1, 1, 1, -1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, -1, 0, 0, 0, 1, 1, 0, 0, 0, 0, -1, 0, 1, 1, 0, -1, -1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, -1, -1, 0, -1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, -1, 0, 1, 0, 1, 1, 0, 1, 1, -1, 1, -1, 1, 1, 0, 1, 1, 1, 1, 0, 0, -1, 1, -1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, -1, 1, 1, 0, -1, 1, 0, 1, -1, 1, 0, 1, 0, 1, 0, 1, 0, -1, -1, 0, -1, 1, -1, 0, 0, -1, 0, 1, 0, 0, 1, 1, 1, 1, 0, -1, 0, 1, 0, -1, 1, 0, 1, 1, 0, 0, 1, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, 1, -1, 0, 1, -1, 0, -1, -1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, -1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, -1, 1, -1, 0, 0, 1, 1, -1, -1, 0, -1, 0, -1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, -1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, -1, 1, 0, 1, 0, 1, 1, 0, -1, 1, 0, -1, 0, 0, 0, -1, 1, 0, -1, 0, 1, -1, -1, 0, 0, 0, 0, 1, 0, 1, -1, -1, 1, -1, -1, 0, -1, 1, 0, -1, 0, 0, 0, 1, 0, 0, -1, -1, 1, 1, 0, 0, 1, 1, 0, -1, -1, 0, -1, 1, 0, 1, -1, 1, 0, 0, -1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, -1, -1, 0, 1, 1, 1, 0, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 0, 0, -1, 1, -1, -1, 1, -1, 1, 0, 1, 0, 1, 0, 1, 0, -1, 0, 1, 0, 1, 1, 0, 1, 1, 0, -1, 1, -1, -1, 1, 1, 0, 1, 0, 0, -1, 1, 1, 0, 1, 1, 1, 0, -1, 0, 0, 0, 1, 0, 1, -1, 1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, -1, -1, 1, 0, 0, 0, 1, -1, 0, 1, 0, 1, 0, 0, -1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 1, 0, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, -1, 0, -1, 0, 1, 0, 0, 0, -1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, -1, 0, 1, 0, 1, -1, 0, 0, 1, 1, 0, 0, 0, 0, -1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, -1, -1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, -1, 0, 0, 0, 0, 1, 0, 1, -1, 1, 0, 0, -1, 0, 1, -1, 0, 1, 1, 1, 0, 0, -1, 1, 1, 0, -1, -1, 1, 1, 1, 0, -1, 0, 0, 1, 1, -1, 0, 1, 1, 0, 0, 1, 1, 1, 0, -1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, -1, 1, 1, 1, -1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, -1, 1, 1, 0, 1, -1, 0, 1, 1, 1, 1, 0, -1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, -1, -1, 1, 0, -1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, -1, 1, -1, 0, 0, 1, 0, 1, -1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, -1, 0, 0, -1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, -1, 0, -1, 0, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 0, 1, -1, 1, -1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, -1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, -1, -1, 1, 0, 0, 0, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, -1, -1, 0, 1, 0, 1, 1, 0, 1, -1, -1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, -1, -1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, -1, -1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, -1, 0, 0, 1, 0, 1, -1, 1, 0, 1, 1, 0, 0, 1, 0, 0, -1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, -1, 1, 0, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, -1, 1, 0, 0, 0, 1, 0, 1, 1, 0, -1, 1, 1, 1, 1, 1, 1, 0, -1, 0, 0, 1, 1, 0, 1, -1, 0, 1, 1, 0, 1, 1, -1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, -1, 0, -1, 0, 0, 0, 0, 1, -1, 1, -1, 0, 0, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, -1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 0, 1, 1, -1, 1, 1, -1, 1, 0, 1, 0, 1, -1, 0, 1, 1, -1, 1, -1, -1, -1, 1, -1, 0, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, -1, 1, 0, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 0, 1, -1, 0, 1, 0, 0, 1, -1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, -1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, -1, -1, 0, 1, 1, 1, 1, 1, 1, 0, -1, 0, 0, 1, -1, 1, 1, -1, 1, -1, 1, 1, 0, 1, 1, -1, 1, 1, 0, 1, -1, 0, 0, -1, 0, 0, 1, 1, 0, 1, 1, -1, 0, 0, 0, 1, 0, 0, 1, 1, -1, 1, -1, 0, 1, 0, -1, 1, 1, 1, 0, 0, -1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, -1, 0, 1, 0, 1, 1, 0, -1, 1, -1, 0, -1, 1, -1, -1, 0, 1, -1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 1, 0, 1, 0, 1, 0, -1, 0, 1, 0, 1, 0, 0, 0, 1, 0, -1, 1, 1, 1, 0, 1, -1, -1, -1, 0, 1, 0, 0, 1, 0, 0, -1, 1, -1, 1, 0, 0, 0, 0, 1, -1, 1, 0, 0, 0, -1, 0, 1, -1, -1, 1, 0, 0, 0, -1, 0, -1, -1, -1, 1, -1, 1, 1, 1, -1, 0, 0, 1, 1, 1, -1, 0, -1, 1, 1, -1, -1, 1, 0, -1, 0, 0, 1, 1, 0, 1, 0, -1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, -1, 0, -1, 0, 1, -1, 1, 0, 1, 0, -1, 1, 1, 0, -1, -1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, -1, 1, 1, -1, -1, 0, 1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, -1, 0, 1, 1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, -1, -1, 0, 0, 1, -1, -1, 1, 0, 0, 0, 0, -1, 1, -1, 0, 0, 1, 0, 0, -1, 1, 1, 0, -1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 1, 1, 0, 1, 0, 0, 0, -1, 1, 1, 1, 0, 0, 0, -1, 0, 1, 0, 0, -1, 0, 0, -1, 0, 1, 1, 0, 1, -1, 0, -1, 0, 1, 1, 0, 1, -1, -1, 1, 0, 0, 0, 1, -1, 1, 1, 1, -1, 0, 1, 0, 0, 1, -1, 1, 1, 0, 0, 0, -1, -1, 0, 0, 0, 1, 1, 1, 1, 0, -1, -1, 0, -1, 1, 1, 0, 1, 1, 0, 1, 1, -1, -1, 1, -1, -1, 0, 0, 0, 1, 0, -1, -1, 0, 0, 1, 1, 1, 1, 0, 0, -1, -1, -1, 1, 1, -1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, -1, 1, -1, 0, 1, 0, -1, 0, 1, 1, -1, 0, -1, 0, 1, 1, 0, 0, -1, 0, 0, -1, 1, 1, -1, 0, 0, 0, 0, 1, 0, -1, 0, 1, 1, -1, 1, -1, 1, 0, 0, 1, 0, -1, 0, -1, 0, 1, 0, 1, 0, -1, 0, 1, 1, 0, 0, -1, -1, 0, 1, -1, 0, -1, 1, 0, 0, 0, 1, 1, 1, -1, -1, 1, -1, 1, 0, 0, 1, -1, 0, 0, 0, 1, 1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, -1, -1, -1, -1, 1, 0, 1, 0, 0, 0, 1, 1, -1, 1, 1, 1, 0, -1, -1, 1, -1, 1, 0, 1, -1, 1, 0, 0, 1, -1, 1, -1, 1, 0, 1, 0, -1, 0, 0, -1, 1, -1, 0, 1, 1, 0, 0, -1, 0, -1, -1, 0, 0, 1, 0, 1, -1, 0, -1, 0, 0, 1, 0, 0, 1, 1, 0, 1, -1, 0, 0, -1, 0, 0, 0, 0, 1, -1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, -1, 0, 1, 0, -1, 0, 0, -1, 1, 0, 0, 0, 0, -1, 0, 1, -1, 0, 1, -1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, -1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, -1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, -1, 0, -1, 1, 1, -1, -1, 0, 0, -1, 0, 1, 1, 0, -1, -1, 1, -1, -1, 1, 0, -1, 0, 1, 0, -1, -1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, -1, 0, 0, 0, -1, -1, 1, 1, 1, 0, 0, 1, 1, 1, -1, -1, 0, 1, 0, -1, 0, 0, 1, 0, 1, -1, 1, -1, 0, 1, -1, 0, -1, 0, 0, -1, 0, 1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 1, -1, 1, 0, -1, -1, -1, 0, -1, 1, -1, 0, 0, 1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 1, 1, 0, -1, 0, 0, 1, -1, 0, 0, -1, 0, 1, -1, -1, 1, 1, -1, 0, 0, 0, 0, 1, 1, -1, 0, 1, 1, 0, -1, -1, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, -1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, -1, 0, 1, 0, -1, 1, -1, 0, 0, 0, 0, 1, -1, 0, 0, 1, 0, 0, 1, -1, 1, 0, 0, 0, 0, -1, -1, 1, 1, 0, -1, 0, -1, 1, 0, -1, 1, 1, 0, 0, 1, -1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, -1, 0, 1, 0, 1, -1, 0, 1, 0, 0, -1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, -1, -1, 0, 0, -1, 1, 0, 0, 0, 1, -1, -1, 0, 0, -1, 1, 1, 0, 0, -1, 1, 0, -1, -1, 1, 1, 0, 1, 1, -1, 0, 1, 0, 1, -1, -1, -1, 0, 0, -1, 0, 1, -1, 0, 1, 0, 0, 0, -1, 0, 0, 0, 1, 1, -1, 1, 0, 1, 0, 1, -1, 1, 0, 1, 0, 1, 1, 1, 1, -1, -1, 0, 1, 1, 0, 0, -1, 1, -1, 0, -1, 1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 1, 1, 1, 0, -1, 0, -1, -1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, -1, 0, 1, 1, 0, 1, 0, -1, 1, -1, 1, 0, 1, 0, -1, 1, 1, -1, 1, -1, 1, 0, 1, 1, -1, 0, 1, 0, 1, 0, 1, -1, 1, 0, -1, 1, -1, 0, 0, 1, 0, 1, 0, -1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, -1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, -1, 0, 0, 0, 1, 1, 1, 0, 0, 1, -1, -1, -1, 0, 1, -1, 0, -1, 1, 1, -1, 1, 0, 1, 0, 1, -1, -1, -1, 1, 0, 0, 1, 1, -1, 1, 0, 0, 1, 1, 0, 0, 0, -1, 1, 1, 1, 1, -1, 0, 0, -1, 1, -1, 0, 0, 1, -1, 0, 0, 1, -1, -1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0, 1, -1, 0, 0, 1, 1, 0, 0, -1, 1, 0, 1, -1, 0, 0, 0, 0, 1, -1, 0, 1, 0, 1, -1, 0, 0, 0, -1, -1, 1, 0, 0, 1, 1, 1, -1, 0, 1, 1, 1, 1, 0, -1, 0, 0, 1, 1, 0, 1, -1, 0, 1, -1, 0, 0, 0, 1, 0, 0, -1, 1, -1, 1, 0, 0, 0, -1, 1, -1, 1, 0, 1, -1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, -1, 0, 1, -1, 0, -1, 1, 0, -1, 1, 0, 1, 0, 0, 0, 0, 0, 1, -1, 0, 1, 0, 0, 1, -1, 1, -1, 0, 1, 0, 0, 0, -1, 1, 0, 0, 0, 1, 1, 0, 0, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, -1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, -1, 0, 0, 1, 0, 0, 1, 1, -1, 0, 1, 0, 1, 0, 1, -1, 0, 1, 0, 0, -1, 1, 1, 0, -1, -1, 1, -1, 1, 0, -1, 0, 0, -1, 0, 1, -1, 1, 0, 1, 1, 0, 1, 1, -1, -1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, -1, -1, 0, 0, 1, 1, 0, 0, 1, -1, 0, -1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, -1, 1, 0, 0, 1, 1, 0, 1, -1, -1, 0, 0, -1, 1, 1, 1, 0, -1, 1, 1, 1, 0, 0, 0, 1, -1, -1, 1, -1, 0, 0, 1, 1, -1, 0, 1, 1, 0, 1, 1, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, -1, 1, 0, 1, 1, -1, 1, 1, -1, 0, 0, 0, 1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, -1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 0, 1, 0, -1, 1, 1, 0, 0, 1, -1, 1, -1, -1, -1, 0, 0, 0, 0, 1, 1, 0, 1, -1, 0, 1, 1, 0, 1, 0, 0, -1, 1, -1, 1, 1, -1, 1, 1, 1, 0, 0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 1, 1, 0, 1, -1, 0, 0, -1, 1, 0, -1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, -1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, -1, 1, 1, 1, 0, 0, 1, 0, 0, 0, -1, -1, 1, 1, 1, 1, 1, -1, 0, -1, 0, 0, 1, 0, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, -1, 1, 0, 0, 0, 1, 1, 0, 0, 0, -1, -1, 1, 1, 0, 1, -1, 0, 0, -1, -1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 1, 0, -1, 0, 1, 0, 0, 0, -1, -1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, -1, 0, 0, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 0, 1, 1, -1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 1, 1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 1, 0, 1, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 1, 0, 1, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 1, 0, 0, 1, -1, 1, 1, 1, -1, 1, 1, 0, 1, -1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, -1, -1, 1, -1, 0, 1, 1, -1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, -1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, -1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, -1, -1, 0, 0, 1, 1, 1, 1, 1, 0, -1, 0, 1, 1, 1, 1, 1, 0, -1, -1, -1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, -1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, -1, 1, 0, 0, 0, 1, 1, 1, 1, -1, 0, 0, -1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, -1, 0, 0, 0, 1, 0, -1, 1, 1, 1, -1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, -1, -1, 0, 0, -1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, -1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, -1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, -1, 1, 0, 0, -1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 0, 1, 0, 1, 1, 0, -1, 0, 0, 0, 0, 1, 1, 0, 1, -1, 0, 0, 0, 1, -1, -1, 0, -1, 1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 1, 0, 0, -1, -1, -1, -1, -1, 1, -1, -1, 1, 0, 0, 0, -1, 0, 1, 0, 0, 1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 1, 0, 1, 0, -1, -1, 1, -1, -1, 0, -1, 0, -1, 1, 0, -1, -1, 1, 1, 0, 0, 0, 0, 0, 1, 0, -1, -1, -1, 1, 0, 0, 0, -1, 0, 1, 0, 0, -1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, -1, 1, -1, 1, 0, 0, 0, 1, 0, 1, -1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, -1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, -1, 1, 1, 0, 1, -1, 0, 1, 1, 1, 1, 0, 0, 0, 0, -1, 0, 1, -1, 0, 0, -1, -1, -1, -1, 1, 0, 1, -1, 1, 0, 0, 1, -1, -1, 0, 0, -1, -1, 1, 0, -1, -1, 1, 0, 0, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, 0, -1, 0, -1, 1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, 0, -1, 1, 0, 0, -1, 0, 0, 1, 1, 1, 1, 0, 0, -1, 1, -1, 1, -1, 0, 1, -1, 1, 0, 0, 0, 1, 0, 0, -1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, -1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, -1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, -1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, -1, -1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, -1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, -1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, -1, 1, 0, -1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, -1, -1, 0, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, 1, -1, -1, -1, -1, -1, 0, 0, 1, 1, -1, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, 1, -1, 1, -1, 0, 0, -1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, -1, 1, 1, 0, 0, 0, 0, 0, -1, 1, 1, 0, 0, 0, -1, 1, 0, 0, 0, -1, 1, -1, 0, -1, 0, 1, 1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 1, 1, 0, 0, -1, -1, 0, -1, 1, 0, 0, 1, -1, -1, 0, 0, -1, -1, -1, 1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, -1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, -1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, -1, 1, 1, 1, -1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, -1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 0, 0, -1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, -1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, -1, 1, 0, 0, 0, -1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "  (0, 52)\t1.0\n",
      "  (0, 93)\t1.0\n",
      "  (0, 111)\t1.0\n",
      "  (0, 523)\t1.0\n",
      "  (0, 1843)\t1.0\n",
      "  (0, 2852)\t1.0\n",
      "  (0, 4388)\t1.0\n",
      "  (0, 5999)\t1.0\n",
      "  (0, 6435)\t1.0\n",
      "  (0, 10214)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 52)\t1.0\n",
      "  (1, 111)\t1.0\n",
      "  (1, 313)\t1.0\n",
      "  (1, 3363)\t1.0\n",
      "  (1, 4082)\t1.0\n",
      "  (1, 4542)\t1.0\n",
      "  (1, 6040)\t1.0\n",
      "  (1, 8861)\t1.0\n",
      "  (1, 9521)\t1.0\n",
      "  (1, 9798)\t1.0\n",
      "  (2, 111)\t3.0\n",
      "  (2, 152)\t1.0\n",
      "  (2, 169)\t1.0\n",
      "  (2, 523)\t1.0\n",
      "  :\t:\n",
      "  (16803, 9003)\t1.0\n",
      "  (16803, 9849)\t1.0\n",
      "  (16803, 10528)\t1.0\n",
      "  (16803, 10722)\t1.0\n",
      "  (16804, 25)\t1.0\n",
      "  (16804, 93)\t1.0\n",
      "  (16804, 111)\t2.0\n",
      "  (16804, 154)\t1.0\n",
      "  (16804, 170)\t1.0\n",
      "  (16804, 250)\t1.0\n",
      "  (16804, 474)\t1.0\n",
      "  (16804, 497)\t1.0\n",
      "  (16804, 528)\t1.0\n",
      "  (16804, 1324)\t1.0\n",
      "  (16804, 2006)\t1.0\n",
      "  (16804, 2819)\t1.0\n",
      "  (16804, 2822)\t1.0\n",
      "  (16804, 5280)\t1.0\n",
      "  (16804, 5783)\t1.0\n",
      "  (16804, 7830)\t1.0\n",
      "  (16804, 9624)\t1.0\n",
      "  (16804, 9796)\t1.0\n",
      "  (16804, 9798)\t1.0\n",
      "  (16804, 9979)\t1.0\n",
      "  (16804, 10690)\t1.0\n",
      "  (0, 60)\t1.0\n",
      "  (0, 68)\t1.0\n",
      "  (0, 100)\t2.0\n",
      "  (0, 162)\t1.0\n",
      "  (0, 294)\t1.0\n",
      "  (0, 543)\t1.0\n",
      "  (0, 545)\t1.0\n",
      "  (0, 900)\t1.0\n",
      "  (0, 1344)\t1.0\n",
      "  (0, 3388)\t1.0\n",
      "  (0, 4603)\t1.0\n",
      "  (0, 5206)\t1.0\n",
      "  (0, 6302)\t1.0\n",
      "  (0, 9751)\t1.0\n",
      "  (1, 120)\t1.0\n",
      "  (1, 474)\t1.0\n",
      "  (1, 949)\t1.0\n",
      "  (1, 5073)\t2.0\n",
      "  (1, 5206)\t1.0\n",
      "  (1, 5245)\t2.0\n",
      "  (1, 5313)\t1.0\n",
      "  (1, 5315)\t1.0\n",
      "  (1, 6302)\t1.0\n",
      "  (1, 8543)\t1.0\n",
      "  (1, 9992)\t1.0\n",
      "  :\t:\n",
      "  (1826, 8649)\t1.0\n",
      "  (1826, 9284)\t1.0\n",
      "  (1826, 9315)\t1.0\n",
      "  (1827, 40)\t2.0\n",
      "  (1827, 111)\t2.0\n",
      "  (1827, 125)\t1.0\n",
      "  (1827, 292)\t2.0\n",
      "  (1827, 766)\t1.0\n",
      "  (1827, 1190)\t1.0\n",
      "  (1827, 1732)\t1.0\n",
      "  (1827, 3336)\t1.0\n",
      "  (1827, 4074)\t2.0\n",
      "  (1827, 4139)\t1.0\n",
      "  (1827, 5572)\t1.0\n",
      "  (1827, 5573)\t1.0\n",
      "  (1827, 6684)\t1.0\n",
      "  (1827, 7381)\t1.0\n",
      "  (1827, 9913)\t1.0\n",
      "  (1828, 52)\t2.0\n",
      "  (1828, 3694)\t1.0\n",
      "  (1828, 4313)\t1.0\n",
      "  (1828, 7598)\t1.0\n",
      "  (1828, 9302)\t1.0\n",
      "  (1828, 9422)\t1.0\n",
      "  (1828, 9727)\t1.0\n",
      "[-1, -1, 0, 1, -1, -1, 1, -1, 1, -1, -1, 0, 1, 0, 0, -1, -1, 1, 1, -1, 1, 1, 1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 0, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 0, 1, 0, -1, -1, 1, 0, 0, 1, 1, 1, 0, 1, -1, 1, 0, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 0, 1, -1, 1, 1, 1, 0, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, -1, 1, -1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, -1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, -1, 0, 1, 1, 1, 0, 0, -1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 0, -1, 1, 1, -1, 0, 0, -1, 1, -1, 1, -1, 1, -1, 1, 0, 1, -1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, -1, 0, 1, -1, 1, 1, 1, 1, 1, 0, 0, 1, 0, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 0, 1, 1, -1, 1, 1, 0, 0, 1, 0, 1, 0, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 0, 1, 1, -1, 1, 1, 0, 0, -1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, 0, 1, -1, 1, -1, 1, 1, 0, 1, -1, 1, 1, -1, -1, 1, 1, 0, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, -1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, -1, 0, 0, 0, -1, 1, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, 1, -1, 1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, -1, -1, 1, 0, 1, -1, 0, 1, 0, 0, -1, 0, 1, -1, -1, 0, 0, 0, 0, 1, 0, -1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, -1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, -1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, -1, 0, 1, 1, 1, 0, -1, 1, 1, 1, 1, 0, 1, -1, 1, 1, -1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, -1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, -1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, -1, 0, 0, 0, 0, 0, -1, 1, 1, 1, 0, 0, -1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, -1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, 1, 0, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, -1, 1, 1, -1, 0, 0, 0, 1, 1, 0, 1, -1, -1, 0, 0, 0, 1, 1, 0, -1, 0, 0, 0, 0, 0, 1, 1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, -1, 0, 1, 1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, -1, 1, 0, 1, 0, 1, 0, 1, -1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, -1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, -1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, -1, 1, 0, -1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, -1, 1, 0, -1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, -1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, -1, 1, 0, -1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, -1, 1, 1, 0, 0, 1, -1, 0, 0, 1, 0, 0, 0, -1, 0, 0, 1, -1, -1, -1, -1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, -1, 1, 1, 1, 0, 1, 0, -1, 1, 1, 0, -1, -1, 1, 0, 0, 0, 1, 1, 0, -1, -1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, -1, 0, -1, 1, 0, 1, 1, 0, 1, 0, -1, 1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, -1, -1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, -1, 0, 0, 1, 0, 1, 0, 1, 0, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, -1, -1, 1, 1, -1, -1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, -1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, -1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 0, 1, 1, 0, 0, 1, 1, -1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, -1, -1, -1, 0, 1, 1, -1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, -1, 1, 1, 1, 0, 1, 1, 1, 1, -1, 1, 1, 1, 0, 1, 1, -1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 1, 0, 1, 0, 0, 1, -1, 1, 1, -1, 0, -1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 1, 0, 1, 1, 0, 0, -1, 0, 0, 1, 0, -1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 1, -1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, -1, 0, 1, 1, 1, 1, -1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, -1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, -1, 1, 1, 1, 1, -1, 0, 0, 0, 1, 1, 1, -1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, -1, 1, 0, -1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, -1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, -1, 0, 0, 1, 1, 0, 1, 0, 1, 1, -1, -1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, -1, 0, 0, 0, 1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, -1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, -1, 0, 0, 0, 0, 0, -1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, -1, 1, 1, 1, 1, 1, -1, -1, 0, 0, 1, 1, 0, 0, 0, -1, -1, 1, 1, 1, 0, -1, 1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, 1, 1, 1, -1, 0, 0, 0, 1, 1, -1, 0, 0, 1, 1, -1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, -1, 0, 1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, -1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, -1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, -1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, -1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, -1, 0, -1, 1, 0, -1, 1, 1, 1, 1, -1, 1, 0, 1, 1, -1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 0, 0, 0, 0, 1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 1, 1, 0, -1, 0, 0, 0, 1, -1, 1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 1, -1, 0, 0, 1, -1, 0, -1, 0, -1, 0, -1, -1, 1, -1, 1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 1, -1, 1, -1, -1, -1, -1, -1, 1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, 1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 0, -1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, -1, -1, 1, 1, 1, 0, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, -1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, -1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, -1, -1, 0, -1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, -1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, -1, 0, -1, 1, 0, 0, 0, 1, 1, -1, 1, 1, 0, 1, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, -1, 0, 0, 0, 0, 1, 0, 1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, -1, -1, 1, 0, 0, -1, -1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, -1, 0, 1, 0, 0, 1, -1, -1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, -1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, -1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, -1, 1, 1, -1, -1, 0, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, -1, 0, 0, -1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, -1, -1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, -1, 1, -1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, -1, 1, 1, 1, 1, 0, 1, 0, 1, -1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, -1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, -1, 1, -1, 0, 0, -1, 1, 1, 1, -1, 1, 1, 0, -1, 0, 1, -1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, -1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, -1, 0, 1, -1, 1, 1, -1, 0, -1, -1, 0, 0, 0, 1, 0, 0, -1, -1, 0, 1, 1, 0, 0, -1, -1, 0, 1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 1, 0, 0, 0, 1, 1, -1, 0, 1, 1, 1, 1, 0, 1, 0, -1, 0, 0, -1, -1, 0, 1, -1, 0, -1, -1, 1, 0, 1, 1, 1, -1, -1, 0, 0, 0, -1, -1, 0, 1, 0, 0, 0, -1, 1, 1, 1, -1, 1, 1, 0, 1, -1, 0, 0, 0, 1, 0, 0, -1, 0, 1, 1, 1, 1, 1, -1, 1, -1, 0, 0, -1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, -1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 0, -1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, -1, -1, -1, 0, 0, 0, 1, 0, -1, 0, 0, 1, 1, -1, -1, -1, 1, 0, 0, 0, -1, -1, 0, 1, -1, 0, -1, 0, -1, 1, 0, 0, 1, 1, 0, 0, 0, 1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 1, 0, 0, 1, -1, -1, -1, 0, -1, 1, 1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, -1, -1, -1, 1, 1, 0, 1, -1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, -1, 0, -1, -1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, -1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, -1, 1, 1, 1, -1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, -1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 1, 0, 0, -1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, -1, 0, 0, 1, 1, 1, 0, 1, 1, 1, -1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, -1, 1, 1, -1, -1, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 0, -1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, -1, 0, 1, 0, 1, 0, 0, -1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, -1, 1, -1, 1, 1, 1, 0, -1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, -1, 0, 1, 1, 0, 1, 0, 0, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, -1, -1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 0, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 0, -1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, -1, 0, 1, 0, 1, -1, -1, 0, 1, 0, 1, -1, 1, 0, -1, 0, 1, -1, -1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, -1, -1, 1, 0, 0, -1, 1, 0, 0, -1, 0, 0, 0, -1, 1, 1, -1, -1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, -1, 0, 1, 1, 0, 0, 1, 1, 0, -1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, -1, 0, 0, 1, 1, 1, 1, 1, 1, -1, 0, 1, -1, -1, 0, 1, 1, 0, 0, 0, 1, 1, 1, -1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, 1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 1, 0, 0, -1, 1, -1, -1, -1, 1, -1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, -1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, -1, -1, -1, -1, 1, -1, 1, -1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, -1, 0, -1, 1, 0, -1, -1, 1, 1, 1, 1, 0, 0, 1, -1, 1, 1, 0, 1, -1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, -1, 1, 1, 1, 1, 1, -1, 1, 0, 1, 1, 1, 1, 1, 0, 1, -1, 1, 0, 1, 1, 0, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, -1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, -1, -1, 1, 1, 1, 1, 0, -1, 1, 1, 1, 1, -1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, -1, -1, -1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, -1, 1, 0, 0, 1, 1, 1, 0, -1, 0, 1, 1, -1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, -1, 0, -1, 1, -1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, -1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, -1, 0, 0, -1, 1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 1, 0, 0, -1, 0, 0, 1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, -1, 1, 0, 0, 0, 1, 0, -1, 1, -1, 1, -1, 0, 0, 0, 1, 0, 1, 1, 1, -1, 0, 1, 1, 0, -1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, -1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, -1, -1, 1, -1, 0, 1, 0, -1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 1, 0, -1, 1, 0, 0, 1, -1, 1, 1, 0, 0, 0, 0, 0, 0, 1, -1, 1, 1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, -1, 0, -1, 0, -1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, -1, -1, 1, 1, -1, 1, 0, 1, 1, 0, 0, -1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, -1, 1, 1, 0, 0, 1, 0, 0, 0, 1, -1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, -1, 0, 1, 0, 1, 0, 0, -1, -1, 1, 1, 1, 1, -1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, -1, 1, 1, -1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, -1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, -1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, -1, 0, -1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, -1, 0, 0, 0, 1, 1, -1, -1, -1, 0, 0, 0, 1, 0, 0, 1, -1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, -1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, -1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, -1, -1, 1, 1, 0, 1, 1, 1, 1, 1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 0, 1, 1, 1, 0, -1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, -1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, -1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, -1, 0, -1, 0, 1, 1, 1, 1, 0, -1, 0, 0, -1, 1, 0, -1, 1, -1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, -1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, -1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, -1, 1, 1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, -1, 1, 0, 0, 1, -1, 1, -1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, -1, 0, 1, 1, 1, 1, 0, -1, 0, 0, 1, 0, 0, 0, 0, 1, -1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, -1, -1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, -1, -1, -1, 0, 0, 1, -1, 0, -1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, -1, -1, -1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, -1, -1, 1, 1, -1, 0, 0, -1, 0, 1, 1, 1, 1, 1, -1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, -1, 1, 1, 1, 0, -1, 1, 0, 1, -1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, -1, 0, 0, 0, 1, -1, 0, 0, 0, 0, 1, 1, 0, -1, 1, 0, 0, 0, 1, 0, -1, 1, -1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, -1, 0, 1, 0, 0, 1, -1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, -1, 0, 0, 0, 1, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, 1, -1, -1, -1, 1, 1, 0, 0, 0, 1, -1, 0, 0, 1, 1, 0, -1, 0, 0, 0, 0, 1, -1, 0, 0, 1, -1, -1, 0, -1, 1, 1, 0, 1, 0, 0, -1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 1, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 1, -1, 1, 0, 1, 1, -1, -1, 1, 0, 1, 0, 0, 0, 0, 1, 0, -1, 1, 1, 0, 1, -1, 1, 0, 1, 0, 0, 0, -1, 1, 0, 1, -1, 0, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, -1, 1, 0, 0, 1, -1, 0, 1, -1, 1, 1, 0, 0, 0, -1, 0, 0, 1, 0, -1, 0, 1, 1, 1, -1, 0, -1, 1, 1, 1, 0, 0, 1, -1, -1, -1, 0, 0, 1, -1, 1, 1, 0, 1, 0, 0, 1, 0, 0, -1, 1, 0, 1, 0, 1, 0, -1, 0, 0, 1, 1, -1, 0, 0, 1, 1, 0, -1, 0, 1, 0, 1, 0, -1, -1, 0, 1, -1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, -1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, -1, 0, 0, 0, 0, -1, 1, 0, 0, 0, -1, 0, -1, -1, 1, 0, 0, -1, 1, 1, 1, 0, -1, 1, 0, 1, 0, -1, 1, 1, -1, 1, 1, -1, 1, 0, 0, 0, 0, 1, 0, 0, -1, 1, 0, 1, -1, -1, 1, 1, 0, -1, 0, -1, 0, -1, 1, 0, -1, 0, -1, 0, 1, 1, 1, 0, 1, -1, 0, 0, -1, 0, 1, 0, 1, 0, -1, 1, 1, 1, 1, 1, 0, 1, -1, 0, 0, 0, -1, 0, 1, 1, 0, 0, -1, 0, 1, 0, 0, -1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, -1, -1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, -1, 0, 0, 0, 1, -1, 0, -1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, -1, 0, 0, 1, -1, 1, -1, 0, 1, 0, -1, 1, 1, 0, 0, 1, 1, -1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, -1, 1, -1, 0, -1, 1, 0, 1, 1, 1, 0, 0, 1, -1, 0, -1, 0, 1, 0, 1, -1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, -1, 0, 0, -1, 1, 0, 0, 1, 0, 1, 1, 1, -1, 0, 1, 1, 1, -1, -1, 0, 1, 1, 0, 0, -1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, -1, 0, 0, -1, 0, 1, 1, -1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, -1, 1, -1, -1, 1, 0, 0, 1, -1, 0, -1, 0, 0, 0, 1, 1, 1, -1, -1, 0, 1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, -1, 1, -1, -1, 1, 1, 0, 1, 1, -1, -1, 0, 0, 1, -1, 0, 1, 1, -1, 0, 1, 0, -1, 0, 1, -1, 1, -1, 0, -1, 0, -1, -1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, -1, 0, 0, -1, 1, 1, 0, -1, 1, 0, 0, 0, -1, 1, 0, -1, 0, 0, -1, 1, -1, 0, 0, 1, 1, -1, 0, 0, 1, -1, 0, 0, 1, -1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, -1, 0, 0, 0, -1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 0, 1, -1, 0, 0, -1, 1, -1, 1, 1, 1, -1, 1, 0, 0, 1, 1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 1, -1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, -1, -1, 1, -1, 0, 0, 0, 1, -1, 1, 0, 0, 1, -1, 1, 0, -1, 0, -1, 1, 1, -1, 1, -1, 0, 1, 1, -1, 0, 0, 0, 1, 1, 0, -1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, -1, 1, -1, 1, 1, 0, 1, 0, 1, 0, 0, -1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, -1, 0, 1, -1, 1, 0, 1, 1, 1, 0, 0, -1, 1, -1, 0, 0, 1, 0, -1, 0, 1, 1, 1, 1, 0, -1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, -1, -1, -1, 1, 1, 1, 0, 1, -1, 0, 0, 0, 1, 1, -1, -1, 1, 0, 1, -1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, -1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, -1, 1, 0, 1, 0, 1, -1, 1, 0, 0, 1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 1, 0, 1, 0, 0, 0, -1, -1, 0, 1, -1, -1, 0, -1, 1, 0, -1, 0, 1, 0, 0, 0, 1, 1, 0, -1, 0, -1, 0, 1, 1, 1, 0, -1, 0, 1, -1, -1, 0, 1, 1, 1, 0, 0, -1, 0, 1, -1, 0, 1, 1, 0, 0, 0, 1, 0, 1, -1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, -1, 0, 1, 1, -1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, -1, -1, 0, -1, 0, -1, 0, 1, 0, 0, 0, -1, 0, 0, 1, 0, 0, 0, 1, -1, 0, 0, 0, 1, 1, -1, 1, -1, 1, 0, 1, -1, 1, 0, 0, -1, 0, 0, 1, 1, 0, 1, -1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, -1, 0, 1, -1, 1, 1, 0, 1, -1, -1, 0, 1, -1, 1, -1, 0, 0, 1, 1, -1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, -1, 1, 1, 0, 0, 1, -1, 1, 0, -1, 0, 1, 0, 0, 1, 0, -1, 0, -1, 0, 1, 1, 1, 1, 0, -1, 1, 1, 0, -1, 0, 0, 0, 1, 0, 1, 1, -1, 1, 0, 1, 0, 0, 1, 0, -1, 1, 1, 0, -1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, -1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, -1, 0, 1, 0, -1, 0, 0, -1, 1, 1, 0, 1, 0, -1, 1, -1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, -1, 0, 0, -1, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 1, 1, -1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, -1, 1, 0, 0, -1, 0, 0, 1, 1, 0, 1, -1, 0, 0, -1, -1, 1, 1, 0, 1, 0, 0, 1, 1, -1, -1, 0, 1, 1, -1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, -1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, -1, -1, -1, 0, 1, 1, 1, 0, 0, 1, -1, -1, -1, -1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, -1, 0, 1, 0, 1, 1, 0, 0, -1, 0, -1, 0, -1, 1, -1, -1, 1, 1, 0, -1, 1, 0, -1, 1, 0, 0, 1, -1, -1, 1, 0, 0, 1, 0, 0, 0, -1, 1, 0, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 1, 0, 0, -1, 0, 0, 0, 0, 1, -1, 0, 1, 0, 1, -1, -1, 0, -1, 0, 1, 1, -1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, -1, 0, 0, 0, 1, -1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, -1, 0, 0, 0, 0, 1, 0, 0, -1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, -1, 0, 0, 1, -1, 1, 0, 0, 0, -1, 1, -1, -1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, -1, 1, -1, 0, 0, 0, -1, 0, 1, 0, 1, -1, 1, 1, 0, -1, 0, 0, 1, 1, 0, -1, 1, 0, -1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, -1, -1, 1, 1, 0, 0, 1, 1, -1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, -1, 0, 1, 1, 1, 1, 0, 1, 0, -1, 1, -1, 0, 0, 0, 1, 0, -1, 0, 0, -1, 0, 0, 1, 0, -1, -1, 1, 0, 0, -1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, -1, 1, 1, 1, 0, 0, -1, 0, -1, 0, -1, 1, 1, 0, 0, 0, 0, 1, -1, 1, 1, 1, 0, 1, 0, 0, -1, -1, 0, 1, 1, 0, 1, 1, 1, -1, -1, 1, 1, -1, 1, 0, 1, 1, 0, 0, 1, 0, -1, 0, 1, 1, 1, 0, 1, 0, 0, 1, -1, 0, 0, -1, 1, 0, -1, 1, 1, 1, 1, 0, 0, -1, 1, -1, 0, 0, 0, 1, 1, -1, 0, -1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, -1, -1, 0, 0, 1, 0, 0, -1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, -1, -1, 0, 0, 1, 0, -1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, -1, 1, 1, 0, 0, 0, 1, 1, 1, 0, -1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, -1, 0, -1, 1, 0, 1, 0, -1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 1, 1, -1, 0, 1, -1, 0, 1, 0, 0, 1, -1, 1, 1, 1, 1, 0, 0, 0, 1, -1, 0, 1, 0, 1, 0, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, -1, -1, 0, 0, 1, 0, -1, 1, 0, 0, 1, -1, 0, 1, 0, -1, 0, -1, 1, -1, 0, 1, 0, 1, 1, 1, 0, 1, -1, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 1, 0, 0, 1, 0, 1, 1, 1, 0, -1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, -1, 1, 1, 0, -1, 0, 1, 1, 1, 0, 1, 1, -1, 0, -1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1, 1, 0, -1, -1, 0, 0, 1, -1, -1, 0, -1, 1, 0, 0, 0, -1, 0, -1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, -1, 1, 1, 0, -1, 0, 0, 0, 1, 1, -1, 0, 0, 1, 1, 0, -1, 1, 0, 0, -1, 1, 0, -1, 0, 0, 1, 0, 1, -1, 0, 1, 0, 0, 1, 0, -1, 0, 1, 1, 0, 1, 0, -1, 1, 0, 1, 1, -1, 0, 1, 0, 0, 0, 1, 0, -1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, -1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, -1, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, -1, 1, 1, -1, 1, 0, 0, -1, 0, -1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, -1, 0, 1, 0, 1, 0, 0, -1, 1, 0, 0, 1, -1, 0, 1, 0, -1, 1, 0, 0, 1, -1, 0, 1, 1, 1, 0, -1, 0, -1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, -1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, -1, 0, 1, 0, -1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0, -1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, -1, 1, -1, 0, 1, -1, 1, 1, 0, 0, -1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, -1, -1, 1, 1, 0, -1, 0, 1, -1, 1, -1, 0, -1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, -1, 1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, -1, -1, 1, 0, 1, 0, 0, 0, 1, 0, -1, 1, 0, -1, 0, -1, 0, 0, 1, 0, 0, -1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, -1, -1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, -1, 0, 1, 0, 0, -1, 1, 1, 1, -1, 1, 1, 0, 0, 1, 1, 1, -1, -1, 1, -1, 0, -1, 0, 0, 1, 1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, -1, 0, 1, 1, -1, -1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, -1, 1, 0, 0, 0, -1, 0, -1, 0, 0, 1, 1, 1, 1, 1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 1, 0, -1, -1, -1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, -1, -1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, -1, 0, 0, 1, 1, -1, -1, 0, 1, 1, -1, 1, 0, 1, -1, 0, 0, 0, 0, 0, -1, 1, 0, -1, 1, 0, 1, 1, -1, 0, 1, -1, 0, 0, 0, -1, 1, 1, -1, 0, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 1, 1, -1, 1, 0, 1, -1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 1, 0, 1, 1, 1, 1, -1, 0, 0, 0, 0, -1, 1, 0, 1, 1, 0, 0, 0, -1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, -1, 0, -1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1, 1, -1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, -1, 0, 0, 0, 1, 0, 0, -1, 1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 1, 0, 0, -1, -1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, -1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, -1, 1, -1, 0, 0, 0, 1, 1, -1, 0, 1, 1, 0, 0, 0, 0, 1, -1, 0, 1, 1, 1, 0, 0, 1, 1, -1, 0, 0, 0, 0, 0, 1, 0, 0, -1, -1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, -1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, -1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, -1, 0, -1, 1, 1, 1, 1, 0, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 1, -1, 0, 0, 0, 1, -1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, -1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 1, 1, -1, 0, 1, 0, -1, 0, 0, -1, 0, 0, 1, 1, 0, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 1, -1, -1, 0, -1, 1, 0, -1, 1, 0, 1, 0, -1, -1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, -1, 0, 1, -1, 0, 0, 0, 0, 1, 1, 0, -1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, -1, 0, 0, 1, 0, 1, -1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 1, 0, 0, 1, 0, -1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, -1, 0, 0, 0, 0, 1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 1, -1, 0, 1, 1, 0, 0, 0, 1, -1, 1, -1, 1, 0, 0, 1, 0, 0, -1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1, 0, 1, 0, 1, 0, 0, -1, 1, 1, 0, 1, 1, -1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, 0, 0, 1, 0, 0, 0, 1, 1, -1, 1, 0, 0, -1, 1, 1, 0, 0, 0, 1, 1, -1, 1, 0, 0, -1, 0, -1, 0, -1, 0, 1, -1, 1, 1, 0, 1, 1, 1, 0, 1, -1, 0, 0, 0, 1, 1, -1, 1, 0, 0, 0, -1, 1, 1, 1, 0, 0, 1, -1, 0, -1, 1, -1, -1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, -1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, -1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, -1, 0, -1, 1, 0, 1, 0, 1, 0, 0, -1, 1, 0, 0, 0, -1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, -1, 0, 1, 0, 0, -1, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 1, 1, 0, 0, -1, 1, 0, 1, 1, 1, -1, 0, 0, 0, 1, 0, 1, 1, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, -1, 1, 1, 1, 1, 1, 1, 0, 0, 0, -1, 0, 1, 0, 0, 0, 1, 0, 0, -1, -1, -1, 0, 0, 1, 1, 0, 0, -1, 1, 0, 0, 1, 0, 0, 0, 0, 1, -1, 0, 1, 1, 1, 1, 0, 0, 1, -1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 1, 0, 0, 0, 0, 1, 1, -1, 1, 0, 1, 0, 1, 0, -1, 0, -1, 1, 1, 1, 1, 0, -1, 1, -1, 0, 1, 0, 1, -1, 0, 1, 1, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, -1, -1, 1, -1, 0, 1, 0, 0, 1, 0, -1, 1, -1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, -1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 1, 0, 1, 1, 0, 0, 0, -1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, -1, 0, 1, 1, -1, -1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, -1, 0, 0, 0, 0, 0, 1, -1, 0, -1, 0, -1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, -1, -1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 1, 1, 0, 0, 0, 1, 1, 1, -1, -1, 0, 0, 0, -1, 1, 1, 0, 1, 1, 1, -1, 0, 0, 0, 1, 0, -1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, -1, 1, 0, 1, 0, 0, 1, -1, -1, 0, 0, 0, 0, 0, 1, -1, -1, 1, -1, -1, 0, 0, 1, 0, 0, 0, -1, 0, 0, 1, -1, 0, 0, -1, 1, -1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, -1, 1, 1, 0, 0, 0, -1, 1, 1, 0, -1, 1, 1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 1, 0, 0, 1, 1, 0, 0, 1, -1, 1, 1, 0, 0, 0, -1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 1, 0, 0, 0, 1, -1, 0, 0, 1, 0, 0, 0, 0, 1, -1, 1, -1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, -1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, -1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, -1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, -1, 0, 0, 1, 0, 1, 0, 1, 1, -1, 0, 0, 1, 0, 0, -1, 1, 0, 0, 1, 0, -1, 0, -1, 0, 0, 0, 1, -1, 1, 0, 1, 0, 0, 0, 0, -1, 0, 1, 1, -1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, -1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, -1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, -1, 1, 1, 0, 1, 1, 1, 1, 0, -1, 1, 0, 0, 0, -1, 1, 0, 0, 1, 1, -1, -1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, -1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, -1, 0, 0, 1, -1, 0, 0, 1, 0, 1, -1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, -1, 1, 0, 0, 1, 0, 1, 0, 1, 1, -1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, -1, 0, -1, 1, 0, 0, 0, -1, 1, 0, -1, 0, 1, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 1, -1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, -1, 0, 1, -1, 0, 0, 1, 0, 1, -1, 1, -1, -1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, -1, -1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, -1, 1, 0, -1, -1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, -1, 0, 0, -1, 1, 1, 0, 0, -1, -1, 0, 1, -1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, -1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, -1, 0, 0, 1, 1, 1, -1, -1, 1, 0, 1, 0, -1, -1, 0, 1, 1, 1, -1, 0, 0, 0, 0, 1, 0, -1, 1, 0, 1, 0, 1, -1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, -1, 1, 0, 1, 0, 1, 1, 0, -1, 1, 0, 1, 0, 1, -1, 1, 1, 0, 0, 0, 1, 1, 0, 1, -1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, -1, -1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, -1, -1, 1, 0, 0, 0, 0, 0, 0, 1, 1, -1, 1, -1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, -1, 1, 0, 0, 1, 0, 1, 0, 1, 0, -1, 0, 1, 1, 1, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, 1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, -1, -1, 1, 1, 1, 0, 1, 0, 1, 1, 0, -1, 0, -1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, -1, 1, 1, 0, 0, 0, -1, 1, -1, 0, 0, 1, 0, 0, 0, 1, -1, 0, 0, -1, 0, 0, -1, 0, 1, 0, 1, -1, 1, 1, 0, 1, -1, 0, 1, -1, -1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, -1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, -1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 1, -1, 0, 1, 1, 0, 1, 1, -1, 0, 0, 1, 1, 0, 0, 1, -1, 0, 0, -1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, -1, 1, -1, 0, 0, 1, 0, 0, 0, 0, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, -1, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, 1, 0, 0, -1, 0, 0, -1, 1, 0, 0, 0, -1, 1, 1, -1, 0, 0, 0, 0, 1, -1, 0, 1, 0, -1, 1, 0, 0, 0, 0, -1, 0, 1, 0, -1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, -1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 1, 0, -1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, -1, -1, 0, -1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, -1, 1, 0, 0, 0, 1, -1, 0, -1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, -1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, -1, 0, 1, 1, 0, -1, 0, 1, 0, 1, -1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, -1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, -1, -1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 1, -1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, -1, 1, -1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, -1, 1, 1, 0, 1, 0, 0, 0, 0, 0, -1, 1, 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, -1, 1, 1, 1, 0, 1, 0, -1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, 0, 0, 0, 1, 0, 0, -1, 1, 0, -1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, -1, 1, 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, -1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, -1, 0, 0, 1, 1, -1, 0, -1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, 0, 1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 1, -1, 0, 0, 0, 1, 0, 1, 1, 1, -1, 0, 1, 0, -1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, -1, 0, 0, 1, 0, 0, -1, -1, 0, 0, 0, 0, 1, 1, -1, 1, 0, 0, 1, 0, 1, 1, 1, 1, -1, 0, -1, 0, 0, 0, 1, 0, 0, -1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, -1, -1, 1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, -1, 1, 0, -1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, -1, 0, 1, -1, 1, 1, 0, -1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, -1, 0, -1, 1, 1, 0, 1, 1, 0, -1, -1, 1, 1, -1, 0, 0, 0, 0, 1, 1, 0, 1, -1, 0, 1, -1, 0, 1, 0, 0, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 0, 1, 0, 0, -1, 0, 1, 1, 1, 0, 0, 0, 1, 0, -1, 0, -1, 0, 0, -1, -1, 0, 1, 0, 0, 0, -1, 1, 0, -1, 1, 0, 1, -1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, -1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, -1, 1, 0, 0, 1, 1, 0, 1, 0, -1, 1, -1, 0, 1, 1, -1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, -1, 1, 1, 0, -1, 1, 0, 1, 0, -1, 0, 1, 0, 1, 1, -1, 1, 1, -1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, -1, 0, 0, 1, 1, 0, 1, -1, -1, -1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, -1, 0, -1, -1, -1, -1, 0, 0, 1, 1, -1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0, 1, -1, -1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, -1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, -1, 1, 0, 0, 0, -1, 0, 1, -1, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, -1, 0, -1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, -1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, -1, 0, 1, -1, 0, 1, 1, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1, -1, 0, 1, 1, 1, 0, 1, -1, 0, 0, 1, -1, 1, 1, 0, -1, 1, -1, 0, 0, 1, 1, 0, 0, 0, 1, -1, 1, 1, 0, 0, 0, -1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, -1, 0, 1, -1, 0, 0, 1, -1, 1, -1, 1, -1, 0, 1, 0, 0, -1, 0, 1, 1, -1, 0, -1, 1, -1, 1, 0, 0, 0, 0, -1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, -1, 1, 0, 1, 0, 0, 0, 1, -1, -1, 0, 1, 0, 1, 1, -1, 1, 0, 1, 1, 1, 0, -1, -1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, -1, -1, 0, 0, 0, 0, -1, 0, 1, 0, -1, 1, 1, 1, 1, 0, 0, 0, 1, 0, -1, 1, 0, 1, -1, 1, 1, 1, 1, -1, 0, 1, -1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, -1, 0, 1, 0, 0, 0, 0, -1, 0, 0, 1, 1, 0, 0, 0, 1, 1, -1, -1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, -1, 1, 1, 0, 1, -1, 0, -1, -1, 0, 0, -1, -1, 1, 0, 1, 0, -1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 0, 1, -1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, -1, -1, 0, 1, 1, 1, 0, 1, -1, 0, 0, 0, 1, 0, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, -1, 0, 1, 0, -1, 0, 1, 0, -1, 1, 1, -1, 1, 1, -1, 1, 1, 0, 1, -1, 1, 0, 1, 0, 0, 0, -1, 1, 1, 1, 0, -1, 1, 1, 1, 0, 0, -1, 0, 0, 0, 1, -1, -1, 0, 0, 1, 0, 1, 0, 1, 0, 0, -1, 1, -1, -1, 1, 0, 0, 1, 0, 1, 1, 0, 0, -1, 1, 1, 0, -1, -1, 1, 1, 1, 0, 0, 0, -1, 0, -1, 0, 1, -1, 1, 1, -1, 0, 0, -1, 0, 0, 1, 0, 1, 0, 0, 0, 0, -1, 1, 0, 1, 0, -1, -1, 1, 0, 0, 1, 1, -1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, -1, -1, 1, -1, -1, 1, 1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 1, 0, -1, 0, 0, -1, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, -1, 0, 1, 1, 0, 0, 1, 1, 0, -1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, -1, 1, 1, 0, 0, 0, 0, 0, 1, -1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, -1, 1, 1, 0, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 0, 0, 0, 1, 1, -1, 0, 0, 1, -1, 0, 1, -1, -1, 1, -1, -1, 1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 1, 1, 0, 1, 0, 0, -1, 0, 1, 0, -1, -1, 0, 1, -1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, -1, 1, 0, 1, 1, 1, -1, 1, 1, -1, 0, 1, 1, 0, -1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, -1, 1, 1, 0, 1, 0, 0, 1, 0, 1, -1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, -1, 1, 0, 1, -1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, -1, 1, -1, 1, 1, -1, 0, 1, 1, 1, 0, 0, 0, -1, 0, 1, -1, 1, 0, 0, -1, 1, 1, 1, 1, 1, 0, 1, 1, -1, 1, 0, 1, 1, -1, -1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, -1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, -1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, -1, 1, 1, -1, 1, 1, 0, -1, 1, 1, 1, 0, -1, 0, -1, 1, 0, 1, 0, 1, 1, 0, -1, 1, -1, 1, 0, -1, 0, 1, 1, 1, 0, 1, 1, -1, 1, 1, -1, 0, 1, 1, 1, -1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, -1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, -1, -1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, -1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, -1, 1, 1, 0, 0, 1, 1, 1, 1, -1, 0, -1, 1, 0, 1, 1, -1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, -1, 0, -1, 0, 1, 1, 1, 0, 0, -1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, -1, -1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0, 1, 1, 0, 0, 0, 1, 0, -1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, -1, -1, 1, 0, 1, -1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, -1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, -1, 0, 1, 1, -1, 1, 0, 1, -1, 0, 1, 0, 1, 0, 1, -1, -1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, -1, 0, 1, 0, 0, -1, 1, 1, 0, 0, 1, 1, -1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, -1, 1, 0, 1, 1, 0, -1, 0, 0, -1, 1, 0, 1, 0, 0, 1, 0, 0, -1, 1, 0, 0, 0, 1, -1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, -1, 1, -1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, -1, 1, 1, 1, 0, 0, 1, 0, 1, 0, -1, 1, 0, 0, 0, 0, 0, -1, 1, 1, -1, 1, 1, -1, 0, -1, -1, 1, 1, 0, 0, -1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, -1, -1, 1, 1, 0, 1, -1, -1, 1, 1, 1, 0, -1, 0, -1, 0, 1, 0, 0, 1, 1, 1, 1, 0, -1, 0, 0, 0, 1, 1, -1, 1, -1, -1, -1, 1, 0, 0, 0, 0, 1, -1, 0, -1, 0, -1, 0, 1, -1, -1, 1, 1, 1, -1, 0, 1, 1, 1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, 1, -1, 1, 1, 0, -1, 0, 0, 1, 0, -1, 1, 1, -1, 0, 0, -1, 1, 0, 1, 1, -1, 0, 0, 0, 1, 1, -1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, -1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, -1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, -1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, -1, 0, -1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, -1, 0, 1, 0, 0, 1, 1, 0, 0, -1, 0, 0, 1, 0, -1, 0, 0, 1, 1, 1, 0, 0, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 1, -1, 0, -1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, -1, 1, -1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, -1, 1, -1, 0, 0, 1, 0, 0, -1, 0, -1, 0, 1, 1, -1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, -1, 0, 0, 1, 0, 0, 1, -1, 0, 1, 0, 0, -1, 0, 1, 0, 1, 1, 1, 0, 0, -1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, -1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, -1, -1, 0, 0, 1, 1, 1, 1, -1, 0, 1, 0, 0, 1, -1, 0, 0, 0, 1, 0, -1, 0, -1, 1, 0, -1, 0, 0, 1, 0, -1, 0, -1, 1, 1, 0, 1, 0, 1, -1, 1, 1, 1, 0, 1, 0, 1, 1, -1, 1, 0, 0, 0, 1, -1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, -1, -1, 0, -1, 0, 1, 0, 1, -1, -1, 0, 1, -1, 0, 0, 0, 1, -1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, -1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, -1, -1, 1, 0, -1, 0, -1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, -1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, -1, 1, 1, 0, 1, 1, 1, 1, 0, -1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 1, 1, -1, -1, 0, -1, -1, 1, 1, 0, 0, -1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, -1, 0, 1, 0, 0, 0, 1, 1, -1, -1, -1, 0, -1, 0, 0, 1, 0, -1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, -1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, -1, -1, 0, 1, 0, -1, 0, 1, -1, 1, -1, 1, 0, 0, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 0, -1, -1, -1, 1, -1, -1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, -1, -1, 1, 0, 0, -1, -1, 0, 1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 1, 1, -1, 1, -1, 0, 1, 0, 0, 0, -1, 1, 0, 1, 0, 1, 0, 1, 1, 1, -1, 1, 1, 1, 0, -1, -1, -1, -1, 1, 0, -1, 1, 0, -1, 0, 0, -1, 1, 0, 0, 0, 0, 1, -1, 1, 0, 1, 0, 1, 0, 0, -1, 1, 1, 1, 1, -1, -1, -1, 0, -1, 1, -1, 1, 0, -1, 1, 0, 1, 1, 1, 0, 0, -1, 1, -1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, -1, 0, -1, 0, 0, -1, 1, 1, -1, 0, 0, 0, 1, 0, 0, 1, 1, 1, -1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, -1, -1, 0, -1, -1, 0, 0, 1, 1, 0, 0, 0, -1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, -1, 0, 0, 1, 0, 0, -1, 0, 1, 1, -1, 0, 1, -1, 0, 1, 0, 1, 0, 1, 1, 0, 1, -1, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, -1, 1, 1, 1, 0, -1, 1, 1, -1, 0, 1, 1, 0, 1, -1, -1, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1, -1, 0, 1, -1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, -1, 1, 0, 1, 1, 1, 0, -1, 0, 1, 1, -1, 1, -1, -1, 1, -1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, -1, 0, 0, 0, 0, 1, 1, 1, -1, 1, 1, 1, 0, 0, 0, -1, 0, -1, 1, 1, 1, 1, 1, 0, -1, -1, 0, 1, -1, 1, 0, 1, -1, 1, 0, -1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, -1, 0, 1, 0, 1, 0, 1, -1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, -1, -1, 0, 1, -1, 1, 0, 0, 0, 1, 0, 1, 1, 0, -1, 1, 0, 0, 1, 0, -1, 1, -1, 0, 1, 0, -1, 0, 1, -1, 0, 1, 1, 1, 1, 0, -1, 0, 0, 0, -1, 0, 1, -1, 1, 1, 1, 1, 1, 1, 0, 1, -1, 0, -1, 0, -1, 1, 0, -1, -1, 1, -1, -1, 0, 1, 1, 0, 0, 1, -1, 0, 0, 1, 0, -1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, -1, -1, 1, 1, -1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, -1, 0, 0, 0, 1, 1, 0, 0, 0, 0, -1, 0, 1, 1, 0, -1, -1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, -1, -1, 0, -1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, -1, 0, 1, 0, 1, 1, 0, 1, 1, -1, 1, -1, 1, 1, 0, 1, 1, 1, 1, 0, 0, -1, 1, -1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, -1, 1, 1, 0, -1, 1, 0, 1, -1, 1, 0, 1, 0, 1, 0, 1, 0, -1, -1, 0, -1, 1, -1, 0, 0, -1, 0, 1, 0, 0, 1, 1, 1, 1, 0, -1, 0, 1, 0, -1, 1, 0, 1, 1, 0, 0, 1, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, 1, -1, 0, 1, -1, 0, -1, -1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, -1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, -1, 1, -1, 0, 0, 1, 1, -1, -1, 0, -1, 0, -1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, -1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, -1, 1, 0, 1, 0, 1, 1, 0, -1, 1, 0, -1, 0, 0, 0, -1, 1, 0, -1, 0, 1, -1, -1, 0, 0, 0, 0, 1, 0, 1, -1, -1, 1, -1, -1, 0, -1, 1, 0, -1, 0, 0, 0, 1, 0, 0, -1, -1, 1, 1, 0, 0, 1, 1, 0, -1, -1, 0, -1, 1, 0, 1, -1, 1, 0, 0, -1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, -1, -1, 0, 1, 1, 1, 0, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 0, 0, -1, 1, -1, -1, 1, -1, 1, 0, 1, 0, 1, 0, 1, 0, -1, 0, 1, 0, 1, 1, 0, 1, 1, 0, -1, 1, -1, -1, 1, 1, 0, 1, 0, 0, -1, 1, 1, 0, 1, 1, 1, 0, -1, 0, 0, 0, 1, 0, 1, -1, 1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, -1, -1, 1, 0, 0, 0, 1, -1, 0, 1, 0, 1, 0, 0, -1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 1, 0, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, -1, 0, -1, 0, 1, 0, 0, 0, -1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, -1, 0, 1, 0, 1, -1, 0, 0, 1, 1, 0, 0, 0, 0, -1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, -1, -1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, -1, 0, 0, 0, 0, 1, 0, 1, -1, 1, 0, 0, -1, 0, 1, -1, 0, 1, 1, 1, 0, 0, -1, 1, 1, 0, -1, -1, 1, 1, 1, 0, -1, 0, 0, 1, 1, -1, 0, 1, 1, 0, 0, 1, 1, 1, 0, -1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, -1, 1, 1, 1, -1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, -1, 1, 1, 0, 1, -1, 0, 1, 1, 1, 1, 0, -1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, -1, -1, 1, 0, -1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, -1, 1, -1, 0, 0, 1, 0, 1, -1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, -1, 0, 0, -1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, -1, 0, -1, 0, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 0, 1, -1, 1, -1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, -1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, -1, -1, 1, 0, 0, 0, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, -1, -1, 0, 1, 0, 1, 1, 0, 1, -1, -1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, -1, -1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, -1, -1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, -1, 0, 0, 1, 0, 1, -1, 1, 0, 1, 1, 0, 0, 1, 0, 0, -1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, -1, 1, 0, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, -1, 1, 0, 0, 0, 1, 0, 1, 1, 0, -1, 1, 1, 1, 1, 1, 1, 0, -1, 0, 0, 1, 1, 0, 1, -1, 0, 1, 1, 0, 1, 1, -1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, -1, 0, -1, 0, 0, 0, 0, 1, -1, 1, -1, 0, 0, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, -1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 0, 1, 1, -1, 1, 1, -1, 1, 0, 1, 0, 1, -1, 0, 1, 1, -1, 1, -1, -1, -1, 1, -1, 0, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, -1, 1, 0, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 0, 1, -1, 0, 1, 0, 0, 1, -1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, -1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, -1, -1, 0, 1, 1, 1, 1, 1, 1, 0, -1, 0, 0, 1, -1, 1, 1, -1, 1, -1, 1, 1, 0, 1, 1, -1, 1, 1, 0, 1, -1, 0, 0, -1, 0, 0, 1, 1, 0, 1, 1, -1, 0, 0, 0, 1, 0, 0, 1, 1, -1, 1, -1, 0, 1, 0, -1, 1, 1, 1, 0, 0, -1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, -1, 0, 1, 0, 1, 1, 0, -1, 1, -1, 0, -1, 1, -1, -1, 0, 1, -1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 1, 0, 1, 0, 1, 0, -1, 0, 1, 0, 1, 0, 0, 0, 1, 0, -1, 1, 1, 1, 0, 1, -1, -1, -1, 0, 1, 0, 0, 1, 0, 0, -1, 1, -1, 1, 0, 0, 0, 0, 1, -1, 1, 0, 0, 0, -1, 0, 1, -1, -1, 1, 0, 0, 0, -1, 0, -1, -1, -1, 1, -1, 1, 1, 1, -1, 0, 0, 1, 1, 1, -1, 0, -1, 1, 1, -1, -1, 1, 0, -1, 0, 0, 1, 1, 0, 1, 0, -1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, -1, 0, -1, 0, 1, -1, 1, 0, 1, 0, -1, 1, 1, 0, -1, -1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, -1, 1, 1, -1, -1, 0, 1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, -1, 0, 1, 1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, -1, -1, 0, 0, 1, -1, -1, 1, 0, 0, 0, 0, -1, 1, -1, 0, 0, 1, 0, 0, -1, 1, 1, 0, -1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 1, 1, 0, 1, 0, 0, 0, -1, 1, 1, 1, 0, 0, 0, -1, 0, 1, 0, 0, -1, 0, 0, -1, 0, 1, 1, 0, 1, -1, 0, -1, 0, 1, 1, 0, 1, -1, -1, 1, 0, 0, 0, 1, -1, 1, 1, 1, -1, 0, 1, 0, 0, 1, -1, 1, 1, 0, 0, 0, -1, -1, 0, 0, 0, 1, 1, 1, 1, 0, -1, -1, 0, -1, 1, 1, 0, 1, 1, 0, 1, 1, -1, -1, 1, -1, -1, 0, 0, 0, 1, 0, -1, -1, 0, 0, 1, 1, 1, 1, 0, 0, -1, -1, -1, 1, 1, -1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, -1, 1, -1, 0, 1, 0, -1, 0, 1, 1, -1, 0, -1, 0, 1, 1, 0, 0, -1, 0, 0, -1, 1, 1, -1, 0, 0, 0, 0, 1, 0, -1, 0, 1, 1, -1, 1, -1, 1, 0, 0, 1, 0, -1, 0, -1, 0, 1, 0, 1, 0, -1, 0, 1, 1, 0, 0, -1, -1, 0, 1, -1, 0, -1, 1, 0, 0, 0, 1, 1, 1, -1, -1, 1, -1, 1, 0, 0, 1, -1, 0, 0, 0, 1, 1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, -1, -1, -1, -1, 1, 0, 1, 0, 0, 0, 1, 1, -1, 1, 1, 1, 0, -1, -1, 1, -1, 1, 0, 1, -1, 1, 0, 0, 1, -1, 1, -1, 1, 0, 1, 0, -1, 0, 0, -1, 1, -1, 0, 1, 1, 0, 0, -1, 0, -1, -1, 0, 0, 1, 0, 1, -1, 0, -1, 0, 0, 1, 0, 0, 1, 1, 0, 1, -1, 0, 0, -1, 0, 0, 0, 0, 1, -1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, -1, 0, 1, 0, -1, 0, 0, -1, 1, 0, 0, 0, 0, -1, 0, 1, -1, 0, 1, -1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, -1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, -1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, -1, 0, -1, 1, 1, -1, -1, 0, 0, -1, 0, 1, 1, 0, -1, -1, 1, -1, -1, 1, 0, -1, 0, 1, 0, -1, -1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, -1, 0, 0, 0, -1, -1, 1, 1, 1, 0, 0, 1, 1, 1, -1, -1, 0, 1, 0, -1, 0, 0, 1, 0, 1, -1, 1, -1, 0, 1, -1, 0, -1, 0, 0, -1, 0, 1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 1, -1, 1, 0, -1, -1, -1, 0, -1, 1, -1, 0, 0, 1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 1, 1, 0, -1, 0, 0, 1, -1, 0, 0, -1, 0, 1, -1, -1, 1, 1, -1, 0, 0, 0, 0, 1, 1, -1, 0, 1, 1, 0, -1, -1, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, -1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, -1, 0, 1, 0, -1, 1, -1, 0, 0, 0, 0, 1, -1, 0, 0, 1, 0, 0, 1, -1, 1, 0, 0, 0, 0, -1, -1, 1, 1, 0, -1, 0, -1, 1, 0, -1, 1, 1, 0, 0, 1, -1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, -1, 0, 1, 0, 1, -1, 0, 1, 0, 0, -1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, -1, -1, 0, 0, -1, 1, 0, 0, 0, 1, -1, -1, 0, 0, -1, 1, 1, 0, 0, -1, 1, 0, -1, -1, 1, 1, 0, 1, 1, -1, 0, 1, 0, 1, -1, -1, -1, 0, 0, -1, 0, 1, -1, 0, 1, 0, 0, 0, -1, 0, 0, 0, 1, 1, -1, 1, 0, 1, 0, 1, -1, 1, 0, 1, 0, 1, 1, 1, 1, -1, -1, 0, 1, 1, 0, 0, -1, 1, -1, 0, -1, 1, 1, 0, 0, 0, 0, 0, -1, 1, 0, 1, 1, 1, 0, -1, 0, -1, -1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, -1, 0, 1, 1, 0, 1, 0, -1, 1, -1, 1, 0, 1, 0, -1, 1, 1, -1, 1, -1, 1, 0, 1, 1, -1, 0, 1, 0, 1, 0, 1, -1, 1, 0, -1, 1, -1, 0, 0, 1, 0, 1, 0, -1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, -1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, -1, 0, 0, 0, 1, 1, 1, 0, 0, 1, -1, -1, -1, 0, 1, -1, 0, -1, 1, 1, -1, 1, 0, 1, 0, 1, -1, -1, -1, 1, 0, 0, 1, 1, -1, 1, 0, 0, 1, 1, 0, 0, 0, -1, 1, 1, 1, 1, -1, 0, 0, -1, 1, -1, 0, 0, 1, -1, 0, 0, 1, -1, -1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0, 1, -1, 0, 0, 1, 1, 0, 0, -1, 1, 0, 1, -1, 0, 0, 0, 0, 1, -1, 0, 1, 0, 1, -1, 0, 0, 0, -1, -1, 1, 0, 0, 1, 1, 1, -1, 0, 1, 1, 1, 1, 0, -1, 0, 0, 1, 1, 0, 1, -1, 0, 1, -1, 0, 0, 0, 1, 0, 0, -1, 1, -1, 1, 0, 0, 0, -1, 1, -1, 1, 0, 1, -1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, -1, 0, 1, -1, 0, -1, 1, 0, -1, 1, 0, 1, 0, 0, 0, 0, 0, 1, -1, 0, 1, 0, 0, 1, -1, 1, -1, 0, 1, 0, 0, 0, -1, 1, 0, 0, 0, 1, 1, 0, 0, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, -1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, -1, 0, 0, 1, 0, 0, 1, 1, -1, 0, 1, 0, 1, 0, 1, -1, 0, 1, 0, 0, -1, 1, 1, 0, -1, -1, 1, -1, 1, 0, -1, 0, 0, -1, 0, 1, -1, 1, 0, 1, 1, 0, 1, 1, -1, -1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, -1, -1, 0, 0, 1, 1, 0, 0, 1, -1, 0, -1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, -1, 1, 0, 0, 1, 1, 0, 1, -1, -1, 0, 0, -1, 1, 1, 1, 0, -1, 1, 1, 1, 0, 0, 0, 1, -1, -1, 1, -1, 0, 0, 1, 1, -1, 0, 1, 1, 0, 1, 1, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, -1, 1, 0, 1, 1, -1, 1, 1, -1, 0, 0, 0, 1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, -1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 0, 1, 0, -1, 1, 1, 0, 0, 1, -1, 1, -1, -1, -1, 0, 0, 0, 0, 1, 1, 0, 1, -1, 0, 1, 1, 0, 1, 0, 0, -1, 1, -1, 1, 1, -1, 1, 1, 1, 0, 0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 1, 1, 0, 1, -1, 0, 0, -1, 1, 0, -1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, -1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, -1, 1, 1, 1, 0, 0, 1, 0, 0, 0, -1, -1, 1, 1, 1, 1, 1, -1, 0, -1, 0, 0, 1, 0, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, -1, 1, 0, 0, 0, 1, 1, 0, 0, 0, -1, -1, 1, 1, 0, 1, -1, 0, 0, -1, -1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 1, 0, -1, 0, 1, 0, 0, 0, -1, -1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, -1, 0, 0, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 0, 1, 1, -1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 1, 1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 1, 0, 1, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 1, 0, 1, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 1, 0, 0, 1, -1, 1, 1, 1, -1, 1, 1, 0, 1, -1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, -1, -1, 1, -1, 0, 1, 1, -1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, -1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, -1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, -1, -1, 0, 0, 1, 1, 1, 1, 1, 0, -1, 0, 1, 1, 1, 1, 1, 0, -1, -1, -1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, -1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, -1, 1, 0, 0, 0, 1, 1, 1, 1, -1, 0, 0, -1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, -1, 0, 0, 0, 1, 0, -1, 1, 1, 1, -1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, -1, -1, 0, 0, -1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, -1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, -1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, -1, 1, 0, 0, -1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 0, 1, 0, 1, 1, 0, -1, 0, 0, 0, 0, 1, 1, 0, 1, -1, 0, 0, 0, 1, -1, -1, 0, -1, 1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 1, 0, 0, -1, -1, -1, -1, -1, 1, -1, -1, 1, 0, 0, 0, -1, 0, 1, 0, 0, 1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 1, 0, 1, 0, -1, -1, 1, -1, -1, 0, -1, 0, -1, 1, 0, -1, -1, 1, 1, 0, 0, 0, 0, 0, 1, 0, -1, -1, -1, 1, 0, 0, 0, -1, 0, 1, 0, 0, -1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, -1, 1, -1, 1, 0, 0, 0, 1, 0, 1, -1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, -1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, -1, 1, 1, 0, 1, -1, 0, 1, 1, 1, 1, 0, 0, 0, 0, -1, 0, 1, -1, 0, 0, -1, -1, -1, -1, 1, 0, 1, -1, 1, 0, 0, 1, -1, -1, 0, 0, -1, -1, 1, 0, -1, -1, 1, 0, 0, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, 0, -1, 0, -1, 1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, 0, -1, 1, 0, 0, -1, 0, 0, 1, 1, 1, 1, 0, 0, -1, 1, -1, 1, -1, 0, 1, -1, 1, 0, 0, 0, 1, 0, 0, -1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, -1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, -1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, -1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, -1, -1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, -1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, -1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, -1, 1, 0, -1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, -1, -1, 0, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, 1, -1, -1, -1, -1, -1, 0, 0, 1, 1, -1, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, 1, -1, 1, -1, 0, 0, -1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, -1, 1, 1, 0, 0, 0, 0, 0, -1, 1, 1, 0, 0, 0, -1, 1, 0, 0, 0, -1, 1, -1, 0, -1, 0, 1, 1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 1, 1, 0, 0, -1, -1, 0, -1, 1, 0, 0, 1, -1, -1, 0, 0, -1, -1, -1, 1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, -1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, -1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, -1, 1, 1, 1, -1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, -1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 0, 0, -1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, -1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, -1, 1, 0, 0, 0, -1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# to preprocess training without lemmatizing the final tokens\n",
    "trainTweets,trainLabels = preprocess_file_no_lemma\\\n",
    "(r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\train.json\")\n",
    "\n",
    "# to preprocess development without lemmatizing the final tokens\n",
    "devTweets,devLabels = preprocess_file_no_lemma\\\n",
    "(r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\dev.json\")\n",
    "\n",
    "#generate the dictionary of whole training dataset\n",
    "tweetDict = dictionary_tweet(trainTweets)\n",
    "engStop = stopwords.words('english')\n",
    "\n",
    "#generate the feature dictionary of training dataset\n",
    "trainFea = convert_to_feature_dicts(trainTweets,engStop,2)\n",
    "\n",
    "#generate the feature dictionary of development dataset\n",
    "devFea = convert_to_feature_dicts(devTweets,engStop,0)\n",
    "\n",
    "#convert the data to the sparse representation used for training classifiers.\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorize = DictVectorizer()\n",
    "trainData = vectorize.fit_transform(trainFea)\n",
    "devData = vectorize.transform(devFea)\n",
    "\n",
    "print trainData\n",
    "print devData\n",
    "print trainLabels\n",
    "print devLabels\n",
    "\n",
    "##########################################testing##########################################\n",
    "print trainData\n",
    "print devData\n",
    "print trainLabels\n",
    "print devLabels\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "eda5eec23f0d0c446839e65cf5ff9d9c49e89541af3e317737919316"
   },
   "source": [
    "<b>Instructions</b>: Now, tune a decision tree classifier using accuracy in the development set as the evaluation metric. For this, you need to consider at least 2 parameters of the model likely to influence performance and which make sense in this context; you should read the documentation for the classifier on sci-kit learn website to learn what these parameters are. For any binary or categorical parameters, you should just consider all options. For numerical values, you should start by keep other settings on default and just randomly try a wide range, looking for values above which there is a steep drop-off in performance, or, alternatively, no effect on performance at all (you don't need to show this process in the notebook).  Remember that some parameters should be tested on a logarithmic scale. Once you're fairly confident of a good range for the parameter, divide it up into at least 5 steps (but no more than 10), and carry out a grid search, which is to say an exhaustive exploration of all parameter options within the limits you've set (this should be included in the notebook). Identify the best parameter values, and discuss the influence of the parameters on performance in the development set. Do you think some values of the parameters are resulting in overfitting?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "signature": "2194f145208cd9e2de2376d726593e09fd657ebbf64b0aa5e9385fc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the opimized settings are  {'max_features': 0.4, 'max_leaf_nodes': 246, 'criterion': 'entropy', 'class_weight': None} , its accuracy is  0.500820120284\n"
     ]
    }
   ],
   "source": [
    "###########################Testing##########################################\n",
    "#rint type(trainData)\n",
    "#rint repr(trainData.shape)\n",
    "#rint 'The average feature sparsity is {0:.3f}%'.format(  \n",
    "#rainData.nnz/float(trainData.shape[0]*trainData.shape[1])*100);  \n",
    "###########################Testing##########################################\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn import cross_validation\n",
    "import random\n",
    "\n",
    "def do_heldout_validation(classifier,dev,classification):\n",
    "    theAcc = classifier.score(dev,classification)\n",
    "    return theAcc\n",
    "\n",
    "def do_multiple_10foldcrossvalidation(classifier,train,classifications):\n",
    "    predictions = cross_validation.cross_val_predict(classifier,\n",
    "                                                     train,classifications, \n",
    "                                                     cv=10)\n",
    "    print classifier\n",
    "    print \"accuracy\"\n",
    "    print accuracy_score(classifications,predictions)\n",
    "    print classification_report(classifications,predictions)\n",
    "\n",
    "def find_best_nodes(buttom,top,step=1): # for looking for the best best_nodes\n",
    "    theBest = 0\n",
    "    theBestNodes = 0\n",
    "    unchange = 0\n",
    "    down = 0\n",
    "    for i in range(buttom,top,step):\n",
    "        dtc = DecisionTreeClassifier(max_leaf_nodes=i)\n",
    "        dtc.fit(trainData,trainLabels)\n",
    "        a = do_heldout_validation(dtc,devData,devLabels)\n",
    "        #print a\n",
    "        #print \"nodes number is \",i\n",
    "        if a > theBest:\n",
    "            theBest = a\n",
    "            theBestNodes = i\n",
    "            unchange = 0\n",
    "            down = 0 #reset the continuous performance down time\n",
    "            print \"testing \",i,\" \",a, \"performance up, keep testing, the current best is \", theBest,\" \",theBestNodes\n",
    "        elif a == theBest:\n",
    "            unchange = unchange + 1\n",
    "            down = 0 #reset the continuous performance down time\n",
    "            print \"testing \",i,\" \",a,\"unchange, keep tesing\"\n",
    "            if unchange > 50:\n",
    "                return theBestNodes\n",
    "                print \"testing \",i,\" \",a,\"no effect, the Best is \", theBest,\" nodes number is \",theBestNodes         \n",
    "        #elif theprev - a > 0.02:\n",
    "        #    theBest = theprev\n",
    "        #    print \"performance down, the Best is \", theBest,\" nodes number is \",theBestNodes\n",
    "        #    return theBestNodes  \n",
    "        elif theBest > a:\n",
    "            if down > 50:\n",
    "                print \"testing \",i,\" \",a,\"performance keep going down, the Best is \", theBest,\" nodes number is \",theBestNodes\n",
    "                return theBestNodes\n",
    "            print \"testing \",i,\" \",a,\"performance slight down, the Best is still \",theBestNodes,\" \",theBest,\" keep testing\"\n",
    "            down = down + 1\n",
    "            unchange = 0\n",
    "    return theBestNodes\n",
    "\n",
    "def find_best_max_features(buttom,top,step=0.1): # for looking for the best max_features\n",
    "    theBest = 0\n",
    "    theBestFea = 0\n",
    "    unchange = 0\n",
    "    down = 0\n",
    "    i = buttom\n",
    "    while i <= top :\n",
    "    #for i in range(buttom,top,step):\n",
    "        dtc = DecisionTreeClassifier(max_features=i)\n",
    "        dtc.fit(trainData,trainLabels)\n",
    "        a = do_heldout_validation(dtc,devData,devLabels)\n",
    "        #print a\n",
    "        #print \"nodes number is \",i\n",
    "        if a > theBest:\n",
    "            theBest = a\n",
    "            theBestFea = i\n",
    "            unchange = 0\n",
    "            down = 0 #reset the continuous performance down time\n",
    "            print \"testing \", i , \" performance up, keep testing,the current best is \", theBest,\" \",theBestFea\n",
    "        elif a == theBest:\n",
    "            unchange = unchange + 1\n",
    "            down = 0 #reset the continuous performance down time\n",
    "            print \"testing \",i,\"unchange, keep tesing\"\n",
    "            if unchange > 50:\n",
    "                return theBestFea\n",
    "                print \"no effect, the Best is \", \\\n",
    "                theBest,\" nodes number is \",theBestFea         \n",
    "        #elif theprev - a > 0.02:\n",
    "        #    theBest = theprev\n",
    "        #    print \"performance down, the Best is \", theBest,\" nodes number is \",theBestNodes\n",
    "        #    return theBestNodes  \n",
    "        elif theBest > a:\n",
    "            if down > 50:\n",
    "                print \"testing \",i,\"performance keep going down, the Best is \",\\\n",
    "                theBest,\" nodes number is \",theBestFea\n",
    "                return theBestFea\n",
    "            print \"testing \",i,\"performance slight down, the Best is still \",\\\n",
    "            theBestFea,\" \",theBest,\" keep testing\"\n",
    "            down = down + 1\n",
    "            unchange = 0\n",
    "        i = i + step\n",
    "    return theBestFea\n",
    "#do_multiple_10foldcrossvalidation(dtc,trainData,trainLabels)    \n",
    "\n",
    "#find_best_nodes(2,400,1)#this is for looking for the best range, by doing this I \n",
    "#find the best value of max_leaf_nodes which is 146\n",
    "#find_best_nodes(106,187,20)\n",
    "\n",
    "#for i in range(140,150,1):\n",
    "#    dtc = DecisionTreeClassifier(criterion=\"entropy\",max_leaf_nodes=i)\n",
    "#    dtc.fit(trainData,trainLabels)\n",
    "#    a = do_heldout_validation(dtc,devData,devLabels)\n",
    "#    print a\n",
    "#    \n",
    "\n",
    "#find_best_max_features(0.1,1.0,step=0.1)\n",
    "\n",
    "\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "parameters = {'class_weight':['balanced',None],'criterion':['gini','entropy'],\\\n",
    "              'max_leaf_nodes':range(106,287,20),\\\n",
    "              'max_features':[\"auto\",\"sqrt\",\"log2\",None,0.2,0.4,0.6,0.8,1.0]}\n",
    "parameterSets = list(ParameterGrid(parameters))\n",
    "optPara = {}\n",
    "bestAcc = 0\n",
    "for parameterset in parameterSets:   \n",
    "    dtc = DecisionTreeClassifier(criterion=parameterset[\"criterion\"],\\\n",
    "                                 max_leaf_nodes=parameterset[\"max_leaf_nodes\"],\\\n",
    "                                 class_weight=parameterset[\"class_weight\"],\\\n",
    "                                 max_features=parameterset[\"max_features\"])\n",
    "    dtc.fit(trainData,trainLabels)\n",
    "    #print dtc\n",
    "    a = do_heldout_validation(dtc,devData,devLabels)\n",
    "    #print a\n",
    "    if a > bestAcc:\n",
    "        optPara = parameterset\n",
    "        bestAcc = a\n",
    "\n",
    "print \"the opimized settings are \",optPara,\", its accuracy is \",bestAcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################discussion#############################################################\n",
    "# after testing the selected parameters, the optimized parameters settings are:\n",
    "# {'max_features': 'sqrt', 'max_leaf_nodes': 126, 'criterion': 'gini', 'class_weight': None},\n",
    "# its accuracy is  0.498633132859\n",
    "# \n",
    "# as can be seen, since decision tree is not compatible for large feature sets, so\n",
    "# by decreasing the size of feature sets, the performance is improved.\n",
    "#\n",
    "# balanced class_weight does not work very well in this classifer, there is a \n",
    "# significant improvment on performance after setting it to None \n",
    "# \n",
    "# For the criterion, in this case, there is no obvious difference on the their performance.\n",
    "#\n",
    "# Besides, a high max_leaf_nodes values(or set as None) bring a drop-off in\n",
    "# performance. This may because the leaf tend to has little impurity which leads\n",
    "# to a excessive high depth. This can bring overfitting, so the classifer cannot \n",
    "# work very well on develpment data. when setting this to around 126, the classifier\n",
    "# has the best accuracy on dev data. \n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "52946d8356c65488c2593bfa89e652de1a1da61e97b0d05da8e27c3e"
   },
   "source": [
    "<b>Instructions</b>: Carry out the same tuning process with the logistic regression classifier. Compare the performance of the two classifiers to each other, and to the most common class baseline. How are the classifiers doing? Is this a challenging task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false,
    "signature": "dface2f1d6b0cef48f5c8671e5268bd0005c13a14501ba21ae36d0b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the defaut settings are LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) accuracy is  0.505194095134\n",
      "0.490978676873\n",
      "0.490978676873\n",
      "0.487151448879\n",
      "0.487151448879\n",
      "0.498633132859\n",
      "0.498633132859\n",
      "0.501366867141\n",
      "0.501366867141\n",
      "0.495352651722\n",
      "0.494805904866\n",
      "0.494805904866\n",
      "0.495352651722\n",
      "0.504100601422\n",
      "0.504647348278\n",
      "0.506287588846\n",
      "0.506287588846\n",
      "0.496446145435\n",
      "0.495899398578\n",
      "0.492072170585\n",
      "0.491525423729\n",
      "0.506287588846\n",
      "0.506834335703\n",
      "0.504100601422\n",
      "0.503553854565\n",
      "0.491525423729\n",
      "0.493712411154\n",
      "0.493165664297\n",
      "0.493165664297\n",
      "0.502460360853\n",
      "0.505194095134\n",
      "0.504647348278\n",
      "0.504647348278\n",
      "0.49425915801\n",
      "0.492618917441\n",
      "0.493165664297\n",
      "0.490431930016\n",
      "0.505194095134\n",
      "0.505194095134\n",
      "0.50574084199\n",
      "0.507381082559\n",
      "0.492618917441\n",
      "0.493165664297\n",
      "0.490431930016\n",
      "0.490978676873\n",
      "0.503007107709\n",
      "0.503553854565\n",
      "0.505194095134\n",
      "0.50574084199\n",
      "0.493165664297\n",
      "0.494805904866\n",
      "0.485511208311\n",
      "0.488791689448\n",
      "0.501366867141\n",
      "0.503007107709\n",
      "0.499179879716\n",
      "0.503553854565\n",
      "0.491525423729\n",
      "0.491525423729\n",
      "0.486057955167\n",
      "0.484964461454\n",
      "0.502460360853\n",
      "0.504100601422\n",
      "0.498086386003\n",
      "0.499179879716\n",
      "0.489338436304\n",
      "0.489338436304\n",
      "0.487151448879\n",
      "0.484964461454\n",
      "0.501913613997\n",
      "0.504100601422\n",
      "0.498633132859\n",
      "0.496992892291\n",
      "0.488791689448\n",
      "0.491525423729\n",
      "0.487698195735\n",
      "0.483324220886\n",
      "0.501366867141\n",
      "0.503553854565\n",
      "0.499179879716\n",
      "0.498633132859\n",
      "the opimized settings are  {'C': 1.0, 'fit_intercept': False, 'solver': 'lbfgs', 'class_weight': None} , its accuracy is  0.507381082559\n",
      "7021 2581 7203\n",
      "769 360 700\n",
      "1829\n",
      "the accuracy of Zero-R is  0.3827227993439037725533078185\n",
      "the Random Baseline's result is  0.3346309458720612356478950245\n"
     ]
    }
   ],
   "source": [
    "def find_best_C(buttom,top,step=0.1): # for looking for the best C\n",
    "    theBest = 0\n",
    "    theBestC = 0\n",
    "    unchange = 0\n",
    "    down = 0\n",
    "    i = buttom\n",
    "    while i <= top :\n",
    "    #for i in range(buttom,top,step):\n",
    "        lr = LogisticRegression(C=i)\n",
    "        lr.fit(trainData,trainLabels)\n",
    "        a = do_heldout_validation(lr,devData,devLabels)\n",
    "        #print a\n",
    "        #print \"nodes number is \",i\n",
    "        if a > theBest:\n",
    "            theBest = a\n",
    "            theBestC = i\n",
    "            unchange = 0\n",
    "            down = 0 #reset the continuous performance down time\n",
    "            print \"testing \", i , \" performance up, keep testing,the current best is \", theBest,\" \",theBestC\n",
    "        elif a == theBest:\n",
    "            unchange = unchange + 1\n",
    "            down = 0 #reset the continuous performance down time\n",
    "            print \"testing \",i,\"unchange, keep tesing\"\n",
    "            if unchange > 50:\n",
    "                return theBestC\n",
    "                print \"no effect, the Best is \", \\\n",
    "                theBest,\" nodes number is \",theBestC         \n",
    "        #elif theprev - a > 0.02:\n",
    "        #    theBest = theprev\n",
    "        #    print \"performance down, the Best is \", theBest,\" nodes number is \",theBestNodes\n",
    "        #    return theBestNodes  \n",
    "        elif theBest > a:\n",
    "            if down > 50:\n",
    "                print \"testing \",i,\"performance keep going down, the Best is \",\\\n",
    "                theBest,\" nodes number is \",theBestC\n",
    "                return theBestC\n",
    "            print \"testing \",i,\"performance slight down, the Best is still \",\\\n",
    "            theBestC,\" \",theBest,\" keep testing\"\n",
    "            down = down + 1\n",
    "            unchange = 0\n",
    "        i = i + step\n",
    "    return theBestC\n",
    "\n",
    "#print find_best_C(0.1,5.0,step=0.1)#0.2,0.4,0.60,.8,1.0,1.2,1.4,1.6,1.8,2.0\n",
    "\n",
    "\n",
    "def one_R(trn,dev):\n",
    "    train_1 = trn.count(1)\n",
    "    train_N1 = trn.count(-1)\n",
    "    train_0 = trn.count(0)\n",
    "    dev_1 = dev.count(1)\n",
    "    dev_N1 = dev.count(-1)\n",
    "    dev_0 = dev.count(0)\n",
    "    \n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(trainData,trainLabels)\n",
    "#print lr\n",
    "a = do_heldout_validation(lr,devData,devLabels)\n",
    "print \"the defaut settings are\", lr, \"accuracy is \",a\n",
    "\n",
    "#For multiclass problems, only â€˜newton-cgâ€™ and â€˜lbfgsâ€™ handle\n",
    "parameters = {'class_weight':['balanced',None],'solver':['newton-cg','lbfgs'],\\\n",
    "              'fit_intercept':[True,False],'C':[0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8,2.0]}\n",
    "parameterSets = list(ParameterGrid(parameters))\n",
    "optPara = {}\n",
    "bestAcc = 0\n",
    "for parameterset in parameterSets:   \n",
    "    lr = LogisticRegression(class_weight=parameterset[\"class_weight\"],\\\n",
    "                            solver=parameterset[\"solver\"],\\\n",
    "                            fit_intercept=parameterset[\"fit_intercept\"],\\\n",
    "                            C=parameterset[\"C\"])\n",
    "    lr.fit(trainData,trainLabels)\n",
    "    #print lr\n",
    "    a = do_heldout_validation(lr,devData,devLabels)\n",
    "    #print a\n",
    "    if a > bestAcc:\n",
    "        optPara = parameterset\n",
    "        bestAcc = a\n",
    "\n",
    "print \"the opimized settings are \",optPara,\", its accuracy is \",bestAcc\n",
    "\n",
    "\n",
    "##############################Zero-R##############################################################\n",
    "train_1 = trainLabels.count(1)\n",
    "train_N1 = trainLabels.count(-1)\n",
    "train_0 = trainLabels.count(0)\n",
    "dev_1 = devLabels.count(1)\n",
    "dev_N1 = devLabels.count(-1)\n",
    "dev_0 = devLabels.count(0)\n",
    "\n",
    "from decimal import Decimal as D\n",
    "print train_1,train_N1,train_0\n",
    "print dev_1,dev_N1,dev_0\n",
    "print len(devLabels)\n",
    "print \"the accuracy of Zero-R is \", D(dev_0)/D(dev_1+dev_N1+dev_0)\n",
    "###################################################################################################\n",
    "\n",
    "########################################Randdom###################################################\n",
    "import random\n",
    "average=0\n",
    "result = 0\n",
    "for b in range(0,100):\n",
    "    randomLabels = []\n",
    "    for i in devLabels:\n",
    "        randomLabels.append(random.choice([-1,1,0]))\n",
    "    #print randomLabels\n",
    "    n = 0\n",
    "    correct = 0\n",
    "    while n < len(devLabels):\n",
    "        if randomLabels[n] == devLabels[n]:\n",
    "            correct = correct + 1\n",
    "        n = n + 1\n",
    "    result = result + D(correct)/D(len(devLabels))\n",
    "result = D(result)/D(100)\n",
    "print \"the Random Baseline's result is \", result\n",
    "#################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the opimized settings are  {'C': 1.0, 'fit_intercept': False, 'solver': 'lbfgs', 'class_weight': None},\n",
    "its accuracy is  0.507381082559\n",
    "\n",
    "As can be seen, after tunning, the performance of Optimized Logic Regression classifer's\n",
    "performance is slightly better than decision tree. This could due to the fact that, Decision\n",
    "Tree is not suitable for large featureset and easy to overfit. \n",
    "\n",
    "I tested two baseline method, Zero-R gives an accuracy around 0.38 on dev data.\n",
    "Random Baseline's accuracy is nearly 0.33. Both of our two classifier have better\n",
    "performance than the baselines. However, 50% accuracy is not good enough, I think. There\n",
    "are still some ways that can improve our classifier, like lemmatization, feature \n",
    "engineering or trying another algorithm. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "769da6d6ca46818f5fe5f9d8e1b08a68b69c2c9ef58a31f0a749aa0a"
   },
   "source": [
    "<b>Instructions</b>: The next task is a slight detour to test your understanding of the logistic regression classifier: you are going to build your own classifier based on the trained model from sci-kit learn. In particular, you should fill in the MyLogisticRegression class started below which is initialized using the feature weights (coefficients) and constants (intercepts) and list of labels (classes) from the sci-kit learn classifier (see the \"Attributes\" in the documentation for the Logistic Regression classifier), and which mimics the predict and predict_proba methods from the sci-kit learn classifier object. You should confirm that your solution works by using it in the task at hand: take the classifier defined below, train it on the training data, then create an instance of MyLogisticRegression, and show that your classifier has the same output as the scikit-learn classifier for both predict and predict_proba for 5 samples from the development set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false,
    "signature": "7936de499a02e9165bea71933c3dd1841a7617e72ca0f27afb770a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef: [[-0.25404204  0.04003661 -0.49149453 ..., -0.01391867 -0.11786486\n",
      "  -0.01287433]\n",
      " [-0.25658304 -0.35257087  0.02465929 ..., -0.25257791 -0.07290167\n",
      "  -0.05305278]\n",
      " [ 0.51062508  0.31253426  0.46683524 ...,  0.26649657  0.19076653\n",
      "   0.06592711]] intercept [-0.64312619  0.88207545 -0.23894927] [-1  0  1]\n",
      "predict_proba of sci classifier  [[ 0.04932374  0.93254637  0.01812989]\n",
      " [ 0.00196456  0.90571948  0.09231596]\n",
      " [ 0.27709902  0.18689202  0.53600896]\n",
      " [ 0.62031975  0.29402265  0.08565761]\n",
      " [ 0.0415693   0.24154061  0.71689009]]\n",
      "predict of sci classifier  [ 0  0  1 -1  1]\n",
      "1.0\n",
      "my_clf.predict_proba(devData)\n",
      "[[0.049323736321603265, 0.9325463734144912, 0.018129890263905643], [0.0019645591394471863, 0.9057194809448975, 0.09231595991565525], [0.2770990201531034, 0.1868920223432074, 0.5360089575036894], [0.6203197458100703, 0.29402264576574394, 0.08565760842418574], [0.04156929933575407, 0.24154060874178987, 0.716890091922456]]\n",
      "my_clf.predict(devData)\n",
      "[0, 0, 1, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "# import math\n",
    "class MyLogisticRegression:\n",
    "    \n",
    "    def __init__(self, weights, constants, labels):\n",
    "        self.weights = weights\n",
    "        self.constants = constants\n",
    "        self.labels = labels\n",
    "        #pass\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "        probaMatrix = []\n",
    "        j = 0\n",
    "        #print X.get_shape()[0]\n",
    "        while j < X.get_shape()[0]:\n",
    "            #print j\n",
    "            theInstance = X.getrow(j).toarray()[0] #\n",
    "            nonZero = devData.getrow(j).nonzero()[1]\n",
    "            #print theInstance\n",
    "            probaSets = []\n",
    "            \n",
    "            ############calculate the partition function ################################\n",
    "            normalizor = 0\n",
    "            feaScore = 0\n",
    "            feaScores = []\n",
    "            sumfeascore = 0\n",
    "            n= 0\n",
    "            while n < len(self.weights): # n means class\n",
    "                #print \"the class position \",n\n",
    "                #while i < len(theInstance):\n",
    "                for feature in nonZero:\n",
    "                    #feaScore = (theInstance[feature] * self.weights[n][feature]+ self.constants[n])   \n",
    "                    feaScore = (theInstance[feature] * self.weights[n][feature])\n",
    "                    #i = i + 1\n",
    "                    #print \"feascore \",(theInstance[feature] * self.weights[n][feature]+ self.constants[n])\n",
    "                    #print \"feascore \",(theInstance[feature] * self.weights[n][feature])\n",
    "                    feaScores.append(feaScore)\n",
    "                feaScore = 0\n",
    "                #print \"feaScores \",feaScores\n",
    "                for score in feaScores:\n",
    "                    sumfeascore = sumfeascore + score\n",
    "                    #print \"sumfeascore \",sumfeascore \n",
    "                feaScores = []\n",
    "                #normalizor = normalizor + math.exp(sumfeascore)\n",
    "                #normalizor = normalizor + math.exp(sumfeascore)+self.constants[n]\n",
    "                normalizor = normalizor + math.exp(sumfeascore+self.constants[n])\n",
    "                #print \"the normalizor change \",math.exp(sumfeascore+self.constants[n])\n",
    "                sumfeascore = 0\n",
    "                n = n + 1  \n",
    "                \n",
    "            #print \"normalizor is \", normalizor  \n",
    "            ########################################################################################\n",
    "            c = 0    \n",
    "            while c < len(self.weights):\n",
    "                #print len(self.weights)\n",
    "                #print c\n",
    "                #i = 0\n",
    "                proba = 0\n",
    "                #while i < len(theInstance):\n",
    "                for feature in nonZero: \n",
    "                    \n",
    "                    #print feature\n",
    "                    #print self.weights[c][feature]\n",
    "                    #print self.constants[c]\n",
    "                    #proba = proba + (theInstance[feature] * self.weights[c][feature]+ self.constants[c])\n",
    "                    proba = proba + (theInstance[feature] * self.weights[c][feature])\n",
    "                    #i = i + 1         \n",
    "                                     \n",
    "                #print \"current \", math.exp(proba)\n",
    "                #print \"with constant \", (math.exp(proba)+self.constants[c])\n",
    "                #proba =  math.exp(proba)/normalizor\n",
    "                #proba =  (math.exp(proba)+self.constants[c])/normalizor\n",
    "                proba =  math.exp(proba+self.constants[c])/normalizor\n",
    "                #print \"normalized\",proba\n",
    "                #print \"class position\", c \n",
    "                probaSets.append(proba) # probabilites for class c for instance i\n",
    "                c = c + 1        \n",
    "            j = j + 1\n",
    "            probaMatrix.append(probaSets)           \n",
    "            #print probaMatrix\n",
    "        return probaMatrix\n",
    "        #pass\n",
    "    \n",
    "    def predict(self,X):\n",
    "        result = []\n",
    "        theProbArray = MyLogisticRegression.predict_proba(self,X)\n",
    "        for instance in theProbArray:\n",
    "            maxIndex = instance.index(max(instance))\n",
    "        \n",
    "            result.append(self.labels[maxIndex])\n",
    "            \n",
    "        return result\n",
    "        #pass\n",
    "    \n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "# train the classifer here\n",
    "clf.fit(trainData,trainLabels)\n",
    "\n",
    "my_clf = MyLogisticRegression(clf.coef_, clf.intercept_, clf.classes_)\n",
    "\n",
    "print \"coef:\", clf.coef_,\"intercept\" , clf.intercept_, clf.classes_\n",
    "print \"predict_proba of sci classifier \",clf.predict_proba(devData)[0:5]\n",
    "print \"predict of sci classifier \",clf.predict(devData)[0:5]\n",
    "print devData.getrow(0).toarray()[0][60]\n",
    "#print devData.getrow(0).getitem()\n",
    "#print \" \"\n",
    "print \"my_clf.predict_proba(devData)\"\n",
    "print my_clf.predict_proba(devData)[0:5]\n",
    "#print devData\n",
    "print \"my_clf.predict(devData)\"\n",
    "print my_clf.predict(devData)[0:5]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as shown, \n",
    "the first five predict_proba of sci classifier are:\n",
    "[[ 0.04932374  0.93254637  0.01812989]\n",
    " [ 0.00196456  0.90571948  0.09231596]\n",
    " [ 0.27709902  0.18689202  0.53600896]\n",
    " [ 0.62031975  0.29402265  0.08565761]\n",
    " [ 0.0415693   0.24154061  0.71689009]]\n",
    " \n",
    "the first five my_clf.predict_proba are\n",
    "[[0.049323736321603265, 0.9325463734144912, 0.018129890263905643], [0.0019645591394471863, 0.9057194809448975, 0.09231595991565525], [0.2770990201531034, 0.1868920223432074, 0.5360089575036894], [0.6203197458100703, 0.29402264576574394, 0.08565760842418574], [0.04156929933575407, 0.24154060874178987, 0.716890091922456]]\n",
    "\n",
    "predict of sci classifier  [ 0  0  1 -1  1]\n",
    "my_clf.predict             [0, 0, 1, -1, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "a7068418e649164d25ff2f44071be574d8fcb7b6c6212774674f74aa"
   },
   "source": [
    "## Polarity Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "ac04d3aa24c7abfc3e7d7c26087a6900336462a2dbca3eb7fdb662c5"
   },
   "source": [
    "<b>Instructions</b>: Next we will try integrating information from sources beyond the training set, in the form of polarity lexicons. The main focus of this section is producing and evaluating 3 automatically-built polarity lexicons. The first of these lexicons is SentiWordNet, which is <a href=\"http://www.nltk.org/howto/sentiwordnet.html\"> accessible through NLTK</a>. SentiWordNet has precalculated scores for positive, negative, and neutral sentiment for some of the words in WordNet, but, like WordNet, it is arranged in synsets; building a WSD system to handle this is beyond the scope of this assignment, instead you should take the most common polarity across its senses (neutral if there is a tie). Do this by iterating through all the synsets in WordNet (which may take a little while, the code snippet below has a counter to show your progress), and then create two lists, one of positive words, one of negative words. Show 5 examples of each of the positive and negative words, and comment on their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false,
    "signature": "3e4de1e31ec9f85fb26b24ae9f8c1a58f80331f6ad694b8343496043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "positive:  comically  score is  1\n",
      "positive:  spotty  score is  1\n",
      "positive:  couthie  score is  1\n",
      "positive:  in_good_taste  score is  1\n",
      "positive:  fit  score is  2\n",
      "negative:  screaming  score is  -1\n",
      "negative:  grueling  score is  -1\n",
      "negative:  inanimate  score is  -2\n",
      "negative:  unsinkable  score is  -1\n",
      "negative:  stern  score is  -2\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "count = 0\n",
    "positive = []\n",
    "negative = []\n",
    "neutral = []\n",
    "positiveScore = []\n",
    "negativeScore = []\n",
    "neutralScore = []\n",
    "def get_polarity_type(synset_name):\n",
    "    swn_synset =  swn.senti_synset(synset_name)\n",
    "    if not swn_synset:\n",
    "        return None\n",
    "    elif swn_synset.pos_score() > swn_synset.neg_score() and swn_synset.pos_score() > swn_synset.obj_score():\n",
    "        return 1\n",
    "    elif swn_synset.neg_score() > swn_synset.pos_score() and swn_synset.neg_score() > swn_synset.obj_score():\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "words = set([])\n",
    "for synset in wn.all_synsets():\n",
    "    count += 1\n",
    "    p = 0\n",
    "    n = 0\n",
    "    if count % 1000 == 0:\n",
    "        print count\n",
    "    for word in synset.lemma_names():\n",
    "        if word not in words:\n",
    "            words.add(word)\n",
    "            \n",
    "#i = 0           \n",
    "#for word in words:\n",
    "#    print word\n",
    "#    i = i + 1\n",
    "#    if i > 100:\n",
    "#        break\n",
    "    \n",
    "for word in words:\n",
    "    polaritySum = 0\n",
    "    for synset in wn.synsets(word):\n",
    "        polarityScore = get_polarity_type(synset.name())\n",
    "        if polarityScore == 1 or polarityScore == -1:\n",
    "            polaritySum = polaritySum + polarityScore\n",
    "    if polaritySum > 0:\n",
    "        positive.append(word)\n",
    "        positiveScore.append(polaritySum)\n",
    "    elif polaritySum == 0:\n",
    "        neutral.append(word)\n",
    "        neutralScore.append(polaritySum)\n",
    "    elif polaritySum < 0:\n",
    "        negative.append(word)\n",
    "        negativeScore.append(polaritySum)\n",
    "    # count synset polarity for each lemma\n",
    "\n",
    "for i in range(0,5):\n",
    "    print \"positive: \",positive[i],\" score is \",positiveScore[i]\n",
    "for i in range(0,5):\n",
    "    print \"negative: \",negative[i],\" score is \",negativeScore[i]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#the examples are:\n",
    "#positive:  comically,  score is  1     -----> more postive than negative, the quality is decent \n",
    "#positive:  spotty,  score is  1        -----> not very positive, the quality is not good\n",
    "#positive:  couthie,  score is  1       -----> more like a neutral word, the quality is not good\n",
    "#positive:  in_good_taste,  score is  1 -----> obvious a positive word\n",
    "#positive:  fit  score is,  2           -----> it has a higher socre than other positives, and it is definitely a \n",
    "                                              positive word, the quality is good\n",
    "#negative:  screaming,  score is  -1    -----> more negative, it has been correctly assigned\n",
    "#negative:  grueling,  score is  -1     -----> definitely negative\n",
    "#negative:  inanimate,  score is  -2    -----> definitely negative, and its socre is -2 which is reasonable \n",
    "#negative:  unsinkable,  score is  -1   -----> seems not to be a negative word, has not been assigned well\n",
    "#negative:  stern,  score is  -2        -----> more negative\n",
    "\n",
    "the scores overally represent the polarity of word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "a77875d5d504333c50a226d9582ef200f345231ad6b7625e665efc20"
   },
   "source": [
    "<b>Instructions</b>: The second lexicon will be built using the word2vec (CBOW) vectors included in NLTK. For this, you will need a small set of positive and negative seed terms, which are given to you below. Calculate cosine similarity between vectors of the seeds terms and each of the words for which you have vectors (if you use Gensim, you can iterate over model.vocab), flip the sign for the negative seeds, and then average to get a score. Use this score to produce a list of positive and negative words; you should include a threshold of Â±0.03 for words to be considered positive or negative. Again, show 5 examples of each of the positive and negative words, and comment on their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false,
    "signature": "517dc616afa3b67f0d0d6cc605070b86393c30ae6f7dececa106e63e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\AppData\\Roaming\\nltk_data\\models\\word2vec_sample\\pruned.word2vec.txt\n",
      "Poetry  is positive\n",
      "originality  is positive\n",
      "Famed  is positive\n",
      "strictest  is positive\n",
      "Loen  is positive\n",
      "fawn  is negative\n",
      "deferment  is negative\n",
      "Debts  is negative\n",
      "woods  is negative\n",
      "clotted  is negative\n"
     ]
    }
   ],
   "source": [
    "positive_seeds = [\"good\",\"nice\",\"excellent\",\"positive\",\"fortunate\",\"correct\",\"superior\",\"great\"]\n",
    "negative_seeds = [\"bad\",\"nasty\",\"poor\",\"negative\",\"unfortunate\",\"wrong\",\"inferior\",\"awful\"]\n",
    "\n",
    "threshold = 0\n",
    "posList = []\n",
    "negList = []\n",
    "neuList = []\n",
    "\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "from nltk.data import find\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "print word2vec_sample\n",
    "\n",
    "allWordVectors = {}\n",
    "f = open(word2vec_sample,'r') \n",
    "next(f) #skip the first line \n",
    "\n",
    "for line in f:\n",
    "    vector = []\n",
    "    wordAndVector = line.split(\" \")\n",
    "    wordAndVector[-1]= re.sub('\\n','',wordAndVector[-1]) # get rid of \"\\n\" of in the last item\n",
    "    \n",
    "    for element in wordAndVector[1:]:\n",
    "        vector.append(float(element))\n",
    "        \n",
    "    allWordVectors[wordAndVector[0]]=vector\n",
    "\n",
    "words = allWordVectors.keys()\n",
    "#print words\n",
    "#print vector\n",
    "\n",
    "for word in words:\n",
    "    #print word\n",
    "    cosSimPos = 0\n",
    "    cosSimNeg = 0\n",
    "    cosScore = 0 #store the value of cosSimPos-cosSimNeg\n",
    "    wordVector = allWordVectors[word]\n",
    "    #print wordVector\n",
    "    for posSeed in positive_seeds:\n",
    "        posSeedVec = allWordVectors[posSeed]\n",
    "        #print posSeedVec\n",
    "        cosSimPos = cosSimPos + (1.0 - cos_distance(wordVector,posSeedVec))\n",
    "    for negSeed in negative_seeds:\n",
    "        negSeedVec = allWordVectors[negSeed]\n",
    "        cosSimNeg = cosSimNeg + (1.0 - cos_distance(wordVector,negSeedVec))\n",
    "    cosSimPos = cosSimPos/len(positive_seeds) # get the average positive similairy\n",
    "    cosSimNeg = cosSimNeg/len(negative_seeds) # get the average negative similairy\n",
    "    cosScore = cosSimPos - cosSimNeg\n",
    "    #print \"the cosin score is \",cosScore\n",
    "    if cosScore > (threshold+0.03):\n",
    "        posList.append(word)\n",
    "    elif cosScore < (threshold-0.03):\n",
    "        negList.append(word)\n",
    "    else:\n",
    "        neuList.append(word)\n",
    "\n",
    "for i in range(0,5):\n",
    "    print posList[i], \" is positive\"\n",
    "for i in range(0,5):\n",
    "    print negList[i], \" is negative\"    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The examples of positive and negative words are as below: \n",
    "\n",
    "Poetry  is positive            -----> more postive, well assigned\n",
    "originality  is positive       -----> a little more postive, well assigned\n",
    "Famed  is positive             -----> obvouisly postive, well assigned\n",
    "strictest  is positive         -----> a little more postive, well assigned\n",
    "Loen  is positive              -----> more like a neutral word, not well assigned\n",
    "fawn  is negative              -----> not clearly negative, not well assigned\n",
    "deferment  is negative         -----> definite negative, well assigned\n",
    "Debts  is negative             -----> more negative, well assigned\n",
    "woods  is negative             -----> more like a neutral word, not well assigned\n",
    "clotted  is negative           -----> more like a negative word, well assigned \n",
    "Generally this works decently, but still not definie accurate. The threshold can\n",
    "be adjusted for some particular aims(like finding postive words as more as possible),\n",
    "which could bring a better performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "009f213770e2084da694871592b275deb84d42a4e352f4147590a7a8"
   },
   "source": [
    "<b>Instructions</b>: The third lexicon will be built by calculating PPMI with the seed terms. For this, use the Brown corpus included in NLTK, with co-occurrence defined as <em>binary</em> text co-occurrence (that is, multiple co-occurrences in the same text are not counted); importantly, your solution should <em>not</em> calculate the entire co-occurrence matrix, since you only care about relative co-occurrence with the seeds. As above, average the resulting similarity scores after switching the sign for the negative seeds and use them to produce a list of positive and negative words, and check 5 of each. For PPMI, use a threshold of  Â±0.3 for deciding if a word is neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false,
    "signature": "183cecb273ea51094ae7d811e25a42b8721a0e0794174d5207917eb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CategorizedTaggedCorpusReader in u'C:\\\\Users\\\\Ryan\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\brown'>\n",
      "start\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41300\n",
      "41400\n",
      "41500\n",
      "41600\n",
      "41700\n",
      "41800\n",
      "41900\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "45000\n",
      "45100\n",
      "45200\n",
      "45300\n",
      "45400\n",
      "45500\n",
      "45600\n",
      "45700\n",
      "45800\n",
      "45900\n",
      "46000\n",
      "46100\n",
      "46200\n",
      "46300\n",
      "46400\n",
      "46500\n",
      "46600\n",
      "46700\n",
      "46800\n",
      "46900\n",
      "47000\n",
      "47100\n",
      "47200\n",
      "47300\n",
      "47400\n",
      "47500\n",
      "47600\n",
      "47700\n",
      "47800\n",
      "47900\n",
      "48000\n",
      "48100\n",
      "48200\n",
      "48300\n",
      "48400\n",
      "48500\n",
      "48600\n",
      "48700\n",
      "48800\n",
      "48900\n",
      "49000\n",
      "49100\n",
      "49200\n",
      "49300\n",
      "49400\n",
      "49500\n",
      "49600\n",
      "49700\n",
      "49800\n",
      "finish loop!\n",
      "fawn  is positive\n",
      "mid-week  is positive\n",
      "1,800  is positive\n",
      "deferment  is positive\n",
      "askew  is positive\n",
      "clotted  is negative\n",
      "bilharziasis  is negative\n",
      "hanging  is negative\n",
      "localized  is negative\n",
      "orthographies  is negative\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import math\n",
    "\n",
    "threshold = 0\n",
    "posList_PPMI = []\n",
    "negList_PPMI = []\n",
    "neuList_PPMI = []\n",
    "wordList = set([])\n",
    "wordList_slim = set([])\n",
    "wordInFiles = {}\n",
    "wordInFiles_slim = {} #define the files contains seed words\n",
    "features = []\n",
    "fileIdSet = set(brown.fileids())\n",
    "\n",
    "print brown\n",
    "\n",
    "#print brown._fileids\n",
    "#print dir(brown)\n",
    "#print brown.sents()\n",
    "\n",
    "def get_word_list(text):\n",
    "    global wordList\n",
    "    for word in text:\n",
    "        word = word.lower()\n",
    "        if word not in wordList:\n",
    "            wordList.add(word)\n",
    "    return wordList\n",
    "\n",
    "get_word_list(brown.words())\n",
    "#print \"len(wordList) length \", len(wordList)\n",
    "\n",
    "# generating the file dictionary, speed up the program\n",
    "for file_id in brown.fileids():\n",
    "    thewords = set([word.lower() for word in brown.words(file_id)])\n",
    "    wordInFiles[file_id] = thewords\n",
    "        \n",
    "#for file_id in brown.fileids():\n",
    "#    thewords = [word.lower() for word in brown.words(file_id)]\n",
    "#    hasSeed = False\n",
    "#    for posSeed in positive_seeds:\n",
    "#        if posSeed in thewords:\n",
    "#            hasSeed = True\n",
    "#            break\n",
    "#        else:\n",
    "#            for negSeed in negative_seeds:\n",
    "#                if negSeed in thewords:\n",
    "#                    hasSeed = True\n",
    "#                    break\n",
    "#    if hasSeed == True:\n",
    "#        wordInFiles_slim[file_id] = set(thewords)\n",
    "\n",
    "#for word in wordList:\n",
    "#    cocur = False\n",
    "#    for thefilewords in wordInFiles_slim.values():\n",
    "#        if word in thefilewords:\n",
    "#            cocur = True\n",
    "#            break\n",
    "#    if cocur == True:\n",
    "#        wordList_slim.add(word)\n",
    "#print \"wordList_slim length \", len(wordList_slim)    \n",
    "#print \"wordInFiles.keys() \",wordInFiles.keys()\n",
    "#print \"wordInFiles.keys() has been printed\"\n",
    "#print \"wordInFiles_slim.keys() \",wordInFiles_slim.keys()\n",
    "#print \"wordInFiles_slim.keys() has been printed\"\n",
    "\n",
    "def get_PMI_for_brown_binary_vector(word1,word2):\n",
    "    global features\n",
    "    word1_count = 0\n",
    "    word2_count = 0\n",
    "    both_count = 0\n",
    "    total_count = 0.0\n",
    "    word1Index = features.index(word1)\n",
    "    word2Index = features.index(word2)\n",
    "    global file_vectors\n",
    "    \n",
    "    i = 0\n",
    "    while i < file_vectors.get_shape()[0]:\n",
    "    #for file_id in brown.fileids():\n",
    "        #thefile = [word.lower() for word in brown.words(file_id)]        \n",
    "        #thefile = wordInFiles[file_id]\n",
    "        \n",
    "        thefile = file_vectors.getrow(i).toarray()[0]        \n",
    "        total_count += 1\n",
    "        if thefile[word1Index] == 1:\n",
    "        #if word1 in thefile:\n",
    "            word1_count = word1_count + 1\n",
    "            if thefile[word2Index] == 1:\n",
    "            #if word2 in thefile:\n",
    "                both_count += 1\n",
    "                word2_count += 1\n",
    "        elif thefile[word2Index] == 1: \n",
    "            word2_count += 1\n",
    "        i = i + 1\n",
    "    #print word1_count\n",
    "    #print word2_count\n",
    "    #print both_count\n",
    "    #print total_count\n",
    "    #print \"------------------\"\n",
    "    if word1_count == 0 or word2_count == 0 or both_count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return math.log((both_count/total_count)/((word1_count/total_count)*(word2_count/total_count)), 2)\n",
    "            \n",
    "for file_id in fileIdSet:\n",
    "    theWordSet = get_word_list(brown.words(file_id))\n",
    "\n",
    "#print \"the word list is \",wordList\n",
    "def get_PMI_for_brown_binary(word1,word2):\n",
    "    global features\n",
    "    word1_count = 0\n",
    "    word2_count = 0\n",
    "    both_count = 0\n",
    "    total_count = 0.0\n",
    "    \n",
    "    global wordInFiles\n",
    "    global fileIdSet\n",
    "    \n",
    "    i = 0\n",
    "    #while i < file_vectors.get_shape()[0]:\n",
    "    for file_id in fileIdSet:\n",
    "        #thefile = [word.lower() for word in brown.words(file_id)]        \n",
    "        thefile = wordInFiles[file_id]       \n",
    "        total_count += 1\n",
    "        if word1 in thefile:\n",
    "            word1_count = word1_count + 1\n",
    "            if word2 in thefile:\n",
    "                both_count += 1\n",
    "                word2_count += 1\n",
    "        elif word2 in thefile: \n",
    "            word2_count += 1\n",
    "        i = i + 1\n",
    "    #print word1_count\n",
    "    #print word2_count\n",
    "    #print both_count\n",
    "    #print total_count\n",
    "    #print \"------------------\"\n",
    "    if word1_count == 0 or word2_count == 0 or both_count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return math.log((both_count/total_count)/((word1_count/total_count)*(word2_count/total_count)), 2)\n",
    "    \n",
    "def get_BOW_binary(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        word = word.lower()\n",
    "        if word not in BOW.keys():\n",
    "            BOW[word] = 1\n",
    "    return BOW\n",
    "\n",
    "def vectorize_files():\n",
    "    wordInfiles_value = []\n",
    "    for file_id in brown.fileids():\n",
    "        fea_dict = get_BOW_binary(brown.words(file_id))\n",
    "        wordInfiles_value.append(fea_dict)\n",
    "    \n",
    "    #print wordInfiles_value[0]\n",
    "    vectorizer = DictVectorizer()\n",
    "    wordInfiles_vetor = vectorizer.fit_transform(wordInfiles_value)\n",
    "    return wordInfiles_vetor,vectorizer.get_feature_names()\n",
    "\n",
    "file_vectors,features = vectorize_files()\n",
    "featureSet = set(features)\n",
    "#print get_PMI_for_brown_binary('good','seed')\n",
    "#print get_PMI_for_brown_binary('bad','seed')\n",
    "#print get_PMI_for_brown_binary('fuck','seed')\n",
    "#print get_PMI_for_brown_binary('hack','seed')\n",
    "\n",
    "#print file_vectors,features[500:600]\n",
    "\n",
    "print \"start\"\n",
    "timer = 0\n",
    "\n",
    "#for word in featureSet:\n",
    "for word in wordList:\n",
    "    #print word\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    overal = 0 \n",
    "    for posSeed in positive_seeds:\n",
    "        #thePosPMI = get_PMI_for_brown_binary_vector(word,posSeed)\n",
    "        thePosPMI = get_PMI_for_brown_binary(word,posSeed)\n",
    "        if thePosPMI < 0: #PPMI\n",
    "            thePosPMI = 0 \n",
    "        pos = pos + thePosPMI\n",
    "    for negSeed in negative_seeds:\n",
    "        #theNegPMI = get_PMI_for_brown_binary_vector(word,negSeed)\n",
    "        theNegPMI = get_PMI_for_brown_binary(word,negSeed)\n",
    "        if theNegPMI < 0: #PPMI\n",
    "            theNegPMI = 0\n",
    "        neg = neg + theNegPMI\n",
    "    pos = pos/len(positive_seeds) # get the average positive PMIs\n",
    "    neg = neg/len(negative_seeds) # get the average negative PMIs\n",
    "    overal = pos - neg\n",
    "    #print overal\n",
    "    if overal > (threshold+0.03):\n",
    "        posList_PPMI.append(word)\n",
    "    elif overal < (threshold-0.03):\n",
    "        negList_PPMI.append(word)\n",
    "    else:\n",
    "        neuList.append(word)       \n",
    "    timer = timer + 1\n",
    "    \n",
    "    if timer%100 == 0:\n",
    "        print timer\n",
    "        \n",
    "print \"finish loop!\"\n",
    "\n",
    "for i in range(0,5):\n",
    "    print posList_PPMI[i], \" is positive\"\n",
    "for i in range(0,5):\n",
    "    print negList_PPMI[i], \" is negative\"    \n",
    "\n",
    "#i = 0\n",
    "#while i < 5:\n",
    "#    if re.match(r'^\\w+$',posList[i]):\n",
    "#        print posList[i], \" is positive\"\n",
    "#        i = i + 1\n",
    "#j = 0\n",
    "#while j < 5:\n",
    "#    if re.match(r'^\\w+$',negList[i]):\n",
    "#        print negList[j], \" is negative\"\n",
    "#        j = j + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this cell, I tested several methods to speed up the processing. I tried to transfer the files \n",
    "to vector using vectorizor, but the performance is still unsatisfied. Finally, I change every \n",
    "'array' need to be looking up to 'set'. Since set is a hashed structure, the speed become \n",
    "acceptable. \n",
    "\n",
    "The example are as below: \n",
    "\n",
    "fawn  is positive          ----> more like a neutral word, not well assigned\n",
    "mid-week  is positive      ----> from the meaning of the word, I don't see any positive meaning\n",
    "1,800  is positive         ----> pure number, should not be positve, not well assigned\n",
    "deferment  is positive     ----> from the meaning of the word, this is more negative, not well assigned\n",
    "askew  is positive         ----> more negative or neutral rather than positve, not well assigned\n",
    "clotted  is negative       ----> could be negative\n",
    "bilharziasis  is negative  ----> a horrible disease, definitely a negative word, well assigned\n",
    "hanging  is negative       ----> could be a negative word, decently assigned\n",
    "localized  is negative     ----> more like a neutral word, not well assigned\n",
    "orthographies  is negative ----> more likea neutral word, not well assigned\n",
    "\n",
    "as can been seen, this method provides some werid classifications, which seems not to be \n",
    "sensible from the words meaning. But the mutual information depends on the statisticall \n",
    "cocurrency of words, which may reflect some underlying relations between words and polarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "89e9c7d079e855414f8baa9529d1abaefda531243280c0365ba8b6dc"
   },
   "source": [
    "<b>Instructions</b>: Now you will test these automatically-produced lexicons against a manually-annotated set. There is a manually-built lexicon (the Hu and Liu lexicon) which is included with NLTK. It has a list of positive and negative words, which are accessed as below. First, investigate what percentage of the words in the manual lexicon are in each of the automatic lexicons, and then, only for those words which overlap and which are <em>not</em> in the seed set, evaluate the accuracy of with each of the automatic lexicons. Discuss the results, mentioning why you think the lexicon which won out did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false,
    "signature": "a5b4d089c41f290229f44813feb1529f566776360584aca3760bd5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage percentage of the words in the manual lexicon the are in method 1 is: 0.326704963912\n",
      "percentage percentage of the words in the manual lexicon the are in method 2 is: 0.524524966858\n",
      "percentage percentage of the words in the manual lexicon the are in method 3 is: 0.618353218442\n",
      "the accuracy of WrodNet is  0.839691189827\n",
      "the accuracy of CBOW is  0.970380818054\n",
      "the accuracy of PPMI is  0.536806883365\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "positive_words = opinion_lexicon.positive()\n",
    "negative_words = opinion_lexicon.negative()\n",
    "\n",
    "ManPosSet = set(positive_words)\n",
    "ManNegSet = set(negative_words)\n",
    "WordnetSetPos = set(positive)\n",
    "WordnetSetNeg = set(negative)\n",
    "cosSetPos = set(posList)\n",
    "cosSetNeg = set(negList)\n",
    "PPMIPos = set(posList_PPMI)\n",
    "PPMINeg = set(negList_PPMI)\n",
    "\n",
    "mutManWordnet = set([])#store the words in both Manual and Wordnet lexicon\n",
    "mutManCBOW = set([]) #store the words in both Manual and Cos lexicon\n",
    "mutManPPMI = set([]) #store the words in both Manual and PPMI lexicon\n",
    "\n",
    "percInWordnet = 0.0\n",
    "percInCos = 0.0\n",
    "percInPPMI = 0.0\n",
    "\n",
    "posSeeds = set(positive_seeds)\n",
    "negSeeds = set(negative_seeds)\n",
    "\n",
    "for word in ManPosSet:\n",
    "    if (word in WordnetSetPos) or (word in WordnetSetNeg):\n",
    "        mutManWordnet.add(word)\n",
    "for word in ManNegSet:\n",
    "    if (word in WordnetSetPos) or (word in WordnetSetNeg):\n",
    "        mutManWordnet.add(word)\n",
    "perInWordnet = float(len(mutManWordnet))/(len(ManPosSet) + len(ManNegSet))\n",
    "print \"percentage percentage of the words in the manual lexicon the are in method 1 is:\",perInWordnet\n",
    "\n",
    "for word in ManPosSet:\n",
    "    if (word in cosSetPos) or (word in cosSetNeg):\n",
    "        mutManCBOW.add(word)\n",
    "for word in ManNegSet:\n",
    "    if (word in cosSetPos) or (word in cosSetNeg):\n",
    "        mutManCBOW.add(word)\n",
    "percInCos = float(len(mutManCBOW))/(len(ManPosSet) + len(ManNegSet))\n",
    "print \"percentage percentage of the words in the manual lexicon the are in method 2 is:\",percInCos\n",
    "\n",
    "for word in ManPosSet:\n",
    "    if (word in PPMIPos) or (word in PPMINeg):\n",
    "        mutManPPMI.add(word)\n",
    "for word in ManNegSet:\n",
    "    if (word in PPMIPos) or (word in PPMINeg):\n",
    "        mutManPPMI.add(word)\n",
    "percInPPMI = float(len(mutManPPMI))/(len(ManPosSet) + len(ManNegSet))\n",
    "print \"percentage percentage of the words in the manual lexicon the are in method 3 is:\",percInPPMI\n",
    "\n",
    "def cal_accuracy(manualPosSet,manualNegSet,autoPosSet,autoNegSet,mutSet):\n",
    "    global posSeeds\n",
    "    global negSeeds\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    accuracy = 0.0 \n",
    "    \n",
    "    for word in mutSet:\n",
    "        if (word not in posSeeds) and (word not in negSeeds):\n",
    "            if (word in manualPosSet) and (word in autoPosSet):\n",
    "                correct = correct + 1\n",
    "            elif (word in manualNegSet) and (word in autoNegSet):\n",
    "                correct += 1\n",
    "            else: \n",
    "                wrong += 1\n",
    "    \n",
    "    accuracy = float(correct)/(correct + wrong)\n",
    "    return accuracy\n",
    "\n",
    "accuracyWordnet = cal_accuracy(ManPosSet,ManNegSet,WordnetSetPos,WordnetSetNeg,mutManWordnet)\n",
    "print \"the accuracy of WrodNet is \", accuracyWordnet\n",
    "accuracyCos = cal_accuracy(ManPosSet,ManNegSet,cosSetPos,cosSetNeg,mutManCBOW)\n",
    "print \"the accuracy of CBOW is \", accuracyCos\n",
    "accuracyPPMI = cal_accuracy(ManPosSet,ManNegSet,PPMIPos,PPMINeg,mutManPPMI)\n",
    "print \"the accuracy of PPMI is \", accuracyPPMI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "the results are :\n",
    "percentage percentage of the words in the manual lexicon the are in method 1 is: 0.326704963912\n",
    "percentage percentage of the words in the manual lexicon the are in method 2 is: 0.524524966858\n",
    "percentage percentage of the words in the manual lexicon the are in method 3 is: 0.618353218442\n",
    "the accuracy of WrodNet is 0.839691189827\n",
    "the accuracy of CBOW is 0.970380818054\n",
    "the accuracy of PPMI is  0.536806883365\n",
    "\n",
    "As can be seen, the method 2 has be best accuracy. The reason could be that CBOW takes word embedding\n",
    "In CBOW, the context has been represented by several words for a given word. In this case, the output\n",
    "vector is corresponding to context words at input, which bring more sense and better performance for this\n",
    "method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e695da573d5b828a3341834bc868fef80271b1249f05af9de9fe42c7"
   },
   "source": [
    "<b>Instructions</b>: Now you will use the lexicons (both manual and automatic) for the main classification problem. Create a function which calculates a polarity score for a sentence based on a given lexicon (i.e. counting positive and negative words that appear in the tweet, and then returning +1 if there are more positive words, -1 if there are more negative words, and 0 otherwise). Then, use this to compare the results of the different lexicons (please convert them to sets!) on the task in the development set, i.e. the accuracy relative to the human-annotated labels. Do the results reflect the quality of the lexicon as indicated by the earlier analysis? How does it compare to the logistic regression classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false,
    "signature": "f0e8167422be910f7a6501ae29d281005b14ff1a4d8338e8aaaa56dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "lexicon 1 accuracy on development data is  0.399671951886\n",
      "lexicon 2 accuracy on development data is  0.423728813559\n",
      "lexicon 3 accuracy on development data is  0.272826681247\n",
      "manual lexicon accuracy on development data is  0.454346637507\n"
     ]
    }
   ],
   "source": [
    "def cal_polarity_score(sentence,posLexicon,negLexicon):\n",
    "    posCount = 0 \n",
    "    negCount = 0\n",
    "    for word in sentence:\n",
    "        if word in posLexicon:\n",
    "            posCount += 1\n",
    "        elif word in negLexicon:\n",
    "            negCount += 1 \n",
    "    if posCount > negCount:\n",
    "        return 1\n",
    "    elif negCount > posCount:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "###########################testing#######################################    \n",
    "sen = \"I like web search and text analysis\"\n",
    "sen = sen.split(' ')\n",
    "print cal_polarity_score(sen,ManPosSet,ManNegSet)\n",
    "#########################################################################\n",
    "\n",
    "def cal_accuracy_on_dev(devData,devLabel,posLexicon,negLexicon):\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    accuracy = 0.0\n",
    "    for instance in devData:\n",
    "        polarityScore = cal_polarity_score(instance,posLexicon,negLexicon)\n",
    "        if devLabel[devData.index(instance)]==polarityScore:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    accuracy = float(correct)/(correct + wrong)\n",
    "    return accuracy\n",
    "\n",
    "print \"lexicon 1 accuracy on development data is \", cal_accuracy_on_dev(devTweets,devLabels,WordnetSetPos,WordnetSetNeg)\n",
    "print \"lexicon 2 accuracy on development data is \", cal_accuracy_on_dev(devTweets,devLabels,cosSetPos,cosSetNeg)\n",
    "print \"lexicon 3 accuracy on development data is \", cal_accuracy_on_dev(devTweets,devLabels,PPMIPos,PPMINeg)\n",
    "print \"manual lexicon accuracy on development data is \", cal_accuracy_on_dev(devTweets,devLabels,ManPosSet,ManNegSet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The result of testing four lexicons on development data are as below:\n",
    "\n",
    "method 1 accuracy on development data is  0.399671951886\n",
    "method 2 accuracy on development data is  0.423728813559\n",
    "method 3 accuracy on development data is  0.272826681247\n",
    "manual lexicon accuracy on development data is  0.454346637507\n",
    "\n",
    "As shown, the result basically reflect the quality of the lexicon as indicated by the earlier analysis.\n",
    "However, none of them works better than logistic regression classifier. This could due to the face that\n",
    "analysis of natural language is too complicated to get a good result by just using some simple rules(e.g. lexicon),\n",
    "where the statistical machine learning method could have a place!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "2c111f578d8646702c0d73e268e8e6b0f63481d9e58963dc09a2ce8e"
   },
   "source": [
    "<b>Instructions</b>: Now you should investigate the effect of adding the polarity score (or scores) as a feature in your statistical classifier. You should create a new version of your convert_to_feature_dict function (with a different name) to include the extra feature (or features), do not modify the code in that earlier section directly. Retrain your best logistic regression classifier from the early tuning, test on the development set. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false,
    "signature": "1a250c2d0ee5ceb03212560703178ec6ec69c47199f6caff909a5e25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.511755057408\n"
     ]
    }
   ],
   "source": [
    "# use selected lexixon to generate the extra feature\n",
    "def convert_to_feature_dict_lexicon(tweets,remove_stop_words,n,posLexicon,negLexicon):\n",
    "    feature_dicts = []\n",
    "    Polarity = \"\"\n",
    "    for tweet in tweets:\n",
    "        # build feature dictionary for tweet\n",
    "        feature_dict = {}\n",
    "        if cal_polarity_score(tweet,posLexicon,negLexicon) == 1:\n",
    "            polarity = \"POSITIVE\"\n",
    "        elif cal_polarity_score(tweet,posLexicon,negLexicon) == -1:\n",
    "            polarity = \"NEGATIVE\"\n",
    "        else:\n",
    "            polarity = \"NEUTRAL\"\n",
    "\n",
    "        feature_dict = BOW_no_stopwords_no_rarewords(tweet,remove_stop_words,n)\n",
    "        feature_dict[polarity] = 1\n",
    "        feature_dicts.append(feature_dict)\n",
    "        #feature_dicts.append(polarity) \n",
    "        \n",
    "    return feature_dicts\n",
    "\n",
    "# use all lexixons developed earlier to generate the extra features\n",
    "def convert_to_feature_dict_all_lexicons(tweets,remove_stop_words,n):\n",
    "    global ManPosSet\n",
    "    global ManNegSet\n",
    "    global WordnetSetPos \n",
    "    global WordnetSetNeg\n",
    "    global cosSetPos\n",
    "    global cosSetNeg \n",
    "    global PPMIPos\n",
    "    global PPMINeg\n",
    "    PolarityWord = \"\"\n",
    "    PolarityCBOW = \"\"\n",
    "    PolarityMan = \"\"\n",
    "    PolarityPPMI = \"\"\n",
    "    \n",
    "    feature_dicts = []\n",
    "    for tweet in tweets:\n",
    "        # build feature dictionary for tweet      \n",
    "        if cal_polarity_score(tweet,ManPosSet,ManNegSet) == 1:\n",
    "            PolarityMan = \"ManPOSITIVE\"\n",
    "        elif cal_polarity_score(tweet,ManNegSet,ManNegSet) == -1:\n",
    "            PolarityMan = \"ManNEGATIVE\"\n",
    "        else:\n",
    "            PolarityMan = \"ManNEUTRAL\"\n",
    "            \n",
    "        if cal_polarity_score(tweet,WordnetSetPos,WordnetSetNeg) == 1:\n",
    "            PolarityWord = \"WordPOSITIVE\"\n",
    "        elif cal_polarity_score(tweet,WordnetSetPos,WordnetSetNeg) == -1:\n",
    "            PolarityWord = \"WordNEGATIVE\"\n",
    "        else:\n",
    "            PolarityWord = \"WordNEUTRAL\"\n",
    "            \n",
    "        if cal_polarity_score(tweet,cosSetPos,cosSetNeg) == 1:\n",
    "            PolarityCBOW = \"CBOWPOSITIVE\"\n",
    "        elif cal_polarity_score(tweet,cosSetPos,cosSetNeg) == -1:\n",
    "            PolarityCBOW = \"CBOWNEGATIVE\"\n",
    "        else:\n",
    "            PolarityCBOW = \"CBOWNEUTRAL\"\n",
    "        \n",
    "        if cal_polarity_score(tweet,PPMIPos,PPMINeg) == 1:\n",
    "            PolarityPPMI = \"PPMIPOSITIVE\"\n",
    "        elif cal_polarity_score(tweet,PPMIPos,PPMINeg) == -1:\n",
    "            PolarityPPMI = \"PPMINEGATIVE\"\n",
    "        else:\n",
    "            PolarityPPMI = \"PPMINEUTRAL\"\n",
    "            \n",
    "        feature_dict = {}\n",
    "        feature_dict = BOW_no_stopwords_no_rarewords(tweet,remove_stop_words,n)\n",
    "        \n",
    "        feature_dict[PolarityMan] = 1\n",
    "        feature_dict[PolarityWord] = 1\n",
    "        feature_dict[PolarityCBOW] = 1\n",
    "        feature_dict[PolarityPPMI] = 1   \n",
    "        \n",
    "        feature_dicts.append(feature_dict)\n",
    "    return feature_dicts\n",
    "\n",
    "#generate the feature dictionary of training dataset\n",
    "trainFea_lex = convert_to_feature_dict_lexicon(trainTweets,engStop,2,WordnetSetPos,WordnetSetNeg)\n",
    "trainFea_lex_all = convert_to_feature_dict_all_lexicons(trainTweets,engStop,2)\n",
    "#generate the feature dictionary of development dataset\n",
    "devFea_lex = convert_to_feature_dict_lexicon(devTweets,engStop,0,WordnetSetPos,WordnetSetNeg)\n",
    "devFea_lex_all = convert_to_feature_dict_all_lexicons(devTweets,engStop,0)\n",
    "\n",
    "#print devFea_lex_all\n",
    "#convert the data to the sparse representation used for training classifiers.\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorize_lex = DictVectorizer()\n",
    "trainData_lex = vectorize_lex.fit_transform(trainFea_lex)\n",
    "devData_lex = vectorize_lex.transform(devFea_lex)\n",
    "\n",
    "vectorize_lex_all = DictVectorizer()\n",
    "trainData_lex_all = vectorize_lex.fit_transform(trainFea_lex_all)\n",
    "devData_lex_all = vectorize_lex.transform(devFea_lex_all)\n",
    "\n",
    "#print trainData_lex\n",
    "#print devData_lex\n",
    "#print trainLabels\n",
    "#print devLabels\n",
    "\n",
    "######################################################################################################\n",
    "# the opimized settings are  {'C': 1.0, 'fit_intercept': False, 'solver': 'lbfgs', 'class_weight': None},\n",
    "# its accuracy is  0.507381082559\n",
    "######################################################################################################\n",
    "\n",
    "lr_lex = LogisticRegression(class_weight=None,\\\n",
    "                            solver='lbfgs',\\\n",
    "                            fit_intercept=False,\\\n",
    "                            C=1.0)\n",
    "\n",
    "lr_lex.fit(trainData_lex,trainLabels)\n",
    "#lr_lex.fit(trainData_lex_all,trainLabels)\n",
    "#print lr_lex\n",
    "\n",
    "a = do_heldout_validation(lr_lex,devData_lex,devLabels)\n",
    "#a = do_heldout_validation(lr_lex,devData_lex_all,devLabels)\n",
    "print a\n",
    "\n",
    "###############################trying to retune the classifier########################################\n",
    "#def find_best_settings(trainData,trainLabel,devData,devLabel):\n",
    "#    parameters = {'class_weight':['balanced',None],\\\n",
    "#              'solver':['newton-cg','lbfgs','liblinear','sag'],\\\n",
    "#              'fit_intercept':[True,False],\\\n",
    "#              'C':[0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8,2.0]}\n",
    "#    parameterSets = list(ParameterGrid(parameters))\n",
    "#    optPara = {}\n",
    "#    bestAcc = 0\n",
    "#    for parameterset in parameterSets:   \n",
    "#        lr = LogisticRegression(class_weight=parameterset[\"class_weight\"], solver=parameterset[\"solver\"],\\\n",
    "#                                fit_intercept=parameterset[\"fit_intercept\"],C=parameterset[\"C\"])\n",
    "#        lr.fit(trainData,trainLabel)\n",
    "#        print lr\n",
    "#        a = do_heldout_validation(lr,devData,devLabel)\n",
    "#        print a\n",
    "#        if a > bestAcc:\n",
    "#            optPara = parameterset\n",
    "#            bestAcc = a\n",
    "#    return optPara,bestAcc\n",
    "#    print \"the opimized settings are \",optPara,\", its accuracy is \",bestAcc\n",
    "#find_best_settings(trainData_lex_all,trainLabels,devData_lex_all,devLabels)\n",
    "#\n",
    "# results:\n",
    "#({'C': 0.2,\n",
    "#  'class_weight': None,\n",
    "#  'fit_intercept': True,\n",
    "#  'solver': 'newton-cg'},\n",
    "# 0.50683433570256975)\n",
    "###########################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "the previously tested opimized settings are  the opimized settings are  {'C': 1.0, 'fit_intercept': False, \n",
    "'solver': 'lbfgs', 'class_weight': None}, its accuracy is  0.507381082559.\n",
    "In this cell, I tested five ways to add the polarity as extra features: adding polarity from each of the \n",
    "four lexicons and adding all of them. The results are as below: \n",
    "\n",
    "Adding PPMI lexicon : 0.504100601422   <----accuracy\n",
    "Adding CBOW lexicon : 0.503007107709\n",
    "Adding WordNet lexicon : 0.511755057408 <----Best\n",
    "Adding manual lexicon : 0.505194095134\n",
    "Adding all lexicons ï¼š 0.503007107709\n",
    "\n",
    "As shown, adding WordNet lexicon as one of the feature get some improvement on the accuracy. This could because\n",
    "WordNet purely depends on word synsets rather than statistical learning, which is less overlapped with \n",
    "the machine learning feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "ae0d41d972e397ddf4f97d87815a637545e9215b0ea5922533725d66"
   },
   "source": [
    "## Error analysis and improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "949523262e1024c06eec165ba0730ceb403d56beddb41a6ad6c616ee"
   },
   "source": [
    "<b>Instructions</b>: Using your best logistic regression classifier so far, first write a function to identify errors your classifier is making where the probability of the predicted class and the actual class are fairly close (less than 0.2); you're looking for cases which you have a good chance of getting right with a small improvement. For this, do an 80/20 split of  the training dataset (that is, train on 80% of the data, test on 20%); do not look at examples from the development set or the test set. You should print out the tweet, the correct class, the predicted class, and the probabilities. Don't print all the errors, just use random.sample to select 30 from the full set. Look for general patterns in the errors, and propose a reasonable improvement to your classifier that you think might help with a problem that you are seeing. It could involve, for instance, better preprocessing, the addition of new features, some kind of feature selection, better lexicons or better use of the lexicons, or even a post-processing step. It should not require additional data, unless it involves a small set of words that you can hardcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false,
    "signature": "90e87cb5227e9cf32c0f8dfded3ebe4e5ad300c0a25d2aa089bb810f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3361 3361\n",
      "0 2\n",
      "0 1\n",
      "0 1\n",
      "2 0\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "2 0\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "2 0\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "2 0\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 2\n",
      "0 2\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "0 2\n",
      "0 2\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "1 0\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "2 0\n",
      "0 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "0 2\n",
      "0 1\n",
      "0 2\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "1 0\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "1 0\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "0 2\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "1 0\n",
      "2 1\n",
      "2 1\n",
      "2 0\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "1 0\n",
      "1 2\n",
      "1 2\n",
      "2 0\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "1 0\n",
      "0 2\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "0 2\n",
      "0 2\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 2\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "1 0\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "0 2\n",
      "0 2\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "2 0\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "1 0\n",
      "2 0\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "2 0\n",
      "0 1\n",
      "0 2\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "1 0\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "0 2\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "1 0\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "2 0\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "0 2\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "2 0\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "0 2\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "0 1\n",
      "2 0\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "1 0\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "1 0\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "1 0\n",
      "0 1\n",
      "1 0\n",
      "2 0\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "0 2\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "0 2\n",
      "2 1\n",
      "0 2\n",
      "2 1\n",
      "0 1\n",
      "0 2\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "0 2\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "0 2\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "0 2\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "1 0\n",
      "1 0\n",
      "2 1\n",
      "2 0\n",
      "2 1\n",
      "1 0\n",
      "2 0\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "0 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 2\n",
      "2 0\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "2 0\n",
      "2 1\n",
      "2 0\n",
      "1 0\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "1 0\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "0 2\n",
      "0 2\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "1 0\n",
      "1 2\n",
      "2 0\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "2 0\n",
      "2 0\n",
      "1 2\n",
      "1 0\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "0 2\n",
      "0 1\n",
      "1 2\n",
      "0 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "1 0\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "1 0\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "1 0\n",
      "1 0\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "0 2\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "1 0\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "1 0\n",
      "0 2\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "0 2\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "0 2\n",
      "0 1\n",
      "2 1\n",
      "1 0\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "2 0\n",
      "0 2\n",
      "2 1\n",
      "2 0\n",
      "2 0\n",
      "2 1\n",
      "0 2\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "0 2\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "2 0\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "2 0\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "0 2\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 2\n",
      "2 1\n",
      "1 0\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "2 0\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "2 0\n",
      "1 2\n",
      "2 1\n",
      "0 2\n",
      "0 1\n",
      "1 0\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "1 0\n",
      "2 1\n",
      "2 0\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "0 2\n",
      "0 2\n",
      "0 1\n",
      "2 0\n",
      "0 2\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "0 2\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "1 0\n",
      "0 1\n",
      "2 1\n",
      "2 0\n",
      "1 2\n",
      "0 2\n",
      "2 1\n",
      "2 0\n",
      "1 2\n",
      "2 1\n",
      "1 0\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "1 0\n",
      "1 2\n",
      "1 0\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "1 0\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "0 2\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "1 0\n",
      "1 2\n",
      "1 2\n",
      "1 0\n",
      "0 2\n",
      "2 1\n",
      "1 2\n",
      "1 0\n",
      "0 2\n",
      "0 1\n",
      "0 1\n",
      "0 2\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "1 0\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "2 0\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "1 0\n",
      "0 1\n",
      "1 0\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "1 0\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "0 2\n",
      "0 2\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "0 2\n",
      "0 1\n",
      "1 0\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "0 2\n",
      "2 1\n",
      "1 0\n",
      "1 2\n",
      "0 2\n",
      "1 2\n",
      "0 1\n",
      "0 2\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "0 2\n",
      "1 2\n",
      "0 2\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 2\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "0 2\n",
      "0 1\n",
      "1 0\n",
      "1 2\n",
      "2 0\n",
      "0 1\n",
      "1 2\n",
      "2 0\n",
      "0 2\n",
      "1 0\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "0 2\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "0 2\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "1 0\n",
      "2 1\n",
      "1 0\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 2\n",
      "1 2\n",
      "0 1\n",
      "2 0\n",
      "0 2\n",
      "0 2\n",
      "2 1\n",
      "0 1\n",
      "0 2\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "0 2\n",
      "2 1\n",
      "0 2\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "0 2\n",
      "0 1\n",
      "1 0\n",
      "2 1\n",
      "0 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "0 2\n",
      "2 1\n",
      "2 1\n",
      "1 0\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "0 2\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "0 2\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "2 0\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "0 2\n",
      "1 0\n",
      "0 1\n",
      "1 0\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "0 2\n",
      "1 0\n",
      "0 1\n",
      "1 0\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "1 0\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "0 2\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "0 2\n",
      "0 2\n",
      "0 2\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "1 0\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "0 2\n",
      "1 0\n",
      "2 1\n",
      "0 2\n",
      "0 1\n",
      "2 0\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "0 2\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "0 2\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "0 2\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "0 2\n",
      "0 1\n",
      "1 2\n",
      "0 2\n",
      "0 2\n",
      "2 1\n",
      "2 1\n",
      "2 1\n",
      "1 0\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "0 2\n",
      "0 1\n",
      "1 0\n",
      "0 2\n",
      "2 1\n",
      "1 2\n",
      "0 2\n",
      "2 1\n",
      "1 0\n",
      "1 0\n",
      "0 1\n",
      "2 1\n",
      "0 2\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "1 0\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "0 2\n",
      "2 1\n",
      "2 0\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "1 0\n",
      "0 1\n",
      "2 1\n",
      "2 0\n",
      "1 2\n",
      "1 2\n",
      "0 2\n",
      "2 1\n",
      "1 0\n",
      "0 2\n",
      "2 0\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "0 2\n",
      "2 0\n",
      "0 1\n",
      "1 0\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "0 2\n",
      "2 0\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "1 0\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "2 0\n",
      "2 0\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "0 1\n",
      "0 2\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "0 2\n",
      "0 2\n",
      "0 2\n",
      "1 0\n",
      "1 2\n",
      "0 1\n",
      "0 1\n",
      "1 0\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "0 2\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "0 2\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "2 0\n",
      "2 1\n",
      "1 0\n",
      "0 2\n",
      "1 2\n",
      "0 1\n",
      "0 2\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "0 2\n",
      "0 2\n",
      "2 1\n",
      "1 2\n",
      "2 1\n",
      "0 2\n",
      "0 2\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "1 0\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "1 0\n",
      "0 1\n",
      "2 0\n",
      "0 2\n",
      "2 0\n",
      "2 0\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "0 2\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "2 1\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "1 0\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "0 2\n",
      "2 1\n",
      "1 2\n",
      "0 1\n",
      "0 2\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "1 2\n",
      "1 2\n",
      "0 2\n",
      "1 0\n",
      "2 1\n",
      "2 1\n",
      "0 2\n",
      "1 2\n",
      "2 1\n",
      "2 1\n",
      "0 1\n",
      "0 2\n",
      "2 1\n",
      "1 2\n",
      "1 2\n",
      "0 1\n",
      "2 1\n",
      "0 1\n",
      "1 0\n",
      "1 2\n",
      "0 2\n",
      "0 1\n",
      "0 1\n",
      "[{u'text': u\"dear @Microsoft the newOoffice for Mac is great and all, but no Lync update? C'mon.\", u'id': u'628949369883000832', u'label': -1}, {u'text': u\"@Microsoft how about you make a system that doesn't eat my friggin discs. This is the 2nd time this has happened and I am so sick of it!\", u'id': u'628976607420645377', u'label': -1}, {u'text': u'If I make a game as a #windows10 Universal App. Will #xboxone owners be able to download and play it in November? @majornelson @Microsoft', u'id': u'629186282179153920', u'label': 0}, {u'text': u'Microsoft, I may not prefer your gaming branch of business. But, you do make a damn fine operating system. #Windows10 @Microsoft', u'id': u'629226490152914944', u'label': 1}, {u'text': u'@MikeWolf1980 @Microsoft I will be downgrading and let #Windows10 be out for almost the 1st yr b4 trying it again. #Windows10fail', u'id': u'629345637155360768', u'label': -1}, {u'text': u'@Microsoft 2nd computer with same error!!! #Windows10fail Guess we will shelve this until SP1! http://t.co/QCcHlKuy8Q', u'id': u'629394528336637953', u'label': -1}, {u'text': u'Just ordered my 1st ever tablet; @Microsoft Surface Pro 3, i7/8GB 512GB SSD. Hopefully it works out for dev to replace my laptop =)', u'id': u'629650766580609026', u'label': 1}, {u'text': u'After attempting a reinstall, it still bricks, says, \"Windows cannot finish installing,\" or somesuch. @Microsoft may have cost me $600.', u'id': u'629797991826722816', u'label': -1}, {u'text': u'Sunday morning, quiet day so time to welcome in #Windows10 @Microsoft @Windows http://t.co/7VtvAzhWmV', u'id': u'630159517058142208', u'label': 1}, {u'text': u\"Did @Microsoft break Windows 10? Was working fine on Wednesday but now I can't get passed the login screen without it freezing up.\", u'id': u'630542330827771904', u'label': -1}]\n",
      "the tweet is : {u'text': u\"My mums going to Liverpool shopping tomorrow and I'm working. Damn\", u'id': u'628394165', u'label': 0}\n",
      "the predicted class is:  -1  with probability :  0.481318975124\n",
      "the correct class is:  0  with probability :  0.415580879315\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'#Entertaining football at White Hart Lane, just the dosage i needed on a rainy Saturday afternoon.', u'id': u'363293469', u'label': 1}\n",
      "the predicted class is:  0  with probability :  0.571220867733\n",
      "the correct class is:  1  with probability :  0.391794786174\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u\"Pakistan to talk counterterrorism with US, Afghans: Pakistan's foreign minister revealed Thursday that her country would soon hold co...\", u'id': u'182194111', u'label': 0}\n",
      "the predicted class is:  1  with probability :  0.464253487911\n",
      "the correct class is:  0  with probability :  0.438544597814\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'Had to cut the Mohawk off gotta look grown for tomorrow night at Club Pulse', u'id': u'333044984', u'label': 0}\n",
      "the predicted class is:  1  with probability :  0.514697941332\n",
      "the correct class is:  0  with probability :  0.363104230752\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'My Pain may be the reason for somebody,s laugh But My laugh must never be a reason for somebody,s pain #Charlie_Chaplin', u'id': u'420621961', u'label': 0}\n",
      "the predicted class is:  -1  with probability :  0.490494805296\n",
      "the correct class is:  0  with probability :  0.464128006186\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'3 college reps on Wednesday the 26th.  Otterbein, West Liberty, and Akron.  Sign up by 8 AM if interested.', u'id': u'741810102', u'label': 0}\n",
      "the predicted class is:  1  with probability :  0.543168146486\n",
      "the correct class is:  0  with probability :  0.451300353173\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'The sit down with Peyton was gr8..He opens up alot. Tune in Sunday on the Nfl today on CBS.', u'id': u'371539066', u'label': 1}\n",
      "the predicted class is:  0  with probability :  0.525573533747\n",
      "the correct class is:  1  with probability :  0.429338903338\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u\"@rajeevnath you say. Sunday I'll be at the Wikipedia Women's Workshop. Can meet before or after that.\", u'id': u'14411773', u'label': 0}\n",
      "the predicted class is:  1  with probability :  0.541629726232\n",
      "the correct class is:  0  with probability :  0.420352781619\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'@MeeshyHope aha yea it still closed and the truck is still filling... Im in my moms car because were leaving for DC tomorrow and we need gas', u'id': u'296563793', u'label': 0}\n",
      "the predicted class is:  -1  with probability :  0.411413881592\n",
      "the correct class is:  0  with probability :  0.317907884942\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'Yeaaah tomorrow I can download the mew Pretty Little Liars again!! #PPL', u'id': u'26710894', u'label': 1}\n",
      "the predicted class is:  0  with probability :  0.416666098498\n",
      "the correct class is:  1  with probability :  0.370282199644\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'Watching bad reality television with Drew before he leaves for Georgia tomorrow. Bad reality TV...is that redundant? Haha #ILoveMyBestFriend', u'id': u'85985243', u'label': -1}\n",
      "the predicted class is:  1  with probability :  0.535011260137\n",
      "the correct class is:  -1  with probability :  0.453299724838\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u\"8/7/11 Stoke beat Hajduk Split 1-0, 2nd Q Round of the Europa League. The Potters' first game in the competition in 37 years. #thankyouTony\", u'id': u'95182392', u'label': 0}\n",
      "the predicted class is:  1  with probability :  0.44457190017\n",
      "the correct class is:  0  with probability :  0.429283127005\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u\"@SpicyMamacita69 I hate those movies and Devil Inside, and I think it's called 4th kind. All those documentary type.\", u'id': u'349434644', u'label': -1}\n",
      "the predicted class is:  0  with probability :  0.506108545459\n",
      "the correct class is:  -1  with probability :  0.487601552935\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u\"My dads all buying beds for the house in Wisconsin . Like bitch I haven't been there since 6th grade\", u'id': u'333651366', u'label': -1}\n",
      "the predicted class is:  0  with probability :  0.529612211347\n",
      "the correct class is:  -1  with probability :  0.367200630467\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'Laguna Hills Phone FOX Tuesday Comedy Sizzle: Catch the premieres of New Girl, Ben and Kate an... http://t.co/rXlC66ds Telephone Systems', u'id': u'277267176', u'label': 0}\n",
      "the predicted class is:  1  with probability :  0.558630772191\n",
      "the correct class is:  0  with probability :  0.425024642589\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'the city of Houston is buzzing right now // Monday Night Football', u'id': u'140667573', u'label': 1}\n",
      "the predicted class is:  0  with probability :  0.482140575178\n",
      "the correct class is:  1  with probability :  0.468321460012\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'@ScarleyByrne hi , you are inspires me and so may fans !!!! I need you here in Brazil !!!!!! you read the casual vacany ???', u'id': u'381674465', u'label': 1}\n",
      "the predicted class is:  0  with probability :  0.421745452384\n",
      "the correct class is:  1  with probability :  0.40280892647\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u\"I was @PSU when they were unbeaten playing OSU to be in the running for the National Championship. This Saturday's game is bigger. #OneTeam\", u'id': u'24731103', u'label': 1}\n",
      "the predicted class is:  0  with probability :  0.388076295718\n",
      "the correct class is:  1  with probability :  0.371060294072\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'Contest Tomorrow! I will post a local Tucson property that is currently Active in the Tucson MLS. The first person... http://t.co/V55HsKTI', u'id': u'805564027', u'label': 1}\n",
      "the predicted class is:  0  with probability :  0.557980017621\n",
      "the correct class is:  1  with probability :  0.425388241606\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u\"Don't forget tomorrow's Girls Night w/Tia & Tamera 9e/6p, Girlfriend Confidential:L.A. 10e/p & Chelsea Lately with Kristin Chenoweth 11e/8p\", u'id': u'216498618', u'label': 0}\n",
      "the predicted class is:  1  with probability :  0.501731344177\n",
      "the correct class is:  0  with probability :  0.479555152896\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'ive got 1st unit: BK RoStu Maggette CV31 GMonroe & 2nd unit: Bynum Daye Tay Jerebko Drummond i hope JR watchez the Pistons stickz w/ em sum.', u'id': u'200203017', u'label': 0}\n",
      "the predicted class is:  1  with probability :  0.531147847648\n",
      "the correct class is:  0  with probability :  0.443218712982\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'\"@joshuaarodgers: We say we have problems but putting life in to perspective we have such fortunate lives\" you may be Martin Luther King.', u'id': u'855275034', u'label': 1}\n",
      "the predicted class is:  -1  with probability :  0.416311346898\n",
      "the correct class is:  1  with probability :  0.329523815977\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'Chris Jericho brings his hard rockin outfit @FOZZYROCK to the Mod Club Monday!', u'id': u'243809316', u'label': 0}\n",
      "the predicted class is:  1  with probability :  0.508588336862\n",
      "the correct class is:  0  with probability :  0.339534806763\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u\"Getting the kidz ready for Aaliyah's 2nd Birthday Party\", u'id': u'469603246', u'label': 0}\n",
      "the predicted class is:  1  with probability :  0.488295396671\n",
      "the correct class is:  0  with probability :  0.474520136217\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u\"It's all about REPTILES THIS Saturday, November 3rd!  Meet Animal Planet's GATOR BOYS, enjoy live venomous... http://t.co/JurhGwTV\", u'id': u'28168092', u'label': 1}\n",
      "the predicted class is:  0  with probability :  0.528379819785\n",
      "the correct class is:  1  with probability :  0.464223629799\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'Mark your calendars ladies: Luke Bryan will be at the Houston Rodeo March 16th, Then George the 17th!! @sarbeco @A_Festervan @KristenKLee', u'id': u'175281436', u'label': 1}\n",
      "the predicted class is:  0  with probability :  0.554942560179\n",
      "the correct class is:  1  with probability :  0.379128075979\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u'@Wheres_myMOJO stfu wit them excuses...Clippers gonna beat the hell outta the Lakers tomorrow', u'id': u'385047220', u'label': -1}\n",
      "the predicted class is:  1  with probability :  0.364944288245\n",
      "the correct class is:  -1  with probability :  0.328818534918\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u\"Think I'm gonna have to have a MaccyD Breakfast on the ways to Spurs tomorrow ;)\", u'id': u'287772617', u'label': 1}\n",
      "the predicted class is:  0  with probability :  0.486987607142\n",
      "the correct class is:  1  with probability :  0.343957095141\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u\"At chayas on Tuesday me Courtney Callum and Carl all cabbaged on the couch to sleep and Courtney had a spasm cos she dreamt that she'd fell\", u'id': u'434062097', u'label': 0}\n",
      "the predicted class is:  -1  with probability :  0.467364596321\n",
      "the correct class is:  0  with probability :  0.4366184673\n",
      "--------------------------------------------------------------------------------------------------\n",
      "the tweet is : {u'text': u\"@dayaneeb isn't it still due tomorrow? Ah dude I'm like rewriting my Drake song.. Lmao and changing words on it\", u'id': u'764170224', u'label': 1}\n",
      "the predicted class is:  0  with probability :  0.488650539191\n",
      "the correct class is:  1  with probability :  0.35036780574\n",
      "--------------------------------------------------------------------------------------------------\n",
      "5 13 12\n"
     ]
    }
   ],
   "source": [
    "trainFea_Wordnet = convert_to_feature_dict_lexicon(trainTweets,engStop,2,WordnetSetPos,WordnetSetNeg)\n",
    "\n",
    "spliter = int(len(trainFea_Wordnet)*0.8)\n",
    "\n",
    "\n",
    "splitedTrainFea = trainFea_Wordnet[0:spliter]\n",
    "splitedTrainLabels = trainLabels[0:spliter]\n",
    "splitedDevFea = trainFea_Wordnet[spliter:]\n",
    "splitedDevLabels = trainLabels[spliter:]\n",
    "print len(splitedDevFea),len(splitedDevLabels) # testing\n",
    "\n",
    "vectorize_split = DictVectorizer()\n",
    "splitedTrainData = vectorize_split.fit_transform(splitedTrainFea)\n",
    "spliteddevData = vectorize_split.transform(splitedDevFea)\n",
    "\n",
    "def find_slight_error(trainData,trainLabels,devData,devLabels):\n",
    "    lr_findError = LogisticRegression(class_weight=None,solver='lbfgs',fit_intercept=False,C=1.0)\n",
    "    lr_findError.fit(trainData,trainLabels)\n",
    "    probArray = lr_findError.predict_proba(devData)\n",
    "    prediction = lr_findError.predict(devData)\n",
    "    allMinorErrors = []\n",
    "    theIndex = 0\n",
    "    classes = [-1,0,1]\n",
    "    for probSet in probArray:\n",
    "        theMinorError = {}\n",
    "        #theIndex = probArray.index(probSet)\n",
    "        if prediction[theIndex]!=devLabels[theIndex]:\n",
    "            theCorrectLabIndex = classes.index(devLabels[theIndex])\n",
    "            theWrongLabIndex = classes.index(prediction[theIndex])\n",
    "            print theCorrectLabIndex,theWrongLabIndex\n",
    "            diff = probSet[theCorrectLabIndex] - probSet[theWrongLabIndex]\n",
    "            if (diff > -0.2) and (diff < 0.2):\n",
    "                theMinorError[\"tweetIndex\"] = theIndex\n",
    "                theMinorError[\"correctClass\"] = devLabels[theIndex]\n",
    "                theMinorError[\"wrongClass\"] = prediction[theIndex]\n",
    "                theMinorError[\"probForCorrect\"] = probSet[theCorrectLabIndex]\n",
    "                theMinorError[\"probForPrediction\"] = probSet[theWrongLabIndex]\n",
    "        if len(theMinorError) != 0:\n",
    "            allMinorErrors.append(theMinorError)\n",
    "        theIndex += 1\n",
    "    return allMinorErrors\n",
    "\n",
    "allMinorErrors = find_slight_error(splitedTrainData,splitedTrainLabels,spliteddevData,splitedDevLabels)            \n",
    "\n",
    "tweets = []# read the original tweets for analysis\n",
    "f = open(r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\train.json\",'r') \n",
    "for line in f:\n",
    "    tweet_dict = json.loads(line)\n",
    "    tweets.append(tweet_dict)\n",
    "print tweets[0:10]\n",
    "\n",
    "\n",
    "a = 0\n",
    "b = 0\n",
    "c = 0\n",
    "for errors in random.sample(allMinorErrors,30):\n",
    "    #print errors\n",
    "    if errors['wrongClass']==0:\n",
    "        b += 1\n",
    "    elif errors['wrongClass']==1:\n",
    "        c += 1\n",
    "    else:\n",
    "        a += 1\n",
    "    print \"the tweet is :\",tweets[errors['tweetIndex']+spliter]\n",
    "    print \"the predicted class is: \",errors['wrongClass'],\" with probability : \",errors['probForPrediction']\n",
    "    print \"the correct class is: \",errors['correctClass'],\" with probability : \",errors['probForCorrect']\n",
    "    print \"--------------------------------------------------------------------------------------------------\"\n",
    "print a,b,c\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Based on the output results, I find that several tweets have some obviously polar words, but they are still\n",
    "not classified well although the polar word reflect the truth. So it is possible that counting number of polar\n",
    "words is not enough when using polarity as a feature. In the next cell, I am going to test whether taking the\n",
    "score of each polar word into account could improve the accuracy. \n",
    " \n",
    "Besides, I tried several times, more than a half error tweets are classified to a more positive class than their \n",
    "true classes, which means adjust the weight for each class could improve the accuracy of the classifier. \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e7b7cd231c640c8a8c5645a36d77f302e3865f06eeb2e1b5347b8901"
   },
   "source": [
    "<b>Instructions</b>: Now implement that improvement, and then investigate its effect <em>in the development data</em>. Obviously, different improvements may involve different amounts of effort; if your improvement is fairly simple, we expect that you will do a more in-depth analysis, testing possible variations. You can also do multiple related improvements. Students who put extra effort into this may get few extra points that can offset any mistakes on other parts of the assignment, though we do not recommend you spend extra time on this before the other parts of the assignment are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false,
    "signature": "49fd7eb5e4acf25bdba12dc3d233836b08bc207c10e7d13e4f3d08b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "accuracy\n",
      "0.504647348278\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.40      0.35      0.37       360\n",
      "          0       0.45      0.65      0.53       700\n",
      "          1       0.67      0.45      0.54       769\n",
      "\n",
      "avg / total       0.53      0.50      0.50      1829\n",
      "\n",
      "accuracy\n",
      "0.496446145435\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.36      0.28      0.31       360\n",
      "          0       0.44      0.65      0.53       700\n",
      "          1       0.67      0.46      0.54       769\n",
      "\n",
      "avg / total       0.52      0.50      0.49      1829\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49644614543466375"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################testing##############################################################################\n",
    "#string = \"Everything is the same with the Heat LeBron can't play in the 4th Heat\\\n",
    "#fans are still cocky as fuck and Chris Bosh STILL a raptor\"\n",
    "#string = string.lower()\n",
    "#array = string.split(\" \")\n",
    "#for element in array:\n",
    "#    element = element.lower()\n",
    "#print array\n",
    "#print cal_polarity_score(array,WordnetSetPos,WordnetSetNeg)\n",
    "\n",
    "#positive\n",
    "#positiveScore\n",
    "#negative\n",
    "#negativeScore\n",
    "\n",
    "########################try to use polarity score instead of just counting numbers of polar words######################\n",
    "def cal_polarity_score_weight(sentence,posLexicon,negLexicon,posWeight,negWeight):\n",
    "    posCount = 0 \n",
    "    negCount = 0\n",
    "    for word in sentence:\n",
    "        if word in posLexicon:\n",
    "            index = posLexicon.index(word)\n",
    "            posCount += posWeight[index]\n",
    "        elif word in negLexicon:\n",
    "            index = negLexicon.index(word)\n",
    "            negCount += negWeight[index] \n",
    "    #print posCount,negCount\n",
    "    return posCount + negCount\n",
    "\n",
    "print cal_polarity_score_weight(array,positive,negative,positiveScore,negativeScore)\n",
    "\n",
    "def convert_to_feature_dict_lex_Score(tweets,remove_stop_words,n,posLexicon,negLexicon,posWeight,negWeight):\n",
    "    feature_dicts = []\n",
    "    Polarity = \"\"\n",
    "    for tweet in tweets:\n",
    "        # build feature dictionary for tweet\n",
    "        feature_dict = {}\n",
    "        if cal_polarity_score_weight(tweet,posLexicon,negLexicon,posWeight,negWeight) > 0:\n",
    "            polarity = \"POSITIVE\"\n",
    "        elif cal_polarity_score_weight(tweet,posLexicon,negLexicon,posWeight,negWeight) < 0:\n",
    "            polarity = \"NEGATIVE\"\n",
    "        else:\n",
    "            polarity = \"NEUTRAL\"\n",
    "\n",
    "        feature_dict = BOW_no_stopwords_no_rarewords(tweet,remove_stop_words,n)\n",
    "        if polarity == \"NEUTRAL\":\n",
    "            feature_dict[polarity] = 1\n",
    "        else: \n",
    "            feature_dict[polarity] = abs(cal_polarity_score_weight(tweet,posLexicon,negLexicon,posWeight,negWeight))\n",
    "        feature_dicts.append(feature_dict)\n",
    "        #feature_dicts.append(polarity) \n",
    "        \n",
    "    return feature_dicts\n",
    "\n",
    "#trainFea_Wordnet_score = convert_to_feature_dict_lex_Score(trainTweets,engStop,2,positive,negative,positiveScore,negativeScore)\n",
    "#devFea_Wordnet_score = convert_to_feature_dict_lex_Score(devTweets,engStop,0,positive,negative,positiveScore,negativeScore)\n",
    "#vectorize_Wordnet_score = DictVectorizer()\n",
    "#trainData_Wordnet_score = vectorize_Wordnet_score.fit_transform(trainFea_Wordnet_score)\n",
    "#devData_Wordnet_score = vectorize_Wordnet_score.transform(devFea_Wordnet_score)\n",
    "\n",
    "#lr_Wordnet_score = LogisticRegression(class_weight=None,\\\n",
    "#                            solver='lbfgs',\\\n",
    "#                            fit_intercept=False,\\\n",
    "#                            C=1.0)\n",
    "\n",
    "#lr_lex.fit(trainData_lex,trainLabels)\n",
    "#lr_Wordnet_score.fit(trainData_Wordnet_score,trainLabels)\n",
    "#print lr_lex\n",
    "\n",
    "#a = do_heldout_validation(lr_lex,devData_lex,devLabels)\n",
    "#a = do_heldout_validation(lr_Wordnet_score,devData_Wordnet_score,devLabels)\n",
    "#print a\n",
    "#0.502460360853\n",
    "##########################################################################################################################\n",
    "\n",
    "#######################################Try to Lemmatizing tweets##########################################################\n",
    "def classifier_lem(trainPath,devPath):\n",
    "    global tweetDict\n",
    "    trainTweets_lem,trainLabels_lem = preprocess_file(trainPath)\n",
    "    devTweets_lem,devLabels_lem = preprocess_file(devPath)\n",
    "    \n",
    "    tweetDict = dictionary_tweet(trainTweets_lem)\n",
    "    \n",
    "    trainFea_lem = convert_to_feature_dicts(trainTweets_lem,engStop,2)\n",
    "    devFea_lem = convert_to_feature_dicts(devTweets_lem,engStop,0)\n",
    "    \n",
    "    vectorize_lem = DictVectorizer()\n",
    "    trainData_lem = vectorize_lem.fit_transform(trainFea_lem)\n",
    "    devData_lem = vectorize_lem.transform(devFea_lem)\n",
    "    \n",
    "    lr_lem = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "    lr_lem.fit(trainData_lem,trainLabels)\n",
    "    #print lr_lem\n",
    "    a = do_heldout_validation(lr_lem,devData_lem,devLabels)\n",
    "    #print a\n",
    "    \n",
    "    prediction = lr_lem.predict(devData_lem)\n",
    "    truth = devLabels\n",
    "    print \"accuracy\"\n",
    "    print accuracy_score(truth,prediction)\n",
    "    print classification_report(truth,prediction)\n",
    "    \n",
    "classifier_lem(r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\train.json\",\\\n",
    "               r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\dev.json\")\n",
    "#########################################################################################################################\n",
    "\n",
    "\n",
    "#########################################try to modify hashtag processing##################################################\n",
    "def classifier_hashtagsV2(trainPath,devPath):   \n",
    "    global tweetDict\n",
    "    trainTweets_hashtagsV2,trainLabels_hashtagsV2 = preprocess_file_no_lemma_hashtagV2(trainPath)\n",
    "    devTweets_hashtagsV2,devLabels_hashtagsV2 = preprocess_file_no_lemma_hashtagV2(devPath)\n",
    "    \n",
    "    tweetDict = dictionary_tweet(trainTweets_hashtagsV2)\n",
    "    \n",
    "    trainFea_hashtagsV2 = convert_to_feature_dicts(trainTweets_hashtagsV2,engStop,2)\n",
    "    devFea_hashtagsV2 = convert_to_feature_dicts(devTweets_hashtagsV2,engStop,0)\n",
    "    \n",
    "    vectorize_hashtagsV2 = DictVectorizer()\n",
    "    trainData_hashtagsV2 = vectorize_hashtagsV2.fit_transform(trainFea_hashtagsV2)\n",
    "    devData_hashtagsV2 = vectorize_hashtagsV2.transform(devFea_hashtagsV2)\n",
    "    \n",
    "    #lr_hashtagsV2 = LogisticRegression(class_weight=None,\\\n",
    "    #                            solver='lbfgs',\\\n",
    "    #                            fit_intercept=False,\\\n",
    "    #                            C=1.0)\n",
    "    lr_hashtagsV2 = LogisticRegression(solver='lbfgs', multi_class='multinomial')    \n",
    "    lr_hashtagsV2.fit(trainData_hashtagsV2,trainLabels_hashtagsV2)\n",
    "    #print lr_hashtagsV2\n",
    "    a = do_heldout_validation(lr_hashtagsV2,devData_hashtagsV2,devLabels_hashtagsV2)\n",
    "    #print a\n",
    "    # 0.496446145435\n",
    "    # the original untuned classifier(solver='lbfgs', multi_class='multinomial') is 0.49425915801\n",
    "    prediction = lr_hashtagsV2.predict(devData_hashtagsV2)\n",
    "    truth = devLabels_hashtagsV2\n",
    "    print \"accuracy\"\n",
    "    print accuracy_score(truth,prediction)\n",
    "    print classification_report(truth,prediction)\n",
    "    return accuracy_score(truth,prediction)\n",
    "\n",
    "classifier_hashtagsV2(r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\train.json\",\\\n",
    "                      r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\dev.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this cell, I tried several ways to improve the accuracy of the classifier. I tried to use polarity score \n",
    "instead of just counting numbers of polar words, but the result did not see too much improvement. I also tried\n",
    "to use lemmatization in preprocessing, the result is improved from 0.49425915801 to 0.496446145435 on default \n",
    "settings(solver='lbfgs', multi_class='multinomial'). And then I tried to modify hashtag processing,\n",
    "More specifically, if there is a hashtag contains only uppercase letters, all the letters will be transferred to \n",
    "lowercase and then imported to MaxMatch. This modification slightly improve the accuracy on \n",
    "default settings(solver='lbfgs', multi_class='multinomial')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "0bdcd27988c6d3946fc4e299aa6b853472e729362c9fee8e6eeb702e"
   },
   "source": [
    "## Final testing and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "6541f015ac2a5cbd17c112478e59964a48a077c0c092b7e6109350f7"
   },
   "source": [
    "<b>Instructions</b>: When the final test set has been released, you should start with your best classifier from your work up to this point, and do a final test of all major options, including at least one from each of the first four sections of the assignment (that is, at least one preprocessing option, at least one tuning parameter/classifier type, at least one lexicon, and your improvement), in this new dataset. You don't need to explore every possible combination (in fact, you shouldn't), but you should make a convincing case that you have probably found the best combination given the possibilities you have implemented. It's okay if you find discrepancies between the best classifier on the test set and development set, just be sure to mention them. In a final discussion (which should be at least 500 words), you should include at least one bar graph of accuracy across various options, and at least one table which reports precision, recall, and F-score for each label as well as the macroaveraged F-score (all figures and tables should be generated inline by your code, using matplotlib). Please conclude your discussion by discussing what you have learned, and mentioning any other ideas for improving performance of this system that you may have.\n",
    "\n",
    "Note that you may have to direct matplotlib to display the figures inline, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false,
    "signature": "e4fc3eac75c291c8eb0ba04f9e3e7b790bb1d41415a4b77aac071a85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------\n",
      "testing no lemmatization in proporcessing, default settings\n",
      "accuracy 0.496402877698\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.35      0.38      0.36       290\n",
      "          0       0.41      0.62      0.49       625\n",
      "          1       0.73      0.45      0.56       892\n",
      "\n",
      "avg / total       0.56      0.50      0.50      1807\n",
      "\n",
      "testing with lemmatization in proporcessing, default settings\n",
      "1807\n",
      "accuracy\n",
      "0.511898173769\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.36      0.42      0.39       290\n",
      "          0       0.43      0.61      0.50       625\n",
      "          1       0.74      0.47      0.58       892\n",
      "\n",
      "avg / total       0.57      0.51      0.52      1807\n",
      "\n",
      "-------------------------------------------------------------------------------------------------\n",
      "testing the classifier with WordNet lexicon and optimized settings (my best classifier)\n",
      "accuracy\n",
      "0.504703929164\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.36      0.37      0.36       290\n",
      "          0       0.41      0.63      0.50       625\n",
      "          1       0.74      0.46      0.57       892\n",
      "\n",
      "avg / total       0.56      0.50      0.51      1807\n",
      "\n",
      "-------------------------------------------------------------------------------------------------\n",
      "testing the classifier with optimized settings from tunning process\n",
      "accuracy\n",
      "0.504150525733\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.36      0.40      0.38       290\n",
      "          0       0.41      0.59      0.49       625\n",
      "          1       0.72      0.48      0.57       892\n",
      "\n",
      "avg / total       0.55      0.50      0.51      1807\n",
      "\n",
      "-------------------------------------------------------------------------------------------------\n",
      "testing the classifier with new hashtag method which is developed in improvement\n",
      "accuracy\n",
      "0.495849474267\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.35      0.38      0.36       290\n",
      "          0       0.41      0.62      0.49       625\n",
      "          1       0.73      0.45      0.56       892\n",
      "\n",
      "avg / total       0.56      0.50      0.50      1807\n",
      "\n",
      "-------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhoAAAEZCAYAAADRxYwVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVYa6NwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x170e5588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#trainTweets,trainLabels = preprocess_file_no_lemma\\\n",
    "#(r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\train.json\")\n",
    "testTweets,testLabels = preprocess_file_no_lemma\\\n",
    "(r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\test.json\")\n",
    "print \"-------------------------------------------------------------------------------------------------\"\n",
    "\n",
    "performance = []\n",
    "\n",
    "def test_classifier_lemma():\n",
    "\n",
    "    global trainTweets\n",
    "    global testTweets\n",
    "    global testLabels\n",
    "    global trainLabels\n",
    "    \n",
    "    print \"testing no lemmatization in proporcessing, default settings\"\n",
    "    trainFea = convert_to_feature_dicts(trainTweets,engStop,2)\n",
    "    testFea = convert_to_feature_dicts(testTweets,engStop,0)\n",
    "\n",
    "    vectorize_final = DictVectorizer()\n",
    "    trainData = vectorize.fit_transform(trainFea)\n",
    "    testData = vectorize.transform(testFea)\n",
    "\n",
    "    lr = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "    lr.fit(trainData,trainLabels)\n",
    "\n",
    "    a = do_heldout_validation(lr,testData,testLabels)\n",
    "    prediction = lr.predict(testData)\n",
    "    truth = testLabels\n",
    "    print \"accuracy\",\n",
    "    print accuracy_score(truth,prediction)\n",
    "    print classification_report(truth,prediction)\n",
    "    performance.append(accuracy_score(truth,prediction)-0.45)\n",
    "    ###########################################################\n",
    "    print \"testing with lemmatization in proporcessing, default settings\"\n",
    "    trainTweets_lem,trainLabels_lem = preprocess_file\\\n",
    "    (r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\train.json\")\n",
    "    testTweets_lem,testLabels_lem = preprocess_file\\\n",
    "    (r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\test.json\")\n",
    "    \n",
    "    trainFea_lem = convert_to_feature_dicts(trainTweets_lem,engStop,2)\n",
    "    testFea_lem = convert_to_feature_dicts(testTweets_lem,engStop,0)\n",
    "    \n",
    "    #print len(testFea_lem),len(testLabels)\n",
    "    #print testFea_lem\n",
    "    vectorize_lem = DictVectorizer()\n",
    "    trainData_lem = vectorize_lem.fit_transform(trainFea_lem)\n",
    "    testData_lem = vectorize_lem.transform(testFea_lem)   \n",
    "    \n",
    "    #print testData_lem.get_shape()\n",
    "    \n",
    "    lr_lem = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "    lr_lem.fit(trainData_lem,trainLabels)\n",
    "    #print lr_lem\n",
    "    #a = do_heldout_validation(lr_lem,testData_lem,testLabels_lem)\n",
    "    #print a   \n",
    "    prediction = lr_lem.predict(testData_lem)\n",
    "    print len(prediction)\n",
    "    truth = testLabels_lem\n",
    "    print \"accuracy\"\n",
    "    print accuracy_score(truth,prediction)\n",
    "    print classification_report(truth,prediction)\n",
    "    performance.append(accuracy_score(truth,prediction)-0.45)\n",
    "    \n",
    "def test_classifier_WordNet():\n",
    "    global trainTweets\n",
    "    global testTweets\n",
    "    global testLabels\n",
    "    global trainLabels\n",
    "    print \"testing the classifier with WordNet lexicon and optimized settings (my best classifier)\"\n",
    "    trainFea_Wordnet = convert_to_feature_dict_lexicon(trainTweets,engStop,2,WordnetSetPos,WordnetSetNeg)\n",
    "    vectorize_Wordnet = DictVectorizer()\n",
    "    trainData_Wordnet = vectorize.fit_transform(trainFea)\n",
    "    testData_Wordnet = vectorize.transform(testFea)\n",
    "    \n",
    "    lr_Wordnet = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "    lr_Wordnet.fit(trainData_Wordnet,trainLabels)\n",
    "    \n",
    "    a = do_heldout_validation(lr_Wordnet,testData_Wordnet,testLabels)\n",
    "    prediction = lr_Wordnet.predict(testData_Wordnet)\n",
    "    truth = testLabels\n",
    "    print \"accuracy\"\n",
    "    print accuracy_score(truth,prediction)\n",
    "    print classification_report(truth,prediction)\n",
    "    performance.append(accuracy_score(truth,prediction)-0.45)\n",
    "\n",
    "def test_classifier_Tuning():\n",
    "    global trainTweets\n",
    "    global testTweets\n",
    "    global testLabels\n",
    "    global trainLabels\n",
    "    print \"testing the classifier with optimized settings from tunning process\"\n",
    "    \n",
    "    trainFea = convert_to_feature_dicts(trainTweets,engStop,2)\n",
    "    testFea = convert_to_feature_dicts(testTweets,engStop,0)\n",
    "\n",
    "    vectorize_final = DictVectorizer()\n",
    "    trainData = vectorize.fit_transform(trainFea)\n",
    "    testData = vectorize.transform(testFea)\n",
    "    \n",
    "    lr_Tunning = LogisticRegression(class_weight=None,solver='lbfgs',\\\n",
    "                                    fit_intercept=False,C=1.0)\n",
    "    lr_Tunning.fit(trainData,trainLabels)\n",
    "    \n",
    "    a = do_heldout_validation(lr_Tunning,testData,testLabels)\n",
    "    prediction = lr_Tunning.predict(testData)\n",
    "    truth = testLabels\n",
    "    print \"accuracy\"\n",
    "    print accuracy_score(truth,prediction)\n",
    "    print classification_report(truth,prediction)\n",
    "    performance.append(accuracy_score(truth,prediction)-0.45)\n",
    "    \n",
    "def test_classifier_hashtagV2():\n",
    "    print \"testing the classifier with new hashtag method which is developed in improvement\"\n",
    "    a = classifier_hashtagsV2(r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\train.json\",\\\n",
    "                          r\"C:\\Users\\Ryan\\OneDrive\\study\\web search and text analysis\\assignment1\\Assignment1_data\\test.json\")\n",
    "    performance.append(a-0.45)\n",
    "    \n",
    "test_classifier_lemma()\n",
    "print \"-------------------------------------------------------------------------------------------------\"\n",
    "test_classifier_WordNet()\n",
    "print \"-------------------------------------------------------------------------------------------------\"\n",
    "test_classifier_Tuning()\n",
    "print \"-------------------------------------------------------------------------------------------------\"\n",
    "test_classifier_hashtagV2()\n",
    "print \"-------------------------------------------------------------------------------------------------\"\n",
    "\n",
    "classifiers = ('no_lemma', 'lemma', 'WordNet_OptSet(best from earlier)', 'OptSet', 'hashtagV2')\n",
    "y_pos = np.arange(len(classifiers))\n",
    "#print y_pos\n",
    "#print performance\n",
    "#error = np.random.rand(len(people))\n",
    "#print error\n",
    "\n",
    "plt.barh(y_pos, performance, height=0.8,align='center',alpha=0.4)\n",
    "plt.yticks(y_pos, classifiers)\n",
    "plt.xlabel('Accuracy(0.45+x)')\n",
    "plt.title('Evaluation on Classifiers')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, I tested five classifiers on the test data, they are:\n",
    "\n",
    "WordNet_OptSet(best from earlier) --> the best classifier I found in the previous sections, using WordNet as a lexicon\n",
    "                                      to provide an extra feature, and also using the \"best\" parameters settings,\n",
    "no_lemma                          --> the default classifier, does not use lemmatization, use default \n",
    "                                      setting(solver='lbfgs', multi_class='multinomial'),does not use any lexicon\n",
    "lemma                             --> this classifier use lemmatization when preprocessing tweets, use default settings\n",
    "OptSet                            --> this is the classifier using the \"best\" parameters settings tested from \n",
    "                                      previous tunning section\n",
    "hashtagV2                         --> this classifier uses a different way to extra the hashtags, if there is a \n",
    "                                      hashtag contains only uppercase letters, all the letters will be transferred to \n",
    "                                      lowercase and then imported to MaxMatch. \n",
    "\n",
    "The tables and bar chart have been genarated in the output area\n",
    "\n",
    "Start from the most affected classifier 'OptSet', there is a performance drop-off on it. It may be because that the \n",
    "\"Best\" parameters are got from the tunning process which is based on the accuracy to development data. So the classifier\n",
    "may need to re-tunning based on the test data(this is meanless in practical) for getting a better performance.\n",
    "\n",
    "Besides, since my best classifier also uses the \"opitimized\" parameters (the same as 'OptSet'), the performance of it\n",
    "is also affected a lot.\n",
    "\n",
    "'hashtagV2' does not bring improvement in this case, this could because the affected features are too little.\n",
    "\n",
    "Fortunately, the classifier 'OptSet' and 'WordNet_OptSet' still works much better than the default 'no_lemma' classifier,\n",
    "which shows the improvment brought by my previous works\n",
    "\n",
    "The classifier with the best performance is 'lemma', because it does not use the previouly tunned parameters, its \n",
    "performance will not be affected by the new dataset too much. The high accuracy demonstrateds the applicability of \n",
    "lemmatization in preprocessing stage. \n",
    "\n",
    "Based on the result, my proposed solution is to add lemmatization in the preprocessing stage, and also add WordNet \n",
    "lexicon as one feature. Then retune the parameter of the new classifier to get a better classifier(I won perform this\n",
    "in this project, since it may cause the program cannot be finished in 10 minutes).\n",
    "\n",
    "In conclusion, I learned a lot from this project:\n",
    "1. the process of performing a machine learning method on Natural Language processing tasks. If I get another project,\n",
    "I won't become confused of not knowing where I should start from. \n",
    "2. How to use programming in data mining smartly, especially for a large project or dataset. More specifically, I learned how hashed data structures(e.g. dictionary & set) is necessary for looking up in a large dataset. Without it, \n",
    "some codes will take forever to finish. \n",
    "3. Overly tunning the parameters of classifier based on one dataset could cause overfitting, which restrict the performance of the classifier to other dataset\n",
    "4. Python is very easy to use\n",
    "5. How to use python to generate visualized data for presentation\n",
    "6. Using Lexicon is a efficient way to feature engineering for text mining tasks\n",
    "\n",
    "For improving the classifier, I have several ideas:\n",
    "1. we can combine different classifiers to generate one classifer. In other words, we can use the result of each different classifier as one feature of each instance\n",
    "2. We should reference some standard parameters for each specific tasks, where the best parameter settings could have been found by others, we don't need to reinvent the wheel\n",
    "3. some dictionaries could be used like disease name dictionaries, which may have some relationship with the sentiment of sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
